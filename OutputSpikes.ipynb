{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrezakhodashenas/PhD-NN/blob/main/OutputSpikes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HiiAfjcTIpU"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LJRXXN6gygC"
      },
      "outputs": [],
      "source": [
        "# !pip install brian2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOq6G-QBejwM",
        "outputId": "900f62df-58e4-4f23-ef90-68ddef14b00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czXuC1x8el5J",
        "outputId": "151bdc10-7573-427a-848c-cfe42e780ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.9-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.1.2-py3-none-any.whl (764 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.8/764.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.9.0 pytorch-lightning-2.0.9 torchmetrics-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTEDV8wqLZXr",
        "outputId": "8397867a-1e90-4004-9e87-5f1d8bc86f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.7.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m102.4/109.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.23.5)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-0.2.0-py3-none-any.whl (21 kB)\n",
            "Collecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-0.2.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->snntorch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->snntorch) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Installing collected packages: nir, nirtorch, snntorch\n",
            "Successfully installed nir-0.2.0 nirtorch-0.2.1 snntorch-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGPDVXvjdGsl",
        "outputId": "39fd6887-fd96-413a-e591-b4c75c865d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7xJifRLMc2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KehrXuVuPiAt"
      },
      "source": [
        "## set seeds for PyTorch and Numpy to ensure reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbSadcLWPedr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for Python, Numpy, and Torch for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Additional steps if you're using GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyXSF4IqLFND"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pickle\n",
        "import matplotlib.animation as animation\n",
        "from scipy.integrate import simps\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import os, sys, time, datetime, json, random\n",
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import utils as utls\n",
        "from snntorch import utils\n",
        "from snntorch import surrogate\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import auc\n",
        "from torchsummary import summary\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn as nn\n",
        "from scipy.stats import entropy\n",
        "from scipy.special import kl_div\n",
        "from torch.autograd import Variable\n",
        "# import spikeflow as snn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.autograd.set_detect_anomaly(True)"
      ],
      "metadata": {
        "id": "ZDOWUXysMxFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOyTY0dE50vG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bdf3af-aac2-4e3b-cc8c-656bbbb4fe43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 346351696.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 12618301.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 138606128.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1349569.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# /////////////////////# Building the Autoencoder\n",
        "#-------------------DataLoaders.  using the MNIST dataset\n",
        "\n",
        "# dataloader arguments\n",
        "batch_size = 250\n",
        "data_path='/data/mnist'\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# /////////////////////////////////# Define a transform\n",
        "input_size = 32 # resizing the original MNIST from 28 to 32\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "#------------------------------------------- Load MNIST\n",
        "# Training data\n",
        "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Testing data\n",
        "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha5yn2VjO39Q"
      },
      "outputs": [],
      "source": [
        "# creating directories where we can save the original and reconstructed images for training and testing:\n",
        "# create training/ and testing/ folders in the chosen path\n",
        "if not os.path.isdir('figures/training'):\n",
        "    os.makedirs('figures/training')\n",
        "if not os.path.isdir('figures/binarytraining'):\n",
        "    os.makedirs('figures/binarytraining')\n",
        "\n",
        "if not os.path.isdir('figures/testing'):\n",
        "    os.makedirs('figures/testing')\n",
        "if not os.path.isdir('figures/binarytesting'):\n",
        "    os.makedirs('figures/binarytesting')\n",
        "\n",
        "\n",
        "if not os.path.isdir('Saved_Trained_Checkpoints/'):\n",
        "    os.makedirs('Saved_Trained_Checkpoints/')\n",
        "\n",
        "if not os.path.isdir('Output_Spikes/'):\n",
        "    os.makedirs('Output_Spikes/')\n",
        "\n",
        "if not os.path.isdir('Enc_syn_Spikes/'):\n",
        "    os.makedirs('Enc_syn_Spikes/')\n",
        "\n",
        "\n",
        "if not os.path.isdir('Intermediate_Lyrs/'):\n",
        "    os.makedirs('Intermediate_Lyrs/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcZMZrCoKFaq",
        "outputId": "fb1a2fea-4ac4-4f85-f81e-48433487bf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Tesla T4 (cuda)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyQvxnL5umDu"
      },
      "source": [
        "### To manipulate and test (Working for Encoder and Decoder outputs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VQHpS2xufu0"
      },
      "outputs": [],
      "source": [
        "# class SAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "\n",
        "#         # Encoder\n",
        "#         self.encoder = nn.Sequential(\n",
        "#                             nn.Conv2d(1, 32, 3,padding = 1,stride=2), # Conv Layer 1\n",
        "#                             # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n",
        "#                                 # bias=True, padding_mode='zeros',  device=None, dtype=None)\n",
        "#                             nn.BatchNorm2d(32),\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             # snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             nn.Conv2d(32, 64, 3,padding = 1,stride=2), # Conv Layer 2\n",
        "#                             nn.BatchNorm2d(64),\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             # snn.Alpha(alpha=alpha1, beta=beta1, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.Conv2d(64, 128, 3,padding = 1,stride=2), # Conv Layer 3\n",
        "#                             nn.BatchNorm2d(128),\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             # snn.Alpha(alpha=alpha11, beta=beta11, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.Flatten(start_dim = 1, end_dim = 3), #Flatten convolutional output\n",
        "#                             nn.Linear(128*4*4, latent_dim), # Fully connected linear layer\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "#                             # snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "#                             )\n",
        "\n",
        "\n",
        "#         self.latent_dim = latent_dim #dimensions of the encoded z-space data\n",
        "\n",
        "#         #ve from the flattened encoded representation (latent_dim) back to a tensor representation to\n",
        "#             # use in transposed convolution.\n",
        "#           # To do so, we need to run an additional fully-connected linear layer transforming the data back into a tensor of 128 x 4 x 4:\n",
        "\n",
        "#         self.linearNet= nn.Sequential(\n",
        "#                                       nn.Linear(latent_dim,128*4*4),\n",
        "#                                       snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "#                                       #snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#       #  The decoder, with three transposed convolutional (nn.ConvTranspose2d) layers and one linear output layer.\n",
        "#       # Although we converted the latent data back into tensor form for convolution, we still need to Unflatten it to a tensor of 128 x 4 x 4,\n",
        "#         # as the input to the network is 1 dimensional.  This is done using nn.Unflatten in the first line of the Decoder:\n",
        "#         # Decoder:\n",
        "#         self.decoder = nn.Sequential(\n",
        "#                             nn.Unflatten(1,(128,4,4)), #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             # snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             nn.BatchNorm2d(64),\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             # snn.Alpha(alpha=alpha2, beta=beta2, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             nn.BatchNorm2d(32),\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             # snn.Alpha(alpha=alpha22, beta=beta22, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=thresh) #---------------------------------------------- ADDED\n",
        "#                             # snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#                             )\n",
        "#         # final Leaky layer, our spiking threshold (thresh) is set extremely high. This is a neat trick in snnTorch, which allows the neuron\n",
        "#         # membrane in the final layer to continuously be updated, without ever reaching a spiking threshold.\n",
        "\n",
        "#         # using the membrane potential output from the final layer for the image reconstruction.\n",
        "#             # snnTorch allows us to use either the spikes or membrane potential of each neuron in training.\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         utils.reset(self.encoder) #need to reset the hidden states of LIF\n",
        "#         utils.reset(self.decoder)\n",
        "#         utils.reset(self.linearNet)\n",
        "\n",
        "#     #-----------------------------encode\n",
        "#         spk_mem=[];\n",
        "#         spk_rec=[];\n",
        "#         spk_rec_syn=[];\n",
        "#         encoder_mem=[];\n",
        "#         spk_rec_dec=[];\n",
        "#         spk_mem_dec=[];\n",
        "#         for step in range(num_steps): #for t in time\n",
        "#             spk_x, mem_x = self.encode(x) #Output spike trains and neuron membrane states\n",
        "#             # print(\"Size of = self.encode(x):\", self.encode(x).size())\n",
        "#             # print(\"len of = self.encode(x):\", len(self.encode(x)))\n",
        "#             spk_rec.append(spk_x)\n",
        "#             spk_mem.append(mem_x)\n",
        "\n",
        "#         spk_rec=torch.stack(spk_rec,dim=2) # stack spikes in second tensor dimension\n",
        "#         spk_mem=torch.stack(spk_mem,dim=2) # stack membranes in second tensor dimension\n",
        "# # torch.stack joins (concatenates) a sequence of tensors (two or more tensors) along a new dimension.\n",
        "#         # print(\"Size of spk_rec:\", spk_rec.size())   # Size of spk_rec: torch.Size([250, 32, 5])\n",
        "#         # print(\"Size of spk_mem:\", spk_mem.size()) # Size of spk_mem: torch.Size([250, 32, 5])\n",
        "#         # out_en = spk_rec[:,:,-1]     #//////////////////////////////////////ADDED---------------- shows  batch size (250) different examples on (32) channel number\n",
        "#         out_en = spk_rec[0, :,:] #//////////////////////////////////////ADDED------------ latent dim (32) and time (num_steps)(5)\n",
        "#         # print(\"Size of out_en:\", out_en.size()) #Size of out_en: torch.Size([250, 32])\n",
        "\n",
        "#     #------------------------------decode\n",
        "#         spk_mem2=[];\n",
        "#         spk_rec2=[];\n",
        "#         decoded_x=[];\n",
        "#         spk_x_dec=[];\n",
        "#         mem_x_dec=[];\n",
        "#         for step in range(num_steps): #for t in time\n",
        "#             # spk_x_dec, mem_x_dec = self.decode(spk_rec[...,step]) #//////////////////////////ADDED\n",
        "#             x_recon, x_mem_recon = self.decode(spk_rec[...,step])\n",
        "\n",
        "#             spk_rec2.append(x_recon)\n",
        "#             spk_mem2.append(x_mem_recon)\n",
        "\n",
        "#             # spk_rec_dec.append(spk_x_dec)   #//////////////////////////ADDED Size of spk_rec_dec: torch.Size([250, 1, 5, 32, 32])\n",
        "#             # spk_mem_dec.append(mem_x_dec)   #//////////////////////////ADDED\n",
        "\n",
        "#         # spk_rec_dec=torch.stack(spk_rec_dec,dim=2) #//////////////////////////ADDED\n",
        "#         # spk_mem_dec=torch.stack(spk_mem_dec,dim=2) #//////////////////////////ADDED\n",
        "#         spk_rec2=torch.stack(spk_rec2,dim=4)\n",
        "#         spk_mem2=torch.stack(spk_mem2,dim=4)\n",
        "\n",
        "#         # out = spk_mem2[:,:,:,:,-1] #return the membrane potential of the output neuron at t = -1 (last t)\n",
        "#         # out = spk_rec2[:,:,:,:,-1]  #//////////////////////////ADDED #  Size of out: torch.Size([250, 1, 32, 32])\n",
        "#         # out = spk_rec_dec[:,:,-1] #//////////////////////////ADDED  # Size of out: torch.Size([250, 1, 32, 32])\n",
        "#         # out = spk_rec_dec[:,0,:,-1] #//////////////////////////ADDED  # Size of out: torch.Size([250, 5, 32])\n",
        "#         # out = spk_rec_dec[:,0,0,:,-1] #//////////////////////////ADDED  # Size of out: torch.Size([250, 32])\n",
        "#         # out = spk_rec_dec[:,-1] #//////////////////////////ADDED  Size of out: torch.Size([250, 5, 32, 32])\n",
        "#         # out = spk_rec2[0,0,:,:] #//////////////////////////ADDED Now working with [250, 1, 32, 32]) because it is (torch.Size([32, 32, 5]))\n",
        "#         out = spk_mem2[:,:,:,:,-1]  #//////////////////////////ADDED #  Size of out: torch.Size([250, 1, 32, 32])--------------- for one digit at the last time t\n",
        "\n",
        "#         # print(\"Size of out:\", out.size())\n",
        "\n",
        "#         # Save the out_en tensor\n",
        "#         self.out_en = out_en\n",
        "#         self.out = out\n",
        "\n",
        "#         return out, out_en\n",
        "\n",
        "\n",
        "#     def encode(self,x):\n",
        "#       spk_latent_x, mem_latent_x = self.encoder(x)\n",
        "#       return spk_latent_x, mem_latent_x, syn1, memsyn1\n",
        "\n",
        "#     def decode(self,x):\n",
        "#         spk_x, mem_x = self.linearNet(x) #convert latent dimension back to total size of features in encoder final layer\n",
        "#         spk_x2, mem_x2 = self.decoder(spk_x)\n",
        "#         return spk_x2, mem_x2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with \"out\" and \"out_en\":\n"
      ],
      "metadata": {
        "id": "vIXf5IFdcTt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         # Encoder\n",
        "#         self.encoder = nn.Sequential(\n",
        "#                             nn.Conv2d(1, 32, 3,padding = 1,stride=2), # Conv Layer 1\n",
        "#                             nn.BatchNorm2d(32),\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.Conv2d(32, 64, 3,padding = 1,stride=2), # Conv Layer 2\n",
        "#                             nn.BatchNorm2d(64),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             nn.Conv2d(64, 128, 3,padding = 1,stride=2), # Conv Layer 3\n",
        "#                             nn.BatchNorm2d(128),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             nn.Flatten(start_dim = 1, end_dim = 3), #Flatten convolutional output\n",
        "#                             nn.Linear(128*4*4, latent_dim), # Fully connected linear layer\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "#                             )\n",
        "#         self.latent_dim = latent_dim #dimensions of the encoded z-space data\n",
        "#         self.linearNet= nn.Sequential(\n",
        "#                                       nn.Linear(latent_dim,128*4*4),\n",
        "#                                       snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "\n",
        "#         # Decoder:\n",
        "#         self.decoder = nn.Sequential(\n",
        "#                             nn.Unflatten(1,(128,4,4)), #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             nn.BatchNorm2d(64),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             nn.BatchNorm2d(32),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#                             )\n",
        "#     def forward(self, x):\n",
        "#         utils.reset(self.encoder) #need to reset the hidden states of LIF\n",
        "#         utils.reset(self.decoder)\n",
        "#         utils.reset(self.linearNet)\n",
        "\n",
        "#     #-----------------------------encode\n",
        "#         spk_mem=[];\n",
        "#         spk_rec=[];\n",
        "#         spk_rec_syn=[];\n",
        "#         encoder_mem=[];\n",
        "#         spk_rec_dec=[];\n",
        "#         spk_mem_dec=[];\n",
        "#         for step in range(num_steps): #for t in time\n",
        "#             spk_x, mem_x = self.encode(x) #Output spike trains and neuron membrane states\n",
        "#             # print(\"spk_x in spk_x, mem_x = self.encode(x): \",spk_x.size())    #                       torch.Size([250, 32])\n",
        "#             # print(\"mem_x in spk_x, mem_x = self.encode(x): \",mem_x.size())    #                       torch.Size([250, 32])\n",
        "#             spk_rec.append(spk_x)\n",
        "#             spk_mem.append(mem_x)\n",
        "#             # print(\"len of spk_rec in spk_rec.append(spk_x): \",len(spk_rec))\n",
        "#             # print(\"len of spk_mem in spk_rec.append(mem_x): \",len(spk_mem))\n",
        "\n",
        "#         spk_rec=torch.stack(spk_rec,dim=2) # stack spikes in second tensor dimension # ----------------spk_rec in torch.stack(spk_rec,dim=2):  torch.Size([250, 32, 5])\n",
        "#         # print(\"----------------spk_rec in torch.stack(spk_rec,dim=2): \",spk_rec.size())\n",
        "#         spk_mem=torch.stack(spk_mem,dim=2) # stack membranes in second tensor dimension # ----------------spk_mem in torch.stack(spk_mem,dim=2):  torch.Size([250, 32, 5])\n",
        "#         # print(\"----------------spk_mem in torch.stack(spk_mem,dim=2): \",spk_mem.size())\n",
        "#         # out_en = spk_rec[0, :,:] #//////////////////////////////////////ADDED------------ latent dim (32) and time (num_steps)(5) # out_en in spk_rec[0, :,:]:----------- torch.Size([32, 5])\n",
        "#         # print(\"out_en in spk_rec[0, :,:]:-----------\" , out_en.size())\n",
        "#         out_en = spk_rec[...,step]\n",
        "#         # print(\"spk_rec[...,step]:-----------\" , spk_rec[...,step].size()) # spk_rec[...,step]:----------- torch.Size([250, 32])       input of the latent and then decoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     #------------------------------decode\n",
        "#         spk_mem2=[];\n",
        "#         spk_rec2=[];\n",
        "#         decoded_x=[];\n",
        "#         spk_x_dec=[];\n",
        "#         mem_x_dec=[];\n",
        "#         for step in range(num_steps): #for t in time                                   from decoder: ([250, 1, 32, 32])\n",
        "#             x_recon, x_mem_recon = self.decode(spk_rec[...,step])                             # x_recon in self.decode(spk_rec[...,step]): torch.Size([250, 1, 32, 32])\n",
        "#             # print(\"x_recon in self.decode(spk_rec[...,step]):\" , x_recon.size())            # x_mem_recon in self.decode(spk_rec[...,step]): torch.Size([250, 1, 32, 32])\n",
        "#             # print(\"x_mem_recon in self.decode(spk_rec[...,step]):\" , x_mem_recon.size())\n",
        "\n",
        "#             spk_rec2.append(x_recon)\n",
        "#             spk_mem2.append(x_mem_recon)\n",
        "#             # print(\"len of spk_rec2.append(x_recon)\",len(spk_rec2))\n",
        "#             # print(\"len of spk_mem2.append(x_recon)\",len(spk_mem2))\n",
        "\n",
        "#         spk_rec2=torch.stack(spk_rec2,dim=4)\n",
        "#         spk_mem2=torch.stack(spk_mem2,dim=4)\n",
        "#         # print(\"spk_rec2 in torch.stack(spk_rec2,dim=4):\", spk_rec2.size())          # spk_rec2 in torch.stack(spk_rec2,dim=4): torch.Size([250, 1, 32, 32, 5])\n",
        "#         # print(\"spk_mem2 in torch.stack(spk_rec2,dim=4):\", spk_mem2.size())          # spk_mem2 in torch.stack(spk_rec2,dim=4): torch.Size([250, 1, 32, 32, 5])\n",
        "\n",
        "#         out = spk_mem2[:,:,:,:,-1]  #//////////////////////////ADDED #  Size of out: torch.Size([250, 1, 32, 32])--------------- for one digit at the last time t\n",
        "\n",
        "#         # self.out_en = out_en\n",
        "#         # self.out = out\n",
        "\n",
        "#         return out, out_en\n",
        "\n",
        "\n",
        "#     def encode(self,x):\n",
        "#       spk_latent_x, mem_latent_x = self.encoder(x)\n",
        "#       return spk_latent_x, mem_latent_x\n",
        "\n",
        "#     def decode(self,x):\n",
        "#         spk_x, mem_x = self.linearNet(x) #convert latent dimension back to total size of features in encoder final layer\n",
        "#         spk_x2, mem_x2 = self.decoder(spk_x)\n",
        "#         return spk_x2, mem_x2"
      ],
      "metadata": {
        "id": "MuOth1ZabvgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # encoder_layers = [\n",
        "        #     ('conv1', nn.Conv2d(1, 32, 3, padding=1, stride=2)),  # Conv Layer 1\n",
        "        #     ('batchnorm1', nn.BatchNorm2d(32)),\n",
        "        #     ('leaky1', snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),\n",
        "        #     ('conv2', nn.Conv2d(32, 64, 3, padding=1, stride=2)),  # Conv Layer 2\n",
        "        #     ('batchnorm2', nn.BatchNorm2d(64)),\n",
        "        #     ('synaptic1', snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),  # SNN TORCH LIF NEURON\n",
        "        #     ('conv3', nn.Conv2d(64, 128, 3, padding=1, stride=2)),  # Conv Layer 3\n",
        "        #     ('batchnorm3', nn.BatchNorm2d(128)),\n",
        "        #     ('synaptic2', snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),  # SNN TORCH LIF NEURON\n",
        "        #     ('flatten', nn.Flatten(start_dim=1, end_dim=3)),  # Flatten convolutional output\n",
        "        #     ('linear', nn.Linear(128 * 4 * 4, latent_dim)),  # Fully connected linear layer\n",
        "        #     ('leaky2', snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh))\n",
        "        # ]\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "                            nn.Conv2d(1, 32, 3,padding = 1,stride=2), # Conv Layer 1\n",
        "                            nn.BatchNorm2d(32),\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.Conv2d(32, 64, 3,padding = 1,stride=2), # Conv Layer 2\n",
        "                            nn.BatchNorm2d(64),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, threshold=thresh), #SNN TORCH LIF NEURON\n",
        "                            nn.Conv2d(64, 128, 3,padding = 1,stride=2), # Conv Layer 3\n",
        "                            nn.BatchNorm2d(128),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "                            nn.Flatten(start_dim = 1, end_dim = 3), #Flatten convolutional output\n",
        "                            nn.Linear(128*4*4, latent_dim), # Fully connected linear layer\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "                            )\n",
        "\n",
        "\n",
        "        self.latent_dim = latent_dim #dimensions of the encoded z-space data\n",
        "        self.linearNet= nn.Sequential(\n",
        "                                      nn.Linear(latent_dim,128*4*4),\n",
        "                                      snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "\n",
        "        # Decoder:\n",
        "        self.decoder = nn.Sequential(\n",
        "                            nn.Unflatten(1,(128,4,4)), #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            nn.BatchNorm2d(64),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            nn.BatchNorm2d(32),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "                            )\n",
        "    def forward(self, x):\n",
        "        utils.reset(self.encoder) #need to reset the hidden states of LIF\n",
        "        utils.reset(self.decoder)\n",
        "        utils.reset(self.linearNet)\n",
        "\n",
        "    #-----------------------------encode\n",
        "        spk_mem=[];\n",
        "        spk_rec=[];\n",
        "        spk_rec_syn=[];\n",
        "        encoder_mem=[];\n",
        "        spk_rec_dec=[];\n",
        "        spk_mem_dec=[];\n",
        "        enc5_rec = [];\n",
        "\n",
        "\n",
        "     #------------------------------ intermediate layers\n",
        "\n",
        "        # for step in range(num_steps):\n",
        "        #     enc5 = self.encoder[5](x)             #  enc5 shape: torch.Size([250, 1, 32, 32])\n",
        "        #     enc5_rec.append(enc5)\n",
        "        # enc5_rec = torch.stack(enc5_rec, dim=2)            #   enc5_rec size: torch.Size([250, 1, 5, 32, 32])\n",
        "        # Enc_syn_1 = enc5_rec[:, :, -1]                      # #   torch.Size([250, 1, 32, 32])\n",
        "\n",
        "     #------------------------------ encode\n",
        "        for step in range(num_steps):\n",
        "            spk_x, mem_x = self.encoder(x)              # spk_x size: ([250, 32])  ,   mem_x size: ([250, 32])  , x.shape : torch.Size([250, 1, 32, 32])\n",
        "            spk_rec.append(spk_x)\n",
        "            spk_mem.append(mem_x)\n",
        "\n",
        "        spk_rec=torch.stack(spk_rec,dim=2) # stack spikes in second tensor dimension # ----------------spk_rec in torch.stack(spk_rec,dim=2):  torch.Size([250, 32, 5])\n",
        "        spk_mem=torch.stack(spk_mem,dim=2) # stack membranes in second tensor dimension # ----------------spk_mem in torch.stack(spk_mem,dim=2):  torch.Size([250, 32, 5])\n",
        "        out_en = spk_rec[...,step]\n",
        "\n",
        "        # print(\"out_en= spk_rec[...,step]:-----------\" , spk_rec[...,step].size()) # spk_rec[...,step]:----------- torch.Size([250, 32])       input of the latent and then decoder\n",
        "\n",
        "     #------------------------------decode\n",
        "        spk_mem2=[];\n",
        "        spk_rec2=[];\n",
        "        decoded_x=[];\n",
        "        spk_x_dec=[];\n",
        "        mem_x_dec=[];\n",
        "        for step in range(num_steps): #for t in time                           #        from decoder: ([250, 1, 32, 32])\n",
        "            x_recon, x_mem_recon = self.decode(spk_rec[...,step])\n",
        "            spk_rec2.append(x_recon)\n",
        "            spk_mem2.append(x_mem_recon)\n",
        "\n",
        "        spk_rec2=torch.stack(spk_rec2,dim=4)\n",
        "        spk_mem2=torch.stack(spk_mem2,dim=4)\n",
        "\n",
        "        out = spk_mem2[:,:,:,:,-1]\n",
        "\n",
        "        self.out_en = out_en\n",
        "        self.out = out\n",
        "\n",
        "        return out, out_en\n",
        "        # return out, Enc_syn_1\n",
        "\n",
        "    def encode(self,x):\n",
        "      spk_latent_x, mem_latent_x = self.encoder(x)\n",
        "      return spk_latent_x, mem_latent_x\n",
        "\n",
        "\n",
        "    def decode(self,x):\n",
        "        spk_x, mem_x = self.linearNet(x) #convert latent dimension back to total size of features in encoder final layer\n",
        "        spk_x2, mem_x2 = self.decoder(spk_x)\n",
        "        return spk_x2, mem_x2\n",
        "\n"
      ],
      "metadata": {
        "id": "U_D8NuZnxpXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape = (250, 1, 32, 32)\n",
        "\n",
        "# # Create a random tensor with the specified shape\n",
        "# x = torch.randn(shape)\n",
        "\n",
        "# print(x.size())\n",
        "# print(x.shape)\n",
        "\n",
        "# print(x.size(0))\n",
        "\n",
        "# print(x.view(x.size(0), -1).size())\n",
        "# x=x.view(x.size(0), -1)\n",
        "# print(\"x new.      \",x.size())\n",
        "\n",
        "# print(\"len(x)----------\",len(x))\n",
        "\n",
        "\n",
        "# # Assuming x is your tensor of size [250, 1024]\n",
        "# new_shape = (-1, 32)  # -1 means \"whatever is needed to keep the total number of elements the same\"\n",
        "# reshaped_x = x.view(new_shape)\n",
        "\n",
        "# print(\"x new.      \",x.size())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d3HCr9tn2JIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALL LEAKY"
      ],
      "metadata": {
        "id": "NQ7EsTgaHefc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#              # Encoder\n",
        "#         self.enc0=    nn.Conv2d(1, 32, 3, padding=1, stride=2)\n",
        "#         self.enc1=    nn.BatchNorm2d(32)\n",
        "#         self.enc2=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.enc3=    nn.Conv2d(32, 64, 3, padding=1, stride=2)\n",
        "#         self.enc4=    nn.BatchNorm2d(64)\n",
        "#         self.enc5=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.enc6=    nn.Conv2d(64, 128, 3, padding=1, stride=2)\n",
        "#         self.enc7=    nn.BatchNorm2d(128)\n",
        "#         self.enc8=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.enc9=    nn.Flatten(start_dim=1, end_dim=3)\n",
        "#         self.enc10=   nn.Linear(128 * 4 * 4, latent_dim)\n",
        "#         self.enc11=   snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "\n",
        "#         self.latent_dim = latent_dim\n",
        "\n",
        "#         self.linearNet0= nn.Linear(latent_dim,128*4*4)\n",
        "#         self.linearNet1=snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "\n",
        "\n",
        "#         # Decoder:\n",
        "#         self.dec0=    nn.Unflatten(1,(128,4,4)) #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "#         self.dec1=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "#         self.dec2=    nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec3=    nn.BatchNorm2d(64)\n",
        "#         self.dec4=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.dec5=    nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec6=    nn.BatchNorm2d(32)\n",
        "#         self.dec7=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.dec8=    nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec9=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#        # self.dec9=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=thresh) #---------------------------------------------- ADDED\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         utils.reset(self.enc0)\n",
        "#         utils.reset(self.enc1)\n",
        "#         utils.reset(self.enc2)\n",
        "#         utils.reset(self.enc3)\n",
        "#         utils.reset(self.enc4)\n",
        "#         utils.reset(self.enc5)\n",
        "#         utils.reset(self.enc6)\n",
        "#         utils.reset(self.enc7)\n",
        "#         utils.reset(self.enc8)\n",
        "#         utils.reset(self.enc9)\n",
        "#         utils.reset(self.enc10)\n",
        "#         utils.reset(self.enc11)\n",
        "#         utils.reset(self.linearNet0)\n",
        "#         utils.reset(self.linearNet1)\n",
        "#         utils.reset(self.dec0)\n",
        "#         utils.reset(self.dec1)\n",
        "#         utils.reset(self.dec2)\n",
        "#         utils.reset(self.dec3)\n",
        "#         utils.reset(self.dec4)\n",
        "#         utils.reset(self.dec5)\n",
        "#         utils.reset(self.dec6)\n",
        "#         utils.reset(self.dec7)\n",
        "#         utils.reset(self.dec8)\n",
        "#         utils.reset(self.dec9)\n",
        "\n",
        "#         # # -------Initialize hidden states at t=0\n",
        "\n",
        "#         x2_rec = []\n",
        "#         x5_rec = []\n",
        "#         x8_rec = []\n",
        "#         x11_rec = []\n",
        "\n",
        "#         d1_rec = []\n",
        "#         d4_rec = []\n",
        "#         d7_rec = []\n",
        "#         d9_rec = []\n",
        "#         d9_rec_mem = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             #------------------------------ encoder:\n",
        "#             x0=self.enc0(x)\n",
        "#             x1=self.enc1(x0)\n",
        "#             x2=self.enc2(x1) #Leaky\n",
        "#             # print(\"len x2: \", len(x2))\n",
        "#             # print(\"x2.size: \", x2.size())\n",
        "#             x3=self.enc3(x2)\n",
        "#             x4=self.enc4(x3)\n",
        "#             x5=self.enc5(x4) #Leaky\n",
        "#             x6=self.enc6(x5)\n",
        "#             x7=self.enc7(x6)\n",
        "#             x8=self.enc8(x7) #Leaky\n",
        "#             x9=self.enc9(x8)\n",
        "#             x10=self.enc10(x9)\n",
        "#             x11=self.enc11(x10) #Leaky\n",
        "#             # print(\"len x11: \", len(x11))\n",
        "#             # print(\"x11[0].size: \", x11[0].size())         torch.Size([250, 32])\n",
        "#             # print(\"x11[1].size: \", x11[1].size())         torch.Size([250, 32])\n",
        "\n",
        "\n",
        "#             x2_rec.append(x2)              #    x2 size:  torch.Size([250, 32, 16, 16])\n",
        "#             x5_rec.append(x5)                 #x5 size:  torch.Size([250, 64, 8, 8])\n",
        "#             x8_rec.append(x8)              #  x8 size:  torch.Size([250, 128, 4, 4])\n",
        "#             x11_rec.append(x11[0])\n",
        "#             # print(\"x11 length: \",len(x11))                        x11 length:  2\n",
        "#             # print(\"x11[1] length: \",len(x11[1]))               #   x11[1] length:  250\n",
        "#             # print(\"x11[0] length: \",len(x11[0]))               #   x11[0] length:  250\n",
        "#             # print(\"x11_rec length: \",len(x11_rec))                x11_rec length:  1 >> 2 >> 3 >> 4 >> 5\n",
        "\n",
        "#             #-------------------------------------------------\n",
        "#         x2_rec = torch.stack(x2_rec, dim=2)\n",
        "#         x5_rec = torch.stack(x5_rec, dim=2)\n",
        "#         x8_rec = torch.stack(x8_rec, dim=2)\n",
        "#         x11_rec = torch.stack(x11_rec, dim=2)                         #       torch.Size([250, 32, 5])\n",
        "\n",
        "#         x2_rec = x2_rec[:, :, -1]                                               # =========  x2_rec[:, :, -1].size() torch.Size([250, 32, 16, 16])\n",
        "#         x5_rec = x5_rec[:, :, -1]                                               # =========  x5_rec[:, :, -1].size() torch.Size([250, 64, 8, 8])\n",
        "#         # print(\"=========  x5_rec[:, :, -1].size()\", x5_rec.size())\n",
        "#         x8_rec = x8_rec[:, :, -1]                                               #  ========= x8_rec[:, :, -1].size() torch.Size([250, 128, 4, 4])\n",
        "#         # print(\"=========  x8_rec[:, :, -1].size()\", x8_rec.size())\n",
        "#         out_en = x11_rec[...,step]\n",
        "#         # print(\"========= out_en: x11_rec[...,step] size: \", out_en.size())\n",
        "\n",
        "#         # x11_rec = x11_rec[:, :, -1]             #  =========  x11_rec[:, :, -1].size() torch.Size([250, 32])\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             #------------------------------ latent:\n",
        "#             # LN0=self.linearNet0(x11[...,step])         # means keep all dimensions of x11 up to the last one (which is usually the time dimension), and then selecting the data at the step position along that dimension.\n",
        "#             LN0=self.linearNet0(x11_rec[...,step])\n",
        "#             # print(\" LN0=self.linearNet0(x11_rec[...,step])======\", LN0.size())       # torch.Size([250, 2048])\n",
        "#             # print(\"type of LN0\",type(LN0))\n",
        "#             # print(\"length of LN0\",len(LN0))\n",
        "#             LN1=self.linearNet1(LN0)\n",
        "#             # print(\"LN1=self.linearNet1(LN0) size:\", len(LN1))\n",
        "#             # print(\"LN1[0].size()\", LN1[0].size())          #  LN1[0].size() torch.Size([250, 2048])\n",
        "#             # print(\"LN1[1].size()\", LN1[1].size())          #  LN1[1].size() torch.Size([250, 2048])\n",
        "\n",
        "#             # print(\"type of LN1\",type(LN1))\n",
        "#             # print(\"---------------length of LN1\",len(LN1))                      # length of LN1: 2\n",
        "#             # print(\"length of LN1\",len(LN1[0]))                                  # length of LN1[0]: 250\n",
        "#             # print(\"length of LN1\",len(LN1[1]))                                  # length of LN1[1]: 250\n",
        "#             #------------------------------ decoder:\n",
        "#             # d0=self.dec0(LN1[...,step])\n",
        "#             d0=self.dec0(LN1[0])\n",
        "#             # print(\"type of d0\",type(d0))\n",
        "#             # print(\"length of d0\",len(d0))\n",
        "#             # print(\"Size of d0\", LN1[0].size())              # Size of d0 torch.Size([250, 128, 4, 4])\n",
        "#             # print(\"Size of d0\", LN1[1].size())              # Size of d0 torch.Size([250, 128, 4, 4])\n",
        "\n",
        "#             d1=self.dec1(d0)     #Leaky\n",
        "#             d2=self.dec2(d1)\n",
        "#             d3=self.dec3(d2)\n",
        "#             d4=self.dec4(d3)     #Leaky\n",
        "#             d5=self.dec5(d4)\n",
        "#             d6=self.dec6(d5)\n",
        "#             d7=self.dec7(d6)     #Leaky\n",
        "#             d8=self.dec8(d7)\n",
        "#             d9=self.dec9(d8)     #Leaky\n",
        "#             # d9,d9_mem =self.dec9(d8)     #Leaky\n",
        "#             # print(\"            d9[0]=self.dec9(d8)\", d9[0].size())  #   torch.Size([250, 1, 32, 32])\n",
        "#             # print(\"            d9[1]=self.dec9(d8)\", d9[1].size())  #   torch.Size([250, 1, 32, 32])\n",
        "\n",
        "#             # x_recon=self.dec9(d8)\n",
        "#             #----------------------------------------\n",
        "\n",
        "#             #-----------decoder:\n",
        "#             d1_rec.append(d1)\n",
        "#             # print(\"length of d1_rec.append(d1)\",len(d1_rec))\n",
        "#             d4_rec.append(d4)\n",
        "#             # print(\"length of d4_rec.append(d4)\",len(d4_rec))\n",
        "#             d7_rec.append(d7)\n",
        "#             # print(\"length of d7_rec.append(d7)\",len(d7_rec))\n",
        "#             d9_rec.append(d9[0])\n",
        "#             # print(\"-----------d9 length: \", len(d9))                                   # length of d9: 2\n",
        "#             # print(\"d9[0] length: \", len(d9[0]))                                   # length of d9[0]: 250\n",
        "#             # print(\"d9[1] length: \", len(d9[1]))                                   # length of d9[1]: 250\n",
        "#             # print(\"length of d9_rec.append(d9)\",len(d9_rec))\n",
        "#             d9_rec_mem.append(d9[1])\n",
        "\n",
        "\n",
        "#        # decoder\n",
        "#         d1_rec = torch.stack(d1_rec, dim=4)                                              # torch.Size([250, 128, 4, 4, 5])\n",
        "#         # print(\"-----------------length of torch.stack(d1_rec, dim=4)\",len(d1_rec))\n",
        "#         # print(\"------size of torch.stack(d1_rec, dim=4)\",d1_rec.size())\n",
        "#         d4_rec = torch.stack(d4_rec, dim=4)                                              #torch.Size([250, 64, 8, 8, 5])\n",
        "#         # print(\"-----------------length of torch.stack(d4_rec, dim=4)\",len(d4_rec))\n",
        "#         # print(\"------size of torch.stack(d4_rec, dim=4)\",d4_rec.size())\n",
        "\n",
        "#         d7_rec = torch.stack(d7_rec, dim=4)                                              #torch.Size([250, 32, 16, 16, 5])\n",
        "#         # print(\"-----------------length of torch.stack(d7_rec, dim=4)\",len(d7_rec))\n",
        "#         # print(\"------size of torch.stack(d7_rec, dim=4)\",d7_rec.size())\n",
        "\n",
        "#         d9_rec = torch.stack(d9_rec, dim=4)\n",
        "#         # print(\"-----------------length of torch.stack(d9_rec, dim=4)\",len(d9_rec))\n",
        "#         # print(\"------size of torch.stack(d9_rec, dim=4)\",d9_rec.size())                  #size of torch.stack(d9_rec, dim=4) torch.Size([250, 1, 32, 32, 5])\n",
        "#         d9_rec_mem = torch.stack(d9_rec_mem, dim=4)\n",
        "\n",
        "#        # decoder\n",
        "#         d1_rec = d1_rec[:, :, :, :, -1]\n",
        "#         # print(\"=========  d1_rec[:, :, :, :, -1].size()\", d1_rec.size())              #   d1_rec[:, :, :, :, -1].size() torch.Size([250, 128, 4, 4])\n",
        "#         d4_rec = d4_rec[:, :, :, :, -1]\n",
        "#         # print(\"=========  d4_rec[:, :, :, :, -1].size()\", d4_rec.size())              #   d4_rec[:, :, :, :, -1].size() torch.Size([250, 64, 8, 8])\n",
        "#         d7_rec = d7_rec[:, :, :, :, -1]\n",
        "#         # print(\"=========  d7_rec[:, :, :, :, -1].size()\", d7_rec.size())              #   d7_rec[:, :, :, :, -1].size() torch.Size([250, 32, 16, 16])\n",
        "#         d9_rec = d9_rec[:, :, :, :, -1]\n",
        "#         # print(\"=========  d9_rec[:, :, :, :, -1].size()\", d9_rec.size())              #   d9_rec[:, :, :, :, -1].size() torch.Size([250, 1, 32, 32])\n",
        "#         out = d9_rec_mem[:, :, :, :, -1]\n",
        "\n",
        "#         # self.out_en = out_en\n",
        "#         # self.out = out\n",
        "\n",
        "#         return d9_rec, out_en\n",
        "\n",
        "#     def get_activation(self, name):\n",
        "#         def hook(module, input, output):\n",
        "#             setattr(self, name, output)  # Store the output as an attribute of the model\n",
        "#         return hook\n",
        "\n",
        "\n",
        "# # activation = {}\n",
        "# #     def get_activation(self, name):\n",
        "# #         def hook(model, input, output):\n",
        "# #             activation[name] = output.detach()\n",
        "# #         return hook\n"
      ],
      "metadata": {
        "id": "AIVR3KS_XFcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#              # Encoder\n",
        "#         self.enc0=    nn.Conv2d(1, 32, 3, padding=1, stride=2)\n",
        "#         self.enc1=    nn.BatchNorm2d(32)\n",
        "#         self.enc2=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, threshold=thresh)\n",
        "#         self.enc3=    nn.Conv2d(32, 64, 3, padding=1, stride=2)\n",
        "#         self.enc4=    nn.BatchNorm2d(64)\n",
        "#         self.enc5=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True , threshold=thresh)\n",
        "#         self.enc6=    nn.Conv2d(64, 128, 3, padding=1, stride=2)\n",
        "#         self.enc7=    nn.BatchNorm2d(128)\n",
        "#         self.enc8=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, threshold=thresh)\n",
        "#         self.enc9=    nn.Flatten(start_dim=1, end_dim=3)\n",
        "#         self.enc10=   nn.Linear(128 * 4 * 4, latent_dim)\n",
        "#         self.enc11=   snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "\n",
        "#         self.latent_dim = latent_dim\n",
        "\n",
        "#         self.linearNet0= nn.Linear(latent_dim,128*4*4)\n",
        "#         self.linearNet1=snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "\n",
        "\n",
        "#         # Decoder:\n",
        "#         self.dec0=    nn.Unflatten(1,(128,4,4)) #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "#         self.dec1=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh)\n",
        "#         self.dec2=    nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec3=    nn.BatchNorm2d(64)\n",
        "#         self.dec4=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, threshold=thresh)\n",
        "#         self.dec5=    nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec6=    nn.BatchNorm2d(32)\n",
        "#         self.dec7=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, threshold=thresh)\n",
        "#         self.dec8=    nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec9=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#        # self.dec9=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=thresh) #---------------------------------------------- ADDED\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         utils.reset(self.enc0)\n",
        "#         utils.reset(self.enc1)\n",
        "#         utils.reset(self.enc2)\n",
        "#         utils.reset(self.enc3)\n",
        "#         utils.reset(self.enc4)\n",
        "#         utils.reset(self.enc5)\n",
        "#         utils.reset(self.enc6)\n",
        "#         utils.reset(self.enc7)\n",
        "#         utils.reset(self.enc8)\n",
        "#         utils.reset(self.enc9)\n",
        "#         utils.reset(self.enc10)\n",
        "#         utils.reset(self.enc11)\n",
        "#         utils.reset(self.linearNet0)\n",
        "#         utils.reset(self.linearNet1)\n",
        "#         utils.reset(self.dec0)\n",
        "#         utils.reset(self.dec1)\n",
        "#         utils.reset(self.dec2)\n",
        "#         utils.reset(self.dec3)\n",
        "#         utils.reset(self.dec4)\n",
        "#         utils.reset(self.dec5)\n",
        "#         utils.reset(self.dec6)\n",
        "#         utils.reset(self.dec7)\n",
        "#         utils.reset(self.dec8)\n",
        "#         utils.reset(self.dec9)\n",
        "\n",
        "\n",
        "#         x2_rec = []\n",
        "#         x5_rec = []\n",
        "#         x8_rec = []\n",
        "#         x11_rec = []\n",
        "\n",
        "#         d1_rec = []\n",
        "#         d4_rec = []\n",
        "#         d7_rec = []\n",
        "#         d9_rec = []\n",
        "#         d9_rec_mem = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             #------------------------------ encoder:\n",
        "#             x0=self.enc0(x)\n",
        "#             x1=self.enc1(x0)\n",
        "#             x2=self.enc2(x1) #Leaky\n",
        "#             x3=self.enc3(x2)\n",
        "#             x4=self.enc4(x3)\n",
        "#             x5, x5mem =self.enc5(x4) #Leaky\n",
        "#             x6=self.enc6(x5)\n",
        "#             x7=self.enc7(x6)\n",
        "#             x8=self.enc8(x7) #Leaky\n",
        "#             x9=self.enc9(x8)\n",
        "#             x10=self.enc10(x9)\n",
        "#             x11=self.enc11(x10) #Leaky\n",
        "\n",
        "#             x2_rec.append(x2)\n",
        "#             x5_rec.append(x5)\n",
        "#             x8_rec.append(x8)\n",
        "#             x11_rec.append(x11[0])\n",
        "\n",
        "#             #-------------------------------------------------\n",
        "#         x2_rec = torch.stack(x2_rec, dim=2)\n",
        "#         x5_rec = torch.stack(x5_rec, dim=2)\n",
        "#         x8_rec = torch.stack(x8_rec, dim=2)\n",
        "#         x11_rec = torch.stack(x11_rec, dim=2)                         #\n",
        "#         x2_rec = x2_rec[:, :, -1]\n",
        "#         x5_rec = x5_rec[:, :, -1]\n",
        "#         x8_rec = x8_rec[:, :, -1]\n",
        "#         out_en = x11_rec[...,step]\n",
        "#         # x11_rec = x11_rec[:, :, -1]             #  =========  x11_rec[:, :, -1].size() torch.Size([250, 32])\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             #------------------------------ latent:\n",
        "#             LN0=self.linearNet0(x11_rec[...,step])\n",
        "#             LN1=self.linearNet1(LN0)\n",
        "#             #------------------------------ decoder:\n",
        "#             # d0=self.dec0(LN1[...,step])\n",
        "#             d0=self.dec0(LN1[0])\n",
        "#             d1=self.dec1(d0)     #Leaky\n",
        "#             d2=self.dec2(d1)\n",
        "#             d3=self.dec3(d2)\n",
        "#             d4=self.dec4(d3)     #Leaky\n",
        "#             d5=self.dec5(d4)\n",
        "#             d6=self.dec6(d5)\n",
        "#             d7=self.dec7(d6)     #Leaky\n",
        "#             d8=self.dec8(d7)\n",
        "#             d9=self.dec9(d8)     #Leaky\n",
        "#             #----------------------------------------\n",
        "\n",
        "#             #-----------decoder:\n",
        "#             d1_rec.append(d1)\n",
        "#             d4_rec.append(d4)\n",
        "#             d7_rec.append(d7)\n",
        "#             d9_rec.append(d9[0])\n",
        "#             d9_rec_mem.append(d9[1])\n",
        "\n",
        "\n",
        "#        # decoder\n",
        "#         d1_rec = torch.stack(d1_rec, dim=4)\n",
        "#         d4_rec = torch.stack(d4_rec, dim=4)\n",
        "#         d7_rec = torch.stack(d7_rec, dim=4)\n",
        "#         d9_rec = torch.stack(d9_rec, dim=4)\n",
        "#         d9_rec_mem = torch.stack(d9_rec_mem, dim=4)\n",
        "\n",
        "#        # decoder\n",
        "#         d1_rec = d1_rec[:, :, :, :, -1]\n",
        "#         d4_rec = d4_rec[:, :, :, :, -1]\n",
        "#         d7_rec = d7_rec[:, :, :, :, -1]\n",
        "#         d9_rec = d9_rec[:, :, :, :, -1]\n",
        "#         out = d9_rec_mem[:, :, :, :, -1]\n",
        "\n",
        "#         # self.out_en = out_en\n",
        "#         # self.out = out\n",
        "\n",
        "#         return d9_rec, out_en\n",
        "\n",
        "#     # def get_activation(self, name):\n",
        "#     #     def hook(module, input, output):\n",
        "#     #         setattr(self, name, output)  # Store the output as an attribute of the model\n",
        "#     #     return hook"
      ],
      "metadata": {
        "id": "_C76N8tQJmcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for each layer"
      ],
      "metadata": {
        "id": "9q4mUUpWdF_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training and Testing\n",
        "# from torchvision.utils import save_image\n",
        "\n",
        "# spike_recordings = []\n",
        "# train_ber_rec = []\n",
        "# test_ber_rec = []\n",
        "# threshold_Real = 0.5\n",
        "# threshold_Recon = 0.5\n",
        "\n",
        "# def train(network, trainloader, opti, epoch):\n",
        "#     network=network.train()\n",
        "#     train_loss_hist=torch.zeros((1), dtype=dtype, device=device)\n",
        "#     train_avg_loss_rec=[]\n",
        "\n",
        "#     for batch_idx, (real_img, labels) in enumerate(trainloader):\n",
        "#         opti.zero_grad()\n",
        "#         real_img = real_img.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         # print(\"real_img size\", real_img.size())      #    real_img size                # -------------------------------------------------ADDED\n",
        "#         out, out_en = network(real_img)\n",
        "#         x_recon, out_en = network(real_img)\n",
        "#         # print(\"out_en size\", out_en.size())      #                  # -------------------------------------------------ADDED\n",
        "#         # print(\"x_recon size\", x_recon.size())      #                  # -------------------------------------------------ADDED\n",
        "\n",
        "\n",
        "#         #Calculate loss\n",
        "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "#         for step in range(num_steps):\n",
        "#           loss_val += F.mse_loss(x_recon, real_img)                  #.view(1, -1)\n",
        "\n",
        "#         train_loss_hist += (loss_val.item())/num_steps\n",
        "#         avg_loss=train_loss_hist.mean()\n",
        "\n",
        "#         # # # ---------------------\n",
        "#         print(f'Train[{epoch}/{max_epoch}][{batch_idx}/{len(trainloader)}] Loss: {loss_val.item()}')\n",
        "\n",
        "#         loss_val.backward()        #\n",
        "\n",
        "#         opti.step()\n",
        "#         train_loss_rec.append(loss_val.item())\n",
        "\n",
        "#         #Save reconstructed images every at the end of the epoch\n",
        "#         if batch_idx == len(trainloader)-1:\n",
        "#             utls.save_image((real_img+1)/2, f'figures/training/epoch{epoch}_finalbatch_inputs.png')\n",
        "#             utls.save_image((x_recon+1)/2, f'figures/training/epoch{epoch}_finalbatch_recon.png')\n",
        "#             train_auc = auc(np.arange(len(train_loss_rec)), train_loss_rec)\n",
        "\n",
        "#     # return loss_val, train_loss_rec, train_auc, d9_rec, out_en  #              # -------------------------------------------------ADDED\n",
        "#     return loss_val, train_loss_rec, train_auc, out, out_en  #              # -------------------------------------------------ADDED\n",
        "\n",
        "\n",
        "# # For Testing, not doing backpropagate, therefore no gradients are required and we use torch.no_grad():\n",
        "# #Testing Loop\n",
        "# def test(network, testloader, opti, epoch):\n",
        "#     network=network.eval()\n",
        "#     test_loss_hist=[]\n",
        "#     test_avg_loss_rec=[]\n",
        "#     test_avg_loss_hist = []\n",
        "\n",
        "#     spk_rec_test = []\n",
        "#     with torch.no_grad(): #no gradient this time\n",
        "#         for batch_idx, (real_img, labels) in enumerate(testloader):\n",
        "#             real_img = real_img.to(device)#\n",
        "#             labels = labels.to(device)\n",
        "#             # x11_rec, d9_rec = network(real_img)             # -------------------------------------------------ADDED\n",
        "#             out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#             x_recon, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#             # average Loss:\n",
        "#             loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "#             for step in range(num_steps):\n",
        "#               loss_val += F.mse_loss(x_recon, real_img)\n",
        "#             avg_loss=loss_val/num_steps\n",
        "#             test_loss_hist.append(loss_val.item())\n",
        "\n",
        "#             real_img_binary = (real_img > threshold_Real).float()\n",
        "#             x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "#             bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "#             total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "#             bit_error_rate = bit_errors.item() / total_pixels\n",
        "#             test_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "#             # Save binary images\n",
        "#             Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "#             if batch_idx == len(testloader)-1:\n",
        "#               if epoch in [0, 25, 49]:\n",
        "#                 save_image(real_img_binary, f'figures/binarytesting/ep{epoch}_inputs_binary.png')\n",
        "#                 save_image(x_recon_binary, f'figures/binarytesting/ep{epoch}_recon_binary.png')\n",
        "#                 save_image(Error_bin, f'figures/binarytesting/ep{epoch}_Error_bin.png')\n",
        "#             # -------------------------------------------------------------------------------------------------\n",
        "\n",
        "#             print(f'Test[{epoch}/{max_epoch}][{batch_idx}/{len(testloader)}]  Loss: {loss_val.item()}, '  f'BER (test): {bit_error_rate}')\n",
        "\n",
        "#             test_loss_rec.append(loss_val.item())\n",
        "\n",
        "#             if batch_idx == len(testloader)-1:\n",
        "#                 utls.save_image((real_img+1)/2, f'figures/testing/epoch{epoch}_finalbatch_inputs.png')\n",
        "#                 utls.save_image((x_recon+1)/2, f'figures/testing/epoch{epoch}_finalbatch_recons.png')\n",
        "#                 test_auc = auc(np.arange(len(test_loss_rec)), test_loss_rec)\n",
        "\n",
        "#     # return loss_val, test_loss_rec, test_auc, out, out_en                     # -------------------------------------------------ADDED\n",
        "#     # return loss_val, test_loss_rec, test_auc, x11_rec, d9_rec  #              # -------------------------------------------------ADDED\n",
        "#     return loss_val, test_loss_rec, test_auc, out, out_en\n",
        "\n"
      ],
      "metadata": {
        "id": "PGWPHsrmPW9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training and Testing\n",
        "# # using MSE loss to compare the reconstructed image (x_recon) with the original image (real_img)\n",
        "# from torchvision.utils import save_image\n",
        "\n",
        "# spike_recordings = []\n",
        "# train_ber_rec = []\n",
        "# test_ber_rec = []\n",
        "# threshold_Real = 0.5\n",
        "# threshold_Recon = 0.5\n",
        "\n",
        "# def train(network, trainloader, opti, epoch):\n",
        "#     network=network.train()\n",
        "#     train_loss_hist=torch.zeros((1), dtype=dtype, device=device)\n",
        "#     train_avg_loss_rec=[]\n",
        "\n",
        "#     for batch_idx, (real_img, labels) in enumerate(trainloader):\n",
        "#         opti.zero_grad()\n",
        "#         real_img = real_img.to(device)\n",
        "#         labels = labels.to(device)\n",
        "#         # d9_rec, out_en = network(real_img)             # -------------------------------------------------ADDED\n",
        "#         # print(\"real_img size\", real_img.size())      #    real_img size torch.Size([250, 1, 32, 32])                # -------------------------------------------------ADDED\n",
        "\n",
        "#         # out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#         x_recon, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#         # print(\"x_recon size\", x_recon.size())      #       x_recon size torch.Size([250, 1, 32, 32])\n",
        "#         # print(\"out size\", out.size())             #       out size torch.Size([250, 32])\n",
        "\n",
        "#         #Calculate loss\n",
        "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "#         for step in range(num_steps):\n",
        "#           loss_val += F.mse_loss(x_recon, real_img)                  #.view(1, -1)\n",
        "\n",
        "#             # Clone the loss_val tensor to avoid in-place modification\n",
        "\n",
        "#         train_loss_hist += (loss_val.item())/num_steps\n",
        "#         avg_loss=train_loss_hist.mean()\n",
        "\n",
        "#         # # ---------------------------- Calculate Bit Error Rate (BER)\n",
        "#         real_img_binary = (real_img > threshold_Real).float()\n",
        "#         x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "#         bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "#         total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "#         bit_error_rate = bit_errors.item() / total_pixels\n",
        "#         train_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "#         # Save binary images\n",
        "#         Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "#         if batch_idx == len(trainloader)-1:\n",
        "#           if epoch in [0, 25, 49]:\n",
        "#             utls.save_image(real_img_binary, f'figures/binarytraining/ep{epoch}_inputs_binary.png')\n",
        "#             utls.save_image(x_recon_binary, f'figures/binarytraining/ep{epoch}_recon_binary.png')\n",
        "#             utls.save_image(Error_bin, f'figures/binarytraining/ep{epoch}_Error_bin.png')\n",
        "#         print(f'Train[{epoch}/{max_epoch}][{batch_idx}/{len(trainloader)}] Loss: {loss_val.item()}, ' f'BER : {bit_error_rate}')\n",
        "\n",
        "#         # loss_val += torch.mean(loss_val)  # Accumulate the loss             # -------------------------------------------------ADDED\n",
        "#         loss_val.backward()         #\n",
        "#         # loss_val.backward(retain_graph=True)        #\n",
        "\n",
        "#         opti.step()\n",
        "#         train_loss_rec.append(loss_val.item())\n",
        "\n",
        "#         #Save reconstructed images every at the end of the epoch\n",
        "#         if batch_idx == len(trainloader)-1:\n",
        "#             utls.save_image((real_img+1)/2, f'figures/training/epoch{epoch}_finalbatch_inputs.png')\n",
        "#             utls.save_image((x_recon+1)/2, f'figures/training/epoch{epoch}_finalbatch_recon.png')\n",
        "#             train_auc = auc(np.arange(len(train_loss_rec)), train_loss_rec)\n",
        "\n",
        "#     # loss_val.backward()                                            # -------------------------------------------------ADDED\n",
        "\n",
        "#     # return loss_val, train_loss_rec, train_auc , out, out_en  #, spk_rec_batches#, train_avg_loss_rec, #avg_loss #, train_loss_hist\n",
        "#     # return loss_val, train_loss_rec, train_auc   #\n",
        "#     return loss_val, train_loss_rec, train_auc, d9_rec, out_en  #              # -------------------------------------------------ADDED\n",
        "\n",
        "\n",
        "# # For Testing, not doing backpropagate, therefore no gradients are required and we use torch.no_grad():\n",
        "# #Testing Loop\n",
        "# def test(network, testloader, opti, epoch):\n",
        "#     network=network.eval()\n",
        "#     test_loss_hist=[]\n",
        "#     test_avg_loss_rec=[]\n",
        "#     test_avg_loss_hist = []\n",
        "\n",
        "#     spk_rec_test = []\n",
        "#     with torch.no_grad(): #no gradient this time\n",
        "#         for batch_idx, (real_img, labels) in enumerate(testloader):\n",
        "#             real_img = real_img.to(device)#\n",
        "#             labels = labels.to(device)\n",
        "#             # x11_rec, d9_rec = network(real_img)             # -------------------------------------------------ADDED\n",
        "#             # out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#             x_recon, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#             # average Loss:\n",
        "#             loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "#             for step in range(num_steps):\n",
        "#               loss_val += F.mse_loss(x_recon, real_img)\n",
        "#             avg_loss=loss_val/num_steps\n",
        "#             test_loss_hist.append(loss_val.item())\n",
        "\n",
        "#             real_img_binary = (real_img > threshold_Real).float()\n",
        "#             x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "#             bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "#             total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "#             bit_error_rate = bit_errors.item() / total_pixels\n",
        "#             test_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "#             # Save binary images\n",
        "#             Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "#             if batch_idx == len(testloader)-1:\n",
        "#               if epoch in [0, 25, 49]:\n",
        "#                 save_image(real_img_binary, f'figures/binarytesting/ep{epoch}_inputs_binary.png')\n",
        "#                 save_image(x_recon_binary, f'figures/binarytesting/ep{epoch}_recon_binary.png')\n",
        "#                 save_image(Error_bin, f'figures/binarytesting/ep{epoch}_Error_bin.png')\n",
        "#             # -------------------------------------------------------------------------------------------------\n",
        "\n",
        "#             print(f'Test[{epoch}/{max_epoch}][{batch_idx}/{len(testloader)}]  Loss: {loss_val.item()}, '  f'BER (test): {bit_error_rate}')\n",
        "\n",
        "#             test_loss_rec.append(loss_val.item())\n",
        "\n",
        "#             if batch_idx == len(testloader)-1:\n",
        "#                 utls.save_image((real_img+1)/2, f'figures/testing/epoch{epoch}_finalbatch_inputs.png')\n",
        "#                 utls.save_image((x_recon+1)/2, f'figures/testing/epoch{epoch}_finalbatch_recons.png')\n",
        "#                 test_auc = auc(np.arange(len(test_loss_rec)), test_loss_rec)\n",
        "\n",
        "#     # return loss_val, test_loss_rec, test_auc, out, out_en                     # -------------------------------------------------ADDED\n",
        "#     # return loss_val, test_loss_rec, test_auc, x11_rec, d9_rec  #              # -------------------------------------------------ADDED\n",
        "#     return loss_val, test_loss_rec, test_auc  #\n",
        "\n"
      ],
      "metadata": {
        "id": "HmvBPb95dEju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HbcSNNp6vK2"
      },
      "outputs": [],
      "source": [
        "# Training and Testing\n",
        "# using MSE loss to compare the reconstructed image (x_recon) with the original image (real_img)\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "spike_recordings = []\n",
        "train_ber_rec = []\n",
        "test_ber_rec = []\n",
        "threshold_Real = 0.5\n",
        "threshold_Recon = 0.5\n",
        "\n",
        "def train(network, trainloader, opti, epoch):\n",
        "    network=network.train()\n",
        "    train_loss_hist=torch.zeros((1), dtype=dtype, device=device)\n",
        "    train_avg_loss_rec=[]\n",
        "\n",
        "    for batch_idx, (real_img, labels) in enumerate(trainloader):\n",
        "        opti.zero_grad()\n",
        "        real_img = real_img.to(device)\n",
        "        labels = labels.to(device)\n",
        "        out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "        x_recon, out = network(real_img)  # Pass data into network and return reconstructed image and spk_rec.  #        x_recon size torch.Size([250, 1, 32, 32]) ,  #        out size torch.Size([250, 32])\n",
        "        #Calculate loss\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "        for step in range(num_steps):\n",
        "          loss_val += F.mse_loss(x_recon, real_img)                  #.view(1, -1)\n",
        "        train_loss_hist += (loss_val.item())/num_steps\n",
        "        avg_loss=train_loss_hist.mean()\n",
        "\n",
        "        # # ---------------------------- Calculate Bit Error Rate (BER)\n",
        "        real_img_binary = (real_img > threshold_Real).float()\n",
        "        x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "        bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "        total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "        bit_error_rate = bit_errors.item() / total_pixels\n",
        "        train_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "        # Save binary images\n",
        "        Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "        if batch_idx == len(trainloader)-1:\n",
        "          if epoch in [0, 25, 49]:\n",
        "            utls.save_image(real_img_binary, f'figures/binarytraining/ep{epoch}_inputs_binary.png')\n",
        "            utls.save_image(x_recon_binary, f'figures/binarytraining/ep{epoch}_recon_binary.png')\n",
        "            utls.save_image(Error_bin, f'figures/binarytraining/ep{epoch}_Error_bin.png')\n",
        "        print(f'Train[{epoch}/{max_epoch}][{batch_idx}/{len(trainloader)}] Loss: {loss_val.item()}, ' f'BER : {bit_error_rate}')\n",
        "\n",
        "        loss_val.backward()\n",
        "        opti.step()\n",
        "        train_loss_rec.append(loss_val.item())\n",
        "\n",
        "        #Save reconstructed images every at the end of the epoch\n",
        "        if batch_idx == len(trainloader)-1:\n",
        "            utls.save_image((real_img+1)/2, f'figures/training/epoch{epoch}_finalbatch_inputs.png')\n",
        "            utls.save_image((x_recon+1)/2, f'figures/training/epoch{epoch}_finalbatch_recon.png')\n",
        "            train_auc = auc(np.arange(len(train_loss_rec)), train_loss_rec)\n",
        "\n",
        "    return loss_val, train_loss_rec, train_auc , out, out_en  #, spk_rec_batches#, train_avg_loss_rec, #avg_loss #, train_loss_hist\n",
        "    # return loss_val, train_loss_rec, train_auc   #\n",
        "\n",
        "\n",
        "# For Testing, not doing backpropagate, therefore no gradients are required and we use torch.no_grad():\n",
        "#Testing Loop\n",
        "def test(network, testloader, opti, epoch):\n",
        "    network=network.eval()\n",
        "    test_loss_hist=[]\n",
        "    test_avg_loss_rec=[]\n",
        "    test_avg_loss_hist = []\n",
        "\n",
        "    spk_rec_test = []\n",
        "    with torch.no_grad(): #no gradient this time\n",
        "        for batch_idx, (real_img, labels) in enumerate(testloader):\n",
        "            real_img = real_img.to(device)#\n",
        "            labels = labels.to(device)\n",
        "            out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "            x_recon , out = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "            # average Loss:\n",
        "            loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "            for step in range(num_steps):\n",
        "              loss_val += F.mse_loss(x_recon, real_img)\n",
        "            avg_loss=loss_val/num_steps\n",
        "            test_loss_hist.append(loss_val.item())\n",
        "\n",
        "            real_img_binary = (real_img > threshold_Real).float()\n",
        "            x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "            bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "            total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "            bit_error_rate = bit_errors.item() / total_pixels\n",
        "            test_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "            # Save binary images\n",
        "            Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "            if batch_idx == len(testloader)-1:\n",
        "              if epoch in [0, 25, 49]:\n",
        "                save_image(real_img_binary, f'figures/binarytesting/ep{epoch}_inputs_binary.png')\n",
        "                save_image(x_recon_binary, f'figures/binarytesting/ep{epoch}_recon_binary.png')\n",
        "                save_image(Error_bin, f'figures/binarytesting/ep{epoch}_Error_bin.png')\n",
        "            # -------------------------------------------------------------------------------------------------\n",
        "\n",
        "            print(f'Test[{epoch}/{max_epoch}][{batch_idx}/{len(testloader)}]  Loss: {loss_val.item()}, '  f'BER (test): {bit_error_rate}')\n",
        "\n",
        "            test_loss_rec.append(loss_val.item())\n",
        "\n",
        "            if batch_idx == len(testloader)-1:\n",
        "                utls.save_image((real_img+1)/2, f'figures/testing/epoch{epoch}_finalbatch_inputs.png')\n",
        "                utls.save_image((x_recon+1)/2, f'figures/testing/epoch{epoch}_finalbatch_recons.png')\n",
        "                test_auc = auc(np.arange(len(test_loss_rec)), test_loss_rec)\n",
        "\n",
        "    return loss_val, test_loss_rec, test_auc, out, out_en                     # -------------------------------------------------ADDED\n",
        "    # return loss_val, test_loss_rec, test_auc  #\n",
        "\n",
        "for batch_spikes in spike_recordings:\n",
        "    print(batch_spikes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF2cwM20PKAL"
      },
      "outputs": [],
      "source": [
        "input_size = 32 #resize of mnist data (optional)\n",
        "\n",
        "#setup GPU\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# neuron and simulation parameters\n",
        "spike_grad = surrogate.atan(alpha=2.0)  # alternate surrogate gradient fast_sigmoid(slope=25)\n",
        "\n",
        "train_loss_rec = []\n",
        "test_loss_rec = []\n",
        "train_loss_record = []\n",
        "test_loss_record = []\n",
        "train_avg_loss_rec=[]\n",
        "test_avg_loss_rec=[]\n",
        "\n",
        "  # Synaptic current and membrane potential decay exponentially with rates of alpha and beta\n",
        "alpha=0.9\n",
        "beta_syn=0.0001\n",
        "# beta_syn=0.9\n",
        "\n",
        "beta =0.9\n",
        "\n",
        "num_steps=5\n",
        "latent_dim = 32 #dimension of latent layer (how compressed we want the information)\n",
        "thresh=1    #spiking threshold (lower = more spikes are let through)\n",
        "epochs=50\n",
        "# epochs=5\n",
        "max_epoch=epochs\n",
        "\n",
        "  #Define Network and optimizer\n",
        "net=SAE()\n",
        "net = net.to(device)\n",
        "optimizer = torch.optim.AdamW(net.parameters(),\n",
        "                            lr=0.0001,\n",
        "                            betas=(0.9, 0.999),\n",
        "                            weight_decay=0.001)\n",
        "\n",
        "\n",
        "\n",
        "activation = {}\n",
        "# def get_activation(name):\n",
        "#     def hook(model, input, output):\n",
        "#         activation[name] = output.detach()\n",
        "#     return hook\n",
        "\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        if isinstance(output, tuple):\n",
        "            activation[name] = [out.detach() for out in output]\n",
        "        else:\n",
        "            activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "\n",
        "# net.encoder[5].register_forward_hook(get_activation('encoder[5]'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2hQ0wiOxUr"
      },
      "source": [
        "## for saving the out_en after each epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUOjPTrBOjmt",
        "outputId": "e9f1311a-ea0d-4a12-c622-df4c8723174b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train[0/3][0/240] Loss: 0.6051520109176636, BER : 0.1281484375\n",
            "Train[0/3][1/240] Loss: 0.6221926212310791, BER : 0.13150390625\n",
            "Train[0/3][2/240] Loss: 0.6240261197090149, BER : 0.1310859375\n",
            "Train[0/3][3/240] Loss: 0.6404139995574951, BER : 0.13375390625\n",
            "Train[0/3][4/240] Loss: 0.7125463485717773, BER : 0.13743359375\n",
            "Train[0/3][5/240] Loss: 0.8500621318817139, BER : 0.1411875\n",
            "Train[0/3][6/240] Loss: 1.0110933780670166, BER : 0.15303515625\n",
            "Train[0/3][7/240] Loss: 1.021156907081604, BER : 0.150640625\n",
            "Train[0/3][8/240] Loss: 0.997146725654602, BER : 0.14925\n",
            "Train[0/3][9/240] Loss: 0.8551381826400757, BER : 0.1416015625\n",
            "Train[0/3][10/240] Loss: 0.7665982246398926, BER : 0.13877734375\n",
            "Train[0/3][11/240] Loss: 0.7022441029548645, BER : 0.132296875\n",
            "Train[0/3][12/240] Loss: 0.7101943492889404, BER : 0.13644921875\n",
            "Train[0/3][13/240] Loss: 0.7256504893302917, BER : 0.1354296875\n",
            "Train[0/3][14/240] Loss: 0.8732196092605591, BER : 0.14524609375\n",
            "Train[0/3][15/240] Loss: 0.8227789998054504, BER : 0.14209765625\n",
            "Train[0/3][16/240] Loss: 0.7733208537101746, BER : 0.1362578125\n",
            "Train[0/3][17/240] Loss: 0.7862025499343872, BER : 0.144359375\n",
            "Train[0/3][18/240] Loss: 0.8485956192016602, BER : 0.147390625\n",
            "Train[0/3][19/240] Loss: 0.8253930807113647, BER : 0.1429140625\n",
            "Train[0/3][20/240] Loss: 0.9236584901809692, BER : 0.151796875\n",
            "Train[0/3][21/240] Loss: 0.9230853319168091, BER : 0.14973046875\n",
            "Train[0/3][22/240] Loss: 0.8710893988609314, BER : 0.1498828125\n",
            "Train[0/3][23/240] Loss: 0.9076521396636963, BER : 0.14809765625\n",
            "Train[0/3][24/240] Loss: 0.9563690423965454, BER : 0.15311328125\n",
            "Train[0/3][25/240] Loss: 0.8959437608718872, BER : 0.15114453125\n",
            "Train[0/3][26/240] Loss: 0.9483879208564758, BER : 0.15052734375\n",
            "Train[0/3][27/240] Loss: 0.9293634295463562, BER : 0.14788671875\n",
            "Train[0/3][28/240] Loss: 0.9152786731719971, BER : 0.14733203125\n",
            "Train[0/3][29/240] Loss: 0.8994038105010986, BER : 0.1481328125\n",
            "Train[0/3][30/240] Loss: 0.8545111417770386, BER : 0.14659375\n",
            "Train[0/3][31/240] Loss: 0.8261399269104004, BER : 0.142234375\n",
            "Train[0/3][32/240] Loss: 0.8984435796737671, BER : 0.14731640625\n",
            "Train[0/3][33/240] Loss: 0.95444256067276, BER : 0.1549296875\n",
            "Train[0/3][34/240] Loss: 1.0281624794006348, BER : 0.1601953125\n",
            "Train[0/3][35/240] Loss: 1.0900875329971313, BER : 0.16446484375\n",
            "Train[0/3][36/240] Loss: 0.982711672782898, BER : 0.152921875\n",
            "Train[0/3][37/240] Loss: 0.905221164226532, BER : 0.1516015625\n",
            "Train[0/3][38/240] Loss: 0.8848900198936462, BER : 0.15215625\n",
            "Train[0/3][39/240] Loss: 1.0053637027740479, BER : 0.15993359375\n",
            "Train[0/3][40/240] Loss: 0.9845548272132874, BER : 0.15628515625\n",
            "Train[0/3][41/240] Loss: 0.9279965758323669, BER : 0.1494765625\n",
            "Train[0/3][42/240] Loss: 0.9788092970848083, BER : 0.154796875\n",
            "Train[0/3][43/240] Loss: 1.0230929851531982, BER : 0.1626640625\n",
            "Train[0/3][44/240] Loss: 1.0637478828430176, BER : 0.1649375\n",
            "Train[0/3][45/240] Loss: 0.9237703680992126, BER : 0.1535625\n",
            "Train[0/3][46/240] Loss: 0.9118431210517883, BER : 0.15255859375\n",
            "Train[0/3][47/240] Loss: 1.0042439699172974, BER : 0.16555859375\n",
            "Train[0/3][48/240] Loss: 0.93662428855896, BER : 0.15940625\n",
            "Train[0/3][49/240] Loss: 0.9389908313751221, BER : 0.15813671875\n",
            "Train[0/3][50/240] Loss: 0.9149842262268066, BER : 0.15537109375\n",
            "Train[0/3][51/240] Loss: 0.9896566867828369, BER : 0.16184375\n",
            "Train[0/3][52/240] Loss: 0.9417210817337036, BER : 0.15961328125\n",
            "Train[0/3][53/240] Loss: 1.0138365030288696, BER : 0.16691796875\n",
            "Train[0/3][54/240] Loss: 1.012025237083435, BER : 0.16361328125\n",
            "Train[0/3][55/240] Loss: 1.0823372602462769, BER : 0.1713125\n",
            "Train[0/3][56/240] Loss: 1.030984878540039, BER : 0.16888671875\n",
            "Train[0/3][57/240] Loss: 0.9793704748153687, BER : 0.16697265625\n",
            "Train[0/3][58/240] Loss: 0.9467548131942749, BER : 0.16457421875\n",
            "Train[0/3][59/240] Loss: 1.0698809623718262, BER : 0.17059375\n",
            "Train[0/3][60/240] Loss: 1.0079386234283447, BER : 0.16896875\n",
            "Train[0/3][61/240] Loss: 0.9330565929412842, BER : 0.16189453125\n",
            "Train[0/3][62/240] Loss: 0.9733372926712036, BER : 0.16724609375\n",
            "Train[0/3][63/240] Loss: 0.9706946611404419, BER : 0.165359375\n",
            "Train[0/3][64/240] Loss: 1.0041099786758423, BER : 0.1689375\n",
            "Train[0/3][65/240] Loss: 1.083534598350525, BER : 0.17684765625\n",
            "Train[0/3][66/240] Loss: 1.0113885402679443, BER : 0.16960546875\n",
            "Train[0/3][67/240] Loss: 1.0046236515045166, BER : 0.17103515625\n",
            "Train[0/3][68/240] Loss: 0.9909861087799072, BER : 0.1693046875\n",
            "Train[0/3][69/240] Loss: 1.1224453449249268, BER : 0.1872265625\n",
            "Train[0/3][70/240] Loss: 1.104293704032898, BER : 0.1843125\n",
            "Train[0/3][71/240] Loss: 1.0400118827819824, BER : 0.17846484375\n",
            "Train[0/3][72/240] Loss: 1.0681087970733643, BER : 0.18130078125\n",
            "Train[0/3][73/240] Loss: 1.0651049613952637, BER : 0.1831328125\n",
            "Train[0/3][74/240] Loss: 1.0458627939224243, BER : 0.18070703125\n",
            "Train[0/3][75/240] Loss: 1.0319973230361938, BER : 0.179390625\n",
            "Train[0/3][76/240] Loss: 1.1285239458084106, BER : 0.18932421875\n",
            "Train[0/3][77/240] Loss: 1.052410364151001, BER : 0.18495703125\n",
            "Train[0/3][78/240] Loss: 1.1242209672927856, BER : 0.1938515625\n",
            "Train[0/3][79/240] Loss: 1.0443774461746216, BER : 0.1854296875\n",
            "Train[0/3][80/240] Loss: 1.07275390625, BER : 0.1881328125\n",
            "Train[0/3][81/240] Loss: 1.0882152318954468, BER : 0.1888515625\n",
            "Train[0/3][82/240] Loss: 1.0949645042419434, BER : 0.1915546875\n",
            "Train[0/3][83/240] Loss: 1.190915822982788, BER : 0.20712890625\n",
            "Train[0/3][84/240] Loss: 1.0511133670806885, BER : 0.1863515625\n",
            "Train[0/3][85/240] Loss: 1.0409623384475708, BER : 0.18883203125\n",
            "Train[0/3][86/240] Loss: 1.0535670518875122, BER : 0.19015625\n",
            "Train[0/3][87/240] Loss: 1.1158808469772339, BER : 0.20108984375\n",
            "Train[0/3][88/240] Loss: 1.0727895498275757, BER : 0.19508984375\n",
            "Train[0/3][89/240] Loss: 1.1469736099243164, BER : 0.2040703125\n",
            "Train[0/3][90/240] Loss: 1.0500578880310059, BER : 0.19380078125\n",
            "Train[0/3][91/240] Loss: 1.1062982082366943, BER : 0.199296875\n",
            "Train[0/3][92/240] Loss: 1.162820816040039, BER : 0.2069296875\n",
            "Train[0/3][93/240] Loss: 1.0416808128356934, BER : 0.19004296875\n",
            "Train[0/3][94/240] Loss: 1.0394463539123535, BER : 0.1924453125\n",
            "Train[0/3][95/240] Loss: 0.997074544429779, BER : 0.18561328125\n",
            "Train[0/3][96/240] Loss: 1.0013492107391357, BER : 0.188109375\n",
            "Train[0/3][97/240] Loss: 1.1078245639801025, BER : 0.20040234375\n",
            "Train[0/3][98/240] Loss: 1.1267366409301758, BER : 0.2053828125\n",
            "Train[0/3][99/240] Loss: 1.0852677822113037, BER : 0.20439453125\n",
            "Train[0/3][100/240] Loss: 1.2227638959884644, BER : 0.2164921875\n",
            "Train[0/3][101/240] Loss: 1.2848844528198242, BER : 0.2251640625\n",
            "Train[0/3][102/240] Loss: 1.1449625492095947, BER : 0.20652734375\n",
            "Train[0/3][103/240] Loss: 1.1445071697235107, BER : 0.20961328125\n",
            "Train[0/3][104/240] Loss: 1.0980147123336792, BER : 0.20451953125\n",
            "Train[0/3][105/240] Loss: 1.1273645162582397, BER : 0.2096875\n",
            "Train[0/3][106/240] Loss: 1.035136342048645, BER : 0.194015625\n",
            "Train[0/3][107/240] Loss: 1.1117689609527588, BER : 0.20741015625\n",
            "Train[0/3][108/240] Loss: 1.2624390125274658, BER : 0.2263671875\n",
            "Train[0/3][109/240] Loss: 1.2313451766967773, BER : 0.224109375\n",
            "Train[0/3][110/240] Loss: 1.0952235460281372, BER : 0.20546875\n",
            "Train[0/3][111/240] Loss: 1.1925456523895264, BER : 0.2199140625\n",
            "Train[0/3][112/240] Loss: 1.1396470069885254, BER : 0.213375\n",
            "Train[0/3][113/240] Loss: 1.052977204322815, BER : 0.20366796875\n",
            "Train[0/3][114/240] Loss: 1.0209239721298218, BER : 0.19705859375\n",
            "Train[0/3][115/240] Loss: 1.128737211227417, BER : 0.214875\n",
            "Train[0/3][116/240] Loss: 1.1598252058029175, BER : 0.21742578125\n",
            "Train[0/3][117/240] Loss: 1.0829260349273682, BER : 0.21067578125\n",
            "Train[0/3][118/240] Loss: 1.1034153699874878, BER : 0.21269140625\n",
            "Train[0/3][119/240] Loss: 1.0686668157577515, BER : 0.20912890625\n",
            "Train[0/3][120/240] Loss: 1.1765350103378296, BER : 0.22433203125\n",
            "Train[0/3][121/240] Loss: 1.0481598377227783, BER : 0.20215234375\n",
            "Train[0/3][122/240] Loss: 1.0233824253082275, BER : 0.200125\n",
            "Train[0/3][123/240] Loss: 1.1221948862075806, BER : 0.21316015625\n",
            "Train[0/3][124/240] Loss: 1.1306475400924683, BER : 0.21212109375\n",
            "Train[0/3][125/240] Loss: 1.113524317741394, BER : 0.212578125\n",
            "Train[0/3][126/240] Loss: 1.0294636487960815, BER : 0.20198046875\n",
            "Train[0/3][127/240] Loss: 0.9718173742294312, BER : 0.1918828125\n",
            "Train[0/3][128/240] Loss: 0.9669917821884155, BER : 0.19326953125\n",
            "Train[0/3][129/240] Loss: 1.0332589149475098, BER : 0.2030390625\n",
            "Train[0/3][130/240] Loss: 1.2093855142593384, BER : 0.228515625\n",
            "Train[0/3][131/240] Loss: 1.2146046161651611, BER : 0.229921875\n",
            "Train[0/3][132/240] Loss: 1.2031476497650146, BER : 0.22883984375\n",
            "Train[0/3][133/240] Loss: 1.181908130645752, BER : 0.22649609375\n",
            "Train[0/3][134/240] Loss: 1.0635087490081787, BER : 0.21008984375\n",
            "Train[0/3][135/240] Loss: 1.0419526100158691, BER : 0.2046171875\n",
            "Train[0/3][136/240] Loss: 0.9756988883018494, BER : 0.1949609375\n",
            "Train[0/3][137/240] Loss: 0.9516385793685913, BER : 0.18960546875\n",
            "Train[0/3][138/240] Loss: 0.9734020233154297, BER : 0.19328515625\n",
            "Train[0/3][139/240] Loss: 0.9846464395523071, BER : 0.1954375\n",
            "Train[0/3][140/240] Loss: 1.030383586883545, BER : 0.20439453125\n",
            "Train[0/3][141/240] Loss: 1.1934877634048462, BER : 0.2297734375\n",
            "Train[0/3][142/240] Loss: 1.1004422903060913, BER : 0.214828125\n",
            "Train[0/3][143/240] Loss: 1.1476941108703613, BER : 0.2230859375\n",
            "Train[0/3][144/240] Loss: 1.1077700853347778, BER : 0.21706640625\n",
            "Train[0/3][145/240] Loss: 1.0842934846878052, BER : 0.2138828125\n",
            "Train[0/3][146/240] Loss: 1.0296263694763184, BER : 0.20355078125\n",
            "Train[0/3][147/240] Loss: 0.9752688407897949, BER : 0.1928828125\n",
            "Train[0/3][148/240] Loss: 0.9616135954856873, BER : 0.191765625\n",
            "Train[0/3][149/240] Loss: 0.9544198513031006, BER : 0.190578125\n",
            "Train[0/3][150/240] Loss: 0.9786468148231506, BER : 0.195953125\n",
            "Train[0/3][151/240] Loss: 0.9796619415283203, BER : 0.19534765625\n",
            "Train[0/3][152/240] Loss: 1.0292611122131348, BER : 0.2039921875\n",
            "Train[0/3][153/240] Loss: 1.0534532070159912, BER : 0.2082109375\n",
            "Train[0/3][154/240] Loss: 1.0815587043762207, BER : 0.209953125\n",
            "Train[0/3][155/240] Loss: 1.1293339729309082, BER : 0.2187421875\n",
            "Train[0/3][156/240] Loss: 1.055438756942749, BER : 0.20732421875\n",
            "Train[0/3][157/240] Loss: 1.0582261085510254, BER : 0.209203125\n",
            "Train[0/3][158/240] Loss: 1.0598905086517334, BER : 0.207703125\n",
            "Train[0/3][159/240] Loss: 1.0253630876541138, BER : 0.20394921875\n",
            "Train[0/3][160/240] Loss: 0.9934782981872559, BER : 0.19964453125\n",
            "Train[0/3][161/240] Loss: 1.015708565711975, BER : 0.20290234375\n",
            "Train[0/3][162/240] Loss: 0.9888043999671936, BER : 0.2000859375\n",
            "Train[0/3][163/240] Loss: 0.9979596734046936, BER : 0.19949609375\n",
            "Train[0/3][164/240] Loss: 1.1054892539978027, BER : 0.21829296875\n",
            "Train[0/3][165/240] Loss: 1.233234167098999, BER : 0.24341796875\n",
            "Train[0/3][166/240] Loss: 1.1364309787750244, BER : 0.220046875\n",
            "Train[0/3][167/240] Loss: 1.2243075370788574, BER : 0.24181640625\n",
            "Train[0/3][168/240] Loss: 1.262976050376892, BER : 0.2484375\n",
            "Train[0/3][169/240] Loss: 1.2177802324295044, BER : 0.23875390625\n",
            "Train[0/3][170/240] Loss: 1.2338874340057373, BER : 0.24262890625\n",
            "Train[0/3][171/240] Loss: 1.2389256954193115, BER : 0.24290234375\n",
            "Train[0/3][172/240] Loss: 1.113917589187622, BER : 0.216421875\n",
            "Train[0/3][173/240] Loss: 1.063554286956787, BER : 0.20834765625\n",
            "Train[0/3][174/240] Loss: 1.0397757291793823, BER : 0.20486328125\n",
            "Train[0/3][175/240] Loss: 1.11385977268219, BER : 0.2054296875\n",
            "Train[0/3][176/240] Loss: 1.0055692195892334, BER : 0.1993515625\n",
            "Train[0/3][177/240] Loss: 1.0885136127471924, BER : 0.20289453125\n",
            "Train[0/3][178/240] Loss: 0.9885167479515076, BER : 0.1971484375\n",
            "Train[0/3][179/240] Loss: 0.9759551286697388, BER : 0.1948203125\n",
            "Train[0/3][180/240] Loss: 0.9407681226730347, BER : 0.18933984375\n",
            "Train[0/3][181/240] Loss: 0.9854339957237244, BER : 0.19875390625\n",
            "Train[0/3][182/240] Loss: 0.9463454484939575, BER : 0.19204296875\n",
            "Train[0/3][183/240] Loss: 0.9410131573677063, BER : 0.1903046875\n",
            "Train[0/3][184/240] Loss: 0.9609144926071167, BER : 0.19351171875\n",
            "Train[0/3][185/240] Loss: 0.9374427795410156, BER : 0.1898046875\n",
            "Train[0/3][186/240] Loss: 0.9670485258102417, BER : 0.1968828125\n",
            "Train[0/3][187/240] Loss: 0.9555008411407471, BER : 0.1965\n",
            "Train[0/3][188/240] Loss: 0.9569542407989502, BER : 0.1947890625\n",
            "Train[0/3][189/240] Loss: 0.9588507413864136, BER : 0.19494140625\n",
            "Train[0/3][190/240] Loss: 1.043143391609192, BER : 0.20278125\n",
            "Train[0/3][191/240] Loss: 1.0291885137557983, BER : 0.201328125\n",
            "Train[0/3][192/240] Loss: 1.0427682399749756, BER : 0.20365625\n",
            "Train[0/3][193/240] Loss: 1.068563461303711, BER : 0.22826171875\n",
            "Train[0/3][194/240] Loss: 1.0003427267074585, BER : 0.20246875\n",
            "Train[0/3][195/240] Loss: 1.0083281993865967, BER : 0.206078125\n",
            "Train[0/3][196/240] Loss: 1.0897555351257324, BER : 0.23359765625\n",
            "Train[0/3][197/240] Loss: 1.0741249322891235, BER : 0.230171875\n",
            "Train[0/3][198/240] Loss: 1.1386919021606445, BER : 0.23936328125\n",
            "Train[0/3][199/240] Loss: 1.0171453952789307, BER : 0.211546875\n",
            "Train[0/3][200/240] Loss: 1.0933924913406372, BER : 0.2341171875\n",
            "Train[0/3][201/240] Loss: 1.0131409168243408, BER : 0.21047265625\n",
            "Train[0/3][202/240] Loss: 1.0150470733642578, BER : 0.21079296875\n",
            "Train[0/3][203/240] Loss: 1.010188102722168, BER : 0.21091015625\n",
            "Train[0/3][204/240] Loss: 0.959772527217865, BER : 0.19948046875\n",
            "Train[0/3][205/240] Loss: 0.970461905002594, BER : 0.19944921875\n",
            "Train[0/3][206/240] Loss: 0.9650086164474487, BER : 0.20091796875\n",
            "Train[0/3][207/240] Loss: 0.9698328375816345, BER : 0.20180859375\n",
            "Train[0/3][208/240] Loss: 0.9834993481636047, BER : 0.2002578125\n",
            "Train[0/3][209/240] Loss: 0.9504802227020264, BER : 0.19505859375\n",
            "Train[0/3][210/240] Loss: 0.9704189896583557, BER : 0.19728125\n",
            "Train[0/3][211/240] Loss: 0.9654552340507507, BER : 0.2013828125\n",
            "Train[0/3][212/240] Loss: 0.9584567546844482, BER : 0.20330078125\n",
            "Train[0/3][213/240] Loss: 0.9478158354759216, BER : 0.199171875\n",
            "Train[0/3][214/240] Loss: 0.9521689414978027, BER : 0.19983203125\n",
            "Train[0/3][215/240] Loss: 0.9369453191757202, BER : 0.19909375\n",
            "Train[0/3][216/240] Loss: 0.9462970495223999, BER : 0.19778125\n",
            "Train[0/3][217/240] Loss: 0.9407596588134766, BER : 0.19720703125\n",
            "Train[0/3][218/240] Loss: 0.9878793358802795, BER : 0.2077578125\n",
            "Train[0/3][219/240] Loss: 0.9701132774353027, BER : 0.203015625\n",
            "Train[0/3][220/240] Loss: 0.9417814612388611, BER : 0.193265625\n",
            "Train[0/3][221/240] Loss: 0.951661229133606, BER : 0.19820703125\n",
            "Train[0/3][222/240] Loss: 0.9636384844779968, BER : 0.20398828125\n",
            "Train[0/3][223/240] Loss: 0.9726142883300781, BER : 0.20462109375\n",
            "Train[0/3][224/240] Loss: 0.9584933519363403, BER : 0.2026796875\n",
            "Train[0/3][225/240] Loss: 0.9229719638824463, BER : 0.19546484375\n",
            "Train[0/3][226/240] Loss: 0.925514817237854, BER : 0.1954765625\n",
            "Train[0/3][227/240] Loss: 0.968925952911377, BER : 0.2058515625\n",
            "Train[0/3][228/240] Loss: 0.9365084171295166, BER : 0.19884765625\n",
            "Train[0/3][229/240] Loss: 0.9318103790283203, BER : 0.19191015625\n",
            "Train[0/3][230/240] Loss: 0.9343224167823792, BER : 0.19463671875\n",
            "Train[0/3][231/240] Loss: 0.9309331178665161, BER : 0.1949765625\n",
            "Train[0/3][232/240] Loss: 0.914420485496521, BER : 0.19198046875\n",
            "Train[0/3][233/240] Loss: 0.923960268497467, BER : 0.1930703125\n",
            "Train[0/3][234/240] Loss: 0.9225738644599915, BER : 0.1973203125\n",
            "Train[0/3][235/240] Loss: 0.919721782207489, BER : 0.19880859375\n",
            "Train[0/3][236/240] Loss: 0.9197548031806946, BER : 0.19682421875\n",
            "Train[0/3][237/240] Loss: 0.902479887008667, BER : 0.1932109375\n",
            "Train[0/3][238/240] Loss: 0.8772571086883545, BER : 0.18917578125\n",
            "Train[0/3][239/240] Loss: 0.8767728805541992, BER : 0.19063671875\n",
            "Test[0/3][0/40]  Loss: 2.1680543422698975, BER (test): 0.38207421875\n",
            "Test[0/3][1/40]  Loss: 2.1577584743499756, BER (test): 0.3826328125\n",
            "Test[0/3][2/40]  Loss: 2.179917812347412, BER (test): 0.384359375\n",
            "Test[0/3][3/40]  Loss: 2.1803529262542725, BER (test): 0.3854453125\n",
            "Test[0/3][4/40]  Loss: 2.1798038482666016, BER (test): 0.38593359375\n",
            "Test[0/3][5/40]  Loss: 2.1781046390533447, BER (test): 0.38528125\n",
            "Test[0/3][6/40]  Loss: 2.173433303833008, BER (test): 0.3835859375\n",
            "Test[0/3][7/40]  Loss: 2.1754794120788574, BER (test): 0.3843515625\n",
            "Test[0/3][8/40]  Loss: 2.166886329650879, BER (test): 0.38291015625\n",
            "Test[0/3][9/40]  Loss: 2.167605400085449, BER (test): 0.38184375\n",
            "Test[0/3][10/40]  Loss: 2.1710009574890137, BER (test): 0.3836015625\n",
            "Test[0/3][11/40]  Loss: 2.1828484535217285, BER (test): 0.38609765625\n",
            "Test[0/3][12/40]  Loss: 2.1676337718963623, BER (test): 0.3834375\n",
            "Test[0/3][13/40]  Loss: 2.159735679626465, BER (test): 0.38169140625\n",
            "Test[0/3][14/40]  Loss: 2.166006088256836, BER (test): 0.38166796875\n",
            "Test[0/3][15/40]  Loss: 2.2005114555358887, BER (test): 0.38708203125\n",
            "Test[0/3][16/40]  Loss: 2.1654627323150635, BER (test): 0.38323046875\n",
            "Test[0/3][17/40]  Loss: 2.16896653175354, BER (test): 0.3836953125\n",
            "Test[0/3][18/40]  Loss: 2.1846790313720703, BER (test): 0.38626953125\n",
            "Test[0/3][19/40]  Loss: 2.1788206100463867, BER (test): 0.38399609375\n",
            "Test[0/3][20/40]  Loss: 2.1537680625915527, BER (test): 0.38149609375\n",
            "Test[0/3][21/40]  Loss: 2.1888933181762695, BER (test): 0.3867578125\n",
            "Test[0/3][22/40]  Loss: 2.1800570487976074, BER (test): 0.38305078125\n",
            "Test[0/3][23/40]  Loss: 2.1901700496673584, BER (test): 0.38636328125\n",
            "Test[0/3][24/40]  Loss: 2.1720118522644043, BER (test): 0.3832109375\n",
            "Test[0/3][25/40]  Loss: 2.1688544750213623, BER (test): 0.384\n",
            "Test[0/3][26/40]  Loss: 2.1668107509613037, BER (test): 0.38419140625\n",
            "Test[0/3][27/40]  Loss: 2.173922061920166, BER (test): 0.38333203125\n",
            "Test[0/3][28/40]  Loss: 2.1494832038879395, BER (test): 0.3798515625\n",
            "Test[0/3][29/40]  Loss: 2.1772875785827637, BER (test): 0.38387109375\n",
            "Test[0/3][30/40]  Loss: 2.1826343536376953, BER (test): 0.38623828125\n",
            "Test[0/3][31/40]  Loss: 2.164379119873047, BER (test): 0.381828125\n",
            "Test[0/3][32/40]  Loss: 2.1683201789855957, BER (test): 0.38277734375\n",
            "Test[0/3][33/40]  Loss: 2.170144557952881, BER (test): 0.3828671875\n",
            "Test[0/3][34/40]  Loss: 2.1892385482788086, BER (test): 0.38567578125\n",
            "Test[0/3][35/40]  Loss: 2.1713168621063232, BER (test): 0.3832890625\n",
            "Test[0/3][36/40]  Loss: 2.194444417953491, BER (test): 0.387109375\n",
            "Test[0/3][37/40]  Loss: 2.185018301010132, BER (test): 0.3853828125\n",
            "Test[0/3][38/40]  Loss: 2.1644511222839355, BER (test): 0.38404296875\n",
            "Test[0/3][39/40]  Loss: 2.164525270462036, BER (test): 0.38351171875\n",
            "Train[1/3][0/240] Loss: 0.9073829650878906, BER : 0.1948515625\n",
            "Train[1/3][1/240] Loss: 0.8902662992477417, BER : 0.1863828125\n",
            "Train[1/3][2/240] Loss: 0.8705868721008301, BER : 0.1876015625\n",
            "Train[1/3][3/240] Loss: 0.8833799362182617, BER : 0.1841640625\n",
            "Train[1/3][4/240] Loss: 0.8943164348602295, BER : 0.19430859375\n",
            "Train[1/3][5/240] Loss: 0.9103339314460754, BER : 0.19647265625\n",
            "Train[1/3][6/240] Loss: 0.8478097915649414, BER : 0.18376953125\n",
            "Train[1/3][7/240] Loss: 0.8992456793785095, BER : 0.1950390625\n",
            "Train[1/3][8/240] Loss: 0.8919771909713745, BER : 0.19378515625\n",
            "Train[1/3][9/240] Loss: 0.8985837697982788, BER : 0.19519921875\n",
            "Train[1/3][10/240] Loss: 0.9353480339050293, BER : 0.211125\n",
            "Train[1/3][11/240] Loss: 0.8986091017723083, BER : 0.1946015625\n",
            "Train[1/3][12/240] Loss: 0.8906282186508179, BER : 0.192828125\n",
            "Train[1/3][13/240] Loss: 0.9072993993759155, BER : 0.19748046875\n",
            "Train[1/3][14/240] Loss: 0.8575206995010376, BER : 0.18524609375\n",
            "Train[1/3][15/240] Loss: 0.8804216384887695, BER : 0.1919921875\n",
            "Train[1/3][16/240] Loss: 0.8472139239311218, BER : 0.18499609375\n",
            "Train[1/3][17/240] Loss: 0.8581251502037048, BER : 0.186625\n",
            "Train[1/3][18/240] Loss: 0.8489059805870056, BER : 0.1866640625\n",
            "Train[1/3][19/240] Loss: 0.8363375067710876, BER : 0.17901171875\n",
            "Train[1/3][20/240] Loss: 0.845898449420929, BER : 0.1827890625\n",
            "Train[1/3][21/240] Loss: 0.8668763637542725, BER : 0.19076953125\n",
            "Train[1/3][22/240] Loss: 0.8285736441612244, BER : 0.1827578125\n",
            "Train[1/3][23/240] Loss: 0.8580331802368164, BER : 0.18762890625\n",
            "Train[1/3][24/240] Loss: 0.8588317036628723, BER : 0.191171875\n",
            "Train[1/3][25/240] Loss: 0.8455969095230103, BER : 0.18791796875\n",
            "Train[1/3][26/240] Loss: 0.8541539311408997, BER : 0.1898671875\n",
            "Train[1/3][27/240] Loss: 0.8500543832778931, BER : 0.1880234375\n",
            "Train[1/3][28/240] Loss: 0.8493045568466187, BER : 0.19005859375\n",
            "Train[1/3][29/240] Loss: 0.8473089337348938, BER : 0.18980859375\n",
            "Train[1/3][30/240] Loss: 0.8114917278289795, BER : 0.1816171875\n",
            "Train[1/3][31/240] Loss: 0.8314650654792786, BER : 0.18398046875\n",
            "Train[1/3][32/240] Loss: 0.8071006536483765, BER : 0.1806328125\n",
            "Train[1/3][33/240] Loss: 0.8177310228347778, BER : 0.18366015625\n",
            "Train[1/3][34/240] Loss: 0.8293703198432922, BER : 0.18625390625\n",
            "Train[1/3][35/240] Loss: 0.8066461086273193, BER : 0.18151171875\n",
            "Train[1/3][36/240] Loss: 0.8003190755844116, BER : 0.18040234375\n",
            "Train[1/3][37/240] Loss: 0.793587327003479, BER : 0.17846484375\n",
            "Train[1/3][38/240] Loss: 0.8120166659355164, BER : 0.184140625\n",
            "Train[1/3][39/240] Loss: 0.7864409685134888, BER : 0.1776328125\n",
            "Train[1/3][40/240] Loss: 0.8215197920799255, BER : 0.18463671875\n",
            "Train[1/3][41/240] Loss: 0.8321887254714966, BER : 0.18890234375\n",
            "Train[1/3][42/240] Loss: 0.8187955021858215, BER : 0.18486328125\n",
            "Train[1/3][43/240] Loss: 0.8044612407684326, BER : 0.18334765625\n",
            "Train[1/3][44/240] Loss: 0.8116120100021362, BER : 0.1812734375\n",
            "Train[1/3][45/240] Loss: 0.8181911706924438, BER : 0.1866015625\n",
            "Train[1/3][46/240] Loss: 0.8052457571029663, BER : 0.18293359375\n",
            "Train[1/3][47/240] Loss: 0.815170407295227, BER : 0.18514453125\n",
            "Train[1/3][48/240] Loss: 0.7975894212722778, BER : 0.17896484375\n",
            "Train[1/3][49/240] Loss: 0.8041027784347534, BER : 0.1817109375\n",
            "Train[1/3][50/240] Loss: 0.7957381010055542, BER : 0.180484375\n",
            "Train[1/3][51/240] Loss: 0.8019919395446777, BER : 0.1794765625\n",
            "Train[1/3][52/240] Loss: 0.8143826723098755, BER : 0.1844296875\n",
            "Train[1/3][53/240] Loss: 0.7788982391357422, BER : 0.17925\n",
            "Train[1/3][54/240] Loss: 0.8037828803062439, BER : 0.1816171875\n",
            "Train[1/3][55/240] Loss: 0.7873144745826721, BER : 0.17942578125\n",
            "Train[1/3][56/240] Loss: 0.7895272970199585, BER : 0.1768046875\n",
            "Train[1/3][57/240] Loss: 0.768741250038147, BER : 0.1764296875\n",
            "Train[1/3][58/240] Loss: 0.7505732774734497, BER : 0.17152734375\n",
            "Train[1/3][59/240] Loss: 0.7434594035148621, BER : 0.1701875\n",
            "Train[1/3][60/240] Loss: 0.7489022016525269, BER : 0.1715\n",
            "Train[1/3][61/240] Loss: 0.7373958230018616, BER : 0.1688671875\n",
            "Train[1/3][62/240] Loss: 0.7332338094711304, BER : 0.16703515625\n",
            "Train[1/3][63/240] Loss: 0.760767936706543, BER : 0.17327734375\n",
            "Train[1/3][64/240] Loss: 0.737316370010376, BER : 0.1694921875\n",
            "Train[1/3][65/240] Loss: 0.7423239946365356, BER : 0.17119921875\n",
            "Train[1/3][66/240] Loss: 0.7303419709205627, BER : 0.16612109375\n",
            "Train[1/3][67/240] Loss: 0.7376372814178467, BER : 0.169546875\n",
            "Train[1/3][68/240] Loss: 0.745652437210083, BER : 0.1722578125\n",
            "Train[1/3][69/240] Loss: 0.7320607304573059, BER : 0.16801171875\n",
            "Train[1/3][70/240] Loss: 0.7244763374328613, BER : 0.16612890625\n",
            "Train[1/3][71/240] Loss: 0.7341256737709045, BER : 0.17163671875\n",
            "Train[1/3][72/240] Loss: 0.7591089010238647, BER : 0.1769453125\n",
            "Train[1/3][73/240] Loss: 0.7605605125427246, BER : 0.17533203125\n",
            "Train[1/3][74/240] Loss: 0.7528061270713806, BER : 0.1744453125\n",
            "Train[1/3][75/240] Loss: 0.8180328607559204, BER : 0.19253515625\n",
            "Train[1/3][76/240] Loss: 0.7508337497711182, BER : 0.1731640625\n",
            "Train[1/3][77/240] Loss: 0.768256425857544, BER : 0.17573828125\n",
            "Train[1/3][78/240] Loss: 0.7544766664505005, BER : 0.1702578125\n",
            "Train[1/3][79/240] Loss: 0.7277200222015381, BER : 0.16819140625\n",
            "Train[1/3][80/240] Loss: 0.7361772060394287, BER : 0.16972265625\n",
            "Train[1/3][81/240] Loss: 0.7176306247711182, BER : 0.16567578125\n",
            "Train[1/3][82/240] Loss: 0.7306801676750183, BER : 0.16915625\n",
            "Train[1/3][83/240] Loss: 0.744645357131958, BER : 0.17214453125\n",
            "Train[1/3][84/240] Loss: 0.7081630229949951, BER : 0.1625859375\n",
            "Train[1/3][85/240] Loss: 0.7337491512298584, BER : 0.16885546875\n",
            "Train[1/3][86/240] Loss: 0.7432123422622681, BER : 0.1726640625\n",
            "Train[1/3][87/240] Loss: 0.7402420043945312, BER : 0.1711171875\n",
            "Train[1/3][88/240] Loss: 0.7381289005279541, BER : 0.17271875\n",
            "Train[1/3][89/240] Loss: 0.7304080724716187, BER : 0.16867578125\n",
            "Train[1/3][90/240] Loss: 0.7080105543136597, BER : 0.16400390625\n",
            "Train[1/3][91/240] Loss: 0.7245555520057678, BER : 0.16991015625\n",
            "Train[1/3][92/240] Loss: 0.7198023796081543, BER : 0.16678125\n",
            "Train[1/3][93/240] Loss: 0.7155427932739258, BER : 0.16520703125\n",
            "Train[1/3][94/240] Loss: 0.7309386730194092, BER : 0.16865234375\n",
            "Train[1/3][95/240] Loss: 0.7262224555015564, BER : 0.167921875\n",
            "Train[1/3][96/240] Loss: 0.7150698900222778, BER : 0.16616015625\n",
            "Train[1/3][97/240] Loss: 0.7177606225013733, BER : 0.16721875\n",
            "Train[1/3][98/240] Loss: 0.7082394361495972, BER : 0.1660625\n",
            "Train[1/3][99/240] Loss: 0.746356725692749, BER : 0.16934765625\n",
            "Train[1/3][100/240] Loss: 0.718018651008606, BER : 0.16898828125\n",
            "Train[1/3][101/240] Loss: 0.7410163879394531, BER : 0.17109375\n",
            "Train[1/3][102/240] Loss: 0.7572123408317566, BER : 0.17255078125\n",
            "Train[1/3][103/240] Loss: 0.74457848072052, BER : 0.169078125\n",
            "Train[1/3][104/240] Loss: 0.7523425817489624, BER : 0.1703359375\n",
            "Train[1/3][105/240] Loss: 0.7296854257583618, BER : 0.1679609375\n",
            "Train[1/3][106/240] Loss: 0.7298879623413086, BER : 0.16896484375\n",
            "Train[1/3][107/240] Loss: 0.7130162715911865, BER : 0.16725\n",
            "Train[1/3][108/240] Loss: 0.7123079299926758, BER : 0.1631875\n",
            "Train[1/3][109/240] Loss: 0.7093833684921265, BER : 0.16435546875\n",
            "Train[1/3][110/240] Loss: 0.7065398097038269, BER : 0.1617734375\n",
            "Train[1/3][111/240] Loss: 0.691916823387146, BER : 0.15968359375\n",
            "Train[1/3][112/240] Loss: 0.6846492290496826, BER : 0.15683984375\n",
            "Train[1/3][113/240] Loss: 0.6925559043884277, BER : 0.1600859375\n",
            "Train[1/3][114/240] Loss: 0.702892005443573, BER : 0.16281640625\n",
            "Train[1/3][115/240] Loss: 0.7013019323348999, BER : 0.1614375\n",
            "Train[1/3][116/240] Loss: 0.7164233326911926, BER : 0.16433203125\n",
            "Train[1/3][117/240] Loss: 0.7211411595344543, BER : 0.16675390625\n",
            "Train[1/3][118/240] Loss: 0.7308192253112793, BER : 0.16794921875\n",
            "Train[1/3][119/240] Loss: 0.7443675398826599, BER : 0.17252734375\n",
            "Train[1/3][120/240] Loss: 0.7336812615394592, BER : 0.16908984375\n",
            "Train[1/3][121/240] Loss: 0.7251358032226562, BER : 0.16687890625\n",
            "Train[1/3][122/240] Loss: 0.737464189529419, BER : 0.166890625\n",
            "Train[1/3][123/240] Loss: 0.7122883796691895, BER : 0.162984375\n",
            "Train[1/3][124/240] Loss: 0.6979759931564331, BER : 0.15880078125\n",
            "Train[1/3][125/240] Loss: 0.6963921785354614, BER : 0.16111328125\n",
            "Train[1/3][126/240] Loss: 0.7073910236358643, BER : 0.16098046875\n",
            "Train[1/3][127/240] Loss: 0.6937709450721741, BER : 0.1609375\n",
            "Train[1/3][128/240] Loss: 0.7067042589187622, BER : 0.1628203125\n",
            "Train[1/3][129/240] Loss: 0.698767900466919, BER : 0.16201953125\n",
            "Train[1/3][130/240] Loss: 0.7088569402694702, BER : 0.1630703125\n",
            "Train[1/3][131/240] Loss: 0.7103694081306458, BER : 0.1632734375\n",
            "Train[1/3][132/240] Loss: 0.6862760782241821, BER : 0.155953125\n",
            "Train[1/3][133/240] Loss: 0.6982592344284058, BER : 0.16065234375\n",
            "Train[1/3][134/240] Loss: 0.6902366876602173, BER : 0.16006640625\n",
            "Train[1/3][135/240] Loss: 0.7276272773742676, BER : 0.16768359375\n",
            "Train[1/3][136/240] Loss: 0.7463651299476624, BER : 0.17040234375\n",
            "Train[1/3][137/240] Loss: 0.7364982962608337, BER : 0.1677890625\n",
            "Train[1/3][138/240] Loss: 0.7396104335784912, BER : 0.16775390625\n",
            "Train[1/3][139/240] Loss: 0.7315248250961304, BER : 0.16527734375\n",
            "Train[1/3][140/240] Loss: 0.7068901658058167, BER : 0.1631796875\n",
            "Train[1/3][141/240] Loss: 0.6831262111663818, BER : 0.15623828125\n",
            "Train[1/3][142/240] Loss: 0.6817619800567627, BER : 0.15662109375\n",
            "Train[1/3][143/240] Loss: 0.6734956502914429, BER : 0.15571484375\n",
            "Train[1/3][144/240] Loss: 0.6788098216056824, BER : 0.1556171875\n",
            "Train[1/3][145/240] Loss: 0.671017050743103, BER : 0.15296484375\n",
            "Train[1/3][146/240] Loss: 0.6815993785858154, BER : 0.15818359375\n",
            "Train[1/3][147/240] Loss: 0.6964225769042969, BER : 0.16008203125\n",
            "Train[1/3][148/240] Loss: 0.6957899928092957, BER : 0.15991015625\n",
            "Train[1/3][149/240] Loss: 0.7057501077651978, BER : 0.16269140625\n",
            "Train[1/3][150/240] Loss: 0.7342406511306763, BER : 0.16903515625\n",
            "Train[1/3][151/240] Loss: 0.7378438115119934, BER : 0.1677109375\n",
            "Train[1/3][152/240] Loss: 0.7398884296417236, BER : 0.16786328125\n",
            "Train[1/3][153/240] Loss: 0.7145336866378784, BER : 0.1626015625\n",
            "Train[1/3][154/240] Loss: 0.7017734050750732, BER : 0.15943359375\n",
            "Train[1/3][155/240] Loss: 0.7087818384170532, BER : 0.1621171875\n",
            "Train[1/3][156/240] Loss: 0.6775702238082886, BER : 0.152359375\n",
            "Train[1/3][157/240] Loss: 0.66923588514328, BER : 0.152625\n",
            "Train[1/3][158/240] Loss: 0.65964674949646, BER : 0.151828125\n",
            "Train[1/3][159/240] Loss: 0.6382903456687927, BER : 0.14568359375\n",
            "Train[1/3][160/240] Loss: 0.6523820757865906, BER : 0.14948046875\n",
            "Train[1/3][161/240] Loss: 0.6527633666992188, BER : 0.1501328125\n",
            "Train[1/3][162/240] Loss: 0.680446982383728, BER : 0.15678515625\n",
            "Train[1/3][163/240] Loss: 0.6821617484092712, BER : 0.15803125\n",
            "Train[1/3][164/240] Loss: 0.7199347615242004, BER : 0.16494140625\n",
            "Train[1/3][165/240] Loss: 0.7240433692932129, BER : 0.1680078125\n",
            "Train[1/3][166/240] Loss: 0.7484033107757568, BER : 0.17165234375\n",
            "Train[1/3][167/240] Loss: 0.7445039749145508, BER : 0.169953125\n",
            "Train[1/3][168/240] Loss: 0.7650513052940369, BER : 0.1734140625\n",
            "Train[1/3][169/240] Loss: 0.7477421164512634, BER : 0.168609375\n",
            "Train[1/3][170/240] Loss: 0.7534483671188354, BER : 0.1688828125\n",
            "Train[1/3][171/240] Loss: 0.7474485039710999, BER : 0.16801171875\n",
            "Train[1/3][172/240] Loss: 0.7507596015930176, BER : 0.168078125\n",
            "Train[1/3][173/240] Loss: 0.7085976004600525, BER : 0.16023046875\n",
            "Train[1/3][174/240] Loss: 0.7180532813072205, BER : 0.16259765625\n",
            "Train[1/3][175/240] Loss: 0.7226320505142212, BER : 0.16334765625\n",
            "Train[1/3][176/240] Loss: 0.7073291540145874, BER : 0.15731640625\n",
            "Train[1/3][177/240] Loss: 0.6840583086013794, BER : 0.1544453125\n",
            "Train[1/3][178/240] Loss: 0.6783372759819031, BER : 0.1529296875\n",
            "Train[1/3][179/240] Loss: 0.6936461329460144, BER : 0.15628125\n",
            "Train[1/3][180/240] Loss: 0.6678286790847778, BER : 0.150890625\n",
            "Train[1/3][181/240] Loss: 0.6654196381568909, BER : 0.15228125\n",
            "Train[1/3][182/240] Loss: 0.6956777572631836, BER : 0.15973828125\n",
            "Train[1/3][183/240] Loss: 0.6945254802703857, BER : 0.158265625\n",
            "Train[1/3][184/240] Loss: 0.6787168979644775, BER : 0.15625\n",
            "Train[1/3][185/240] Loss: 0.7218199372291565, BER : 0.1652265625\n",
            "Train[1/3][186/240] Loss: 0.6940773725509644, BER : 0.1596015625\n",
            "Train[1/3][187/240] Loss: 0.7251340746879578, BER : 0.16421875\n",
            "Train[1/3][188/240] Loss: 0.7269636392593384, BER : 0.16422265625\n",
            "Train[1/3][189/240] Loss: 0.6946986317634583, BER : 0.15917578125\n",
            "Train[1/3][190/240] Loss: 0.691307783126831, BER : 0.1557421875\n",
            "Train[1/3][191/240] Loss: 0.7156550884246826, BER : 0.1623203125\n",
            "Train[1/3][192/240] Loss: 0.69596266746521, BER : 0.158796875\n",
            "Train[1/3][193/240] Loss: 0.6950558423995972, BER : 0.158921875\n",
            "Train[1/3][194/240] Loss: 0.6968864798545837, BER : 0.158453125\n",
            "Train[1/3][195/240] Loss: 0.6788502931594849, BER : 0.15317578125\n",
            "Train[1/3][196/240] Loss: 0.6963586211204529, BER : 0.1586953125\n",
            "Train[1/3][197/240] Loss: 0.6753559112548828, BER : 0.1538984375\n",
            "Train[1/3][198/240] Loss: 0.6827107667922974, BER : 0.15530078125\n",
            "Train[1/3][199/240] Loss: 0.6835101842880249, BER : 0.1556328125\n",
            "Train[1/3][200/240] Loss: 0.6815072298049927, BER : 0.15576953125\n",
            "Train[1/3][201/240] Loss: 0.6820621490478516, BER : 0.156265625\n",
            "Train[1/3][202/240] Loss: 0.6872533559799194, BER : 0.15859375\n",
            "Train[1/3][203/240] Loss: 0.6635740995407104, BER : 0.15230859375\n",
            "Train[1/3][204/240] Loss: 0.6893917322158813, BER : 0.15748046875\n",
            "Train[1/3][205/240] Loss: 0.6831247210502625, BER : 0.15937890625\n",
            "Train[1/3][206/240] Loss: 0.6855624914169312, BER : 0.15877734375\n",
            "Train[1/3][207/240] Loss: 0.6843984723091125, BER : 0.15898046875\n",
            "Train[1/3][208/240] Loss: 0.6843962669372559, BER : 0.1589375\n",
            "Train[1/3][209/240] Loss: 0.687651515007019, BER : 0.16128515625\n",
            "Train[1/3][210/240] Loss: 0.6812223792076111, BER : 0.15711328125\n",
            "Train[1/3][211/240] Loss: 0.6676477193832397, BER : 0.154390625\n",
            "Train[1/3][212/240] Loss: 0.6515830159187317, BER : 0.14915234375\n",
            "Train[1/3][213/240] Loss: 0.6714674830436707, BER : 0.155484375\n",
            "Train[1/3][214/240] Loss: 0.6691944003105164, BER : 0.15647265625\n",
            "Train[1/3][215/240] Loss: 0.6502852439880371, BER : 0.15042578125\n",
            "Train[1/3][216/240] Loss: 0.6540205478668213, BER : 0.15244921875\n",
            "Train[1/3][217/240] Loss: 0.6628410816192627, BER : 0.1549453125\n",
            "Train[1/3][218/240] Loss: 0.6692584753036499, BER : 0.15653515625\n",
            "Train[1/3][219/240] Loss: 0.6679186820983887, BER : 0.15658203125\n",
            "Train[1/3][220/240] Loss: 0.6637730598449707, BER : 0.15607421875\n",
            "Train[1/3][221/240] Loss: 0.6617793440818787, BER : 0.1550390625\n",
            "Train[1/3][222/240] Loss: 0.6740666031837463, BER : 0.15751171875\n",
            "Train[1/3][223/240] Loss: 0.6677225232124329, BER : 0.1566328125\n",
            "Train[1/3][224/240] Loss: 0.6674110889434814, BER : 0.1546796875\n",
            "Train[1/3][225/240] Loss: 0.6728084087371826, BER : 0.157140625\n",
            "Train[1/3][226/240] Loss: 0.6505033373832703, BER : 0.14860546875\n",
            "Train[1/3][227/240] Loss: 0.6600584983825684, BER : 0.15379296875\n",
            "Train[1/3][228/240] Loss: 0.6894258260726929, BER : 0.1605546875\n",
            "Train[1/3][229/240] Loss: 0.6751025319099426, BER : 0.158140625\n",
            "Train[1/3][230/240] Loss: 0.6555526256561279, BER : 0.15169921875\n",
            "Train[1/3][231/240] Loss: 0.6707556247711182, BER : 0.1566328125\n",
            "Train[1/3][232/240] Loss: 0.65088951587677, BER : 0.1521875\n",
            "Train[1/3][233/240] Loss: 0.6530088782310486, BER : 0.15134765625\n",
            "Train[1/3][234/240] Loss: 0.6598747968673706, BER : 0.1536796875\n",
            "Train[1/3][235/240] Loss: 0.6578466296195984, BER : 0.1536328125\n",
            "Train[1/3][236/240] Loss: 0.6693208813667297, BER : 0.1567734375\n",
            "Train[1/3][237/240] Loss: 0.6501388549804688, BER : 0.1516875\n",
            "Train[1/3][238/240] Loss: 0.6432663202285767, BER : 0.149359375\n",
            "Train[1/3][239/240] Loss: 0.6676450371742249, BER : 0.15587109375\n",
            "Test[1/3][0/40]  Loss: 1.2194658517837524, BER (test): 0.25494140625\n",
            "Test[1/3][1/40]  Loss: 1.2195043563842773, BER (test): 0.25393359375\n",
            "Test[1/3][2/40]  Loss: 1.2118648290634155, BER (test): 0.25366015625\n",
            "Test[1/3][3/40]  Loss: 1.2311877012252808, BER (test): 0.25588671875\n",
            "Test[1/3][4/40]  Loss: 1.2121498584747314, BER (test): 0.2520390625\n",
            "Test[1/3][5/40]  Loss: 1.2175934314727783, BER (test): 0.25344921875\n",
            "Test[1/3][6/40]  Loss: 1.2163777351379395, BER (test): 0.25325390625\n",
            "Test[1/3][7/40]  Loss: 1.219290018081665, BER (test): 0.25405859375\n",
            "Test[1/3][8/40]  Loss: 1.2290096282958984, BER (test): 0.25328125\n",
            "Test[1/3][9/40]  Loss: 1.2210286855697632, BER (test): 0.2538046875\n",
            "Test[1/3][10/40]  Loss: 1.2165207862854004, BER (test): 0.25158203125\n",
            "Test[1/3][11/40]  Loss: 1.218345046043396, BER (test): 0.253203125\n",
            "Test[1/3][12/40]  Loss: 1.2173371315002441, BER (test): 0.25290234375\n",
            "Test[1/3][13/40]  Loss: 1.216944694519043, BER (test): 0.25369921875\n",
            "Test[1/3][14/40]  Loss: 1.2184041738510132, BER (test): 0.25434375\n",
            "Test[1/3][15/40]  Loss: 1.2172609567642212, BER (test): 0.25346875\n",
            "Test[1/3][16/40]  Loss: 1.2133493423461914, BER (test): 0.25259765625\n",
            "Test[1/3][17/40]  Loss: 1.2158281803131104, BER (test): 0.25448828125\n",
            "Test[1/3][18/40]  Loss: 1.2158451080322266, BER (test): 0.2524296875\n",
            "Test[1/3][19/40]  Loss: 1.2168292999267578, BER (test): 0.25329296875\n",
            "Test[1/3][20/40]  Loss: 1.2173100709915161, BER (test): 0.2536484375\n",
            "Test[1/3][21/40]  Loss: 1.2223857641220093, BER (test): 0.25322265625\n",
            "Test[1/3][22/40]  Loss: 1.2235136032104492, BER (test): 0.2545390625\n",
            "Test[1/3][23/40]  Loss: 1.2157902717590332, BER (test): 0.25340234375\n",
            "Test[1/3][24/40]  Loss: 1.20928955078125, BER (test): 0.25215625\n",
            "Test[1/3][25/40]  Loss: 1.2148927450180054, BER (test): 0.2528046875\n",
            "Test[1/3][26/40]  Loss: 1.221398949623108, BER (test): 0.25407421875\n",
            "Test[1/3][27/40]  Loss: 1.2245546579360962, BER (test): 0.25327734375\n",
            "Test[1/3][28/40]  Loss: 1.2156662940979004, BER (test): 0.2531953125\n",
            "Test[1/3][29/40]  Loss: 1.223934292793274, BER (test): 0.25448828125\n",
            "Test[1/3][30/40]  Loss: 1.2188104391098022, BER (test): 0.25362890625\n",
            "Test[1/3][31/40]  Loss: 1.2200692892074585, BER (test): 0.25650390625\n",
            "Test[1/3][32/40]  Loss: 1.2179895639419556, BER (test): 0.25461328125\n",
            "Test[1/3][33/40]  Loss: 1.220005750656128, BER (test): 0.25354296875\n",
            "Test[1/3][34/40]  Loss: 1.214136004447937, BER (test): 0.25216015625\n",
            "Test[1/3][35/40]  Loss: 1.2223255634307861, BER (test): 0.25537890625\n",
            "Test[1/3][36/40]  Loss: 1.2195230722427368, BER (test): 0.253515625\n",
            "Test[1/3][37/40]  Loss: 1.2199501991271973, BER (test): 0.2547734375\n",
            "Test[1/3][38/40]  Loss: 1.216813564300537, BER (test): 0.25523828125\n",
            "Test[1/3][39/40]  Loss: 1.2141011953353882, BER (test): 0.25500390625\n",
            "Train[2/3][0/240] Loss: 0.6640655994415283, BER : 0.1558203125\n",
            "Train[2/3][1/240] Loss: 0.6463940143585205, BER : 0.15050390625\n",
            "Train[2/3][2/240] Loss: 0.6699647903442383, BER : 0.15730078125\n",
            "Train[2/3][3/240] Loss: 0.6412348747253418, BER : 0.1489453125\n",
            "Train[2/3][4/240] Loss: 0.6623950600624084, BER : 0.1560859375\n",
            "Train[2/3][5/240] Loss: 0.65151447057724, BER : 0.153015625\n",
            "Train[2/3][6/240] Loss: 0.6520528793334961, BER : 0.15185546875\n",
            "Train[2/3][7/240] Loss: 0.6692236661911011, BER : 0.15580078125\n",
            "Train[2/3][8/240] Loss: 0.6648184061050415, BER : 0.156421875\n",
            "Train[2/3][9/240] Loss: 0.652259886264801, BER : 0.152296875\n",
            "Train[2/3][10/240] Loss: 0.6861766576766968, BER : 0.16198046875\n",
            "Train[2/3][11/240] Loss: 0.6567500829696655, BER : 0.15224609375\n",
            "Train[2/3][12/240] Loss: 0.6546511650085449, BER : 0.15253515625\n",
            "Train[2/3][13/240] Loss: 0.6535404920578003, BER : 0.1525703125\n",
            "Train[2/3][14/240] Loss: 0.6565752029418945, BER : 0.15162109375\n",
            "Train[2/3][15/240] Loss: 0.6621119379997253, BER : 0.153078125\n",
            "Train[2/3][16/240] Loss: 0.6729699373245239, BER : 0.15689453125\n",
            "Train[2/3][17/240] Loss: 0.6630775928497314, BER : 0.15414453125\n",
            "Train[2/3][18/240] Loss: 0.6663760542869568, BER : 0.1569609375\n",
            "Train[2/3][19/240] Loss: 0.6450897455215454, BER : 0.1492734375\n",
            "Train[2/3][20/240] Loss: 0.642157793045044, BER : 0.1494296875\n",
            "Train[2/3][21/240] Loss: 0.6444578766822815, BER : 0.14895703125\n",
            "Train[2/3][22/240] Loss: 0.6455525755882263, BER : 0.1516328125\n",
            "Train[2/3][23/240] Loss: 0.6606969237327576, BER : 0.15448046875\n",
            "Train[2/3][24/240] Loss: 0.6451396942138672, BER : 0.15075\n",
            "Train[2/3][25/240] Loss: 0.6407973766326904, BER : 0.15128515625\n",
            "Train[2/3][26/240] Loss: 0.6567685604095459, BER : 0.1560546875\n",
            "Train[2/3][27/240] Loss: 0.6586665511131287, BER : 0.15605078125\n",
            "Train[2/3][28/240] Loss: 0.6435421705245972, BER : 0.1503125\n",
            "Train[2/3][29/240] Loss: 0.6652364134788513, BER : 0.15834765625\n",
            "Train[2/3][30/240] Loss: 0.6461960077285767, BER : 0.15187890625\n",
            "Train[2/3][31/240] Loss: 0.6355141401290894, BER : 0.148296875\n",
            "Train[2/3][32/240] Loss: 0.6564674377441406, BER : 0.15230078125\n",
            "Train[2/3][33/240] Loss: 0.6367523670196533, BER : 0.14869140625\n",
            "Train[2/3][34/240] Loss: 0.6497484445571899, BER : 0.15153515625\n",
            "Train[2/3][35/240] Loss: 0.6344323754310608, BER : 0.14828515625\n",
            "Train[2/3][36/240] Loss: 0.6335592269897461, BER : 0.14913671875\n",
            "Train[2/3][37/240] Loss: 0.6530646085739136, BER : 0.1555078125\n",
            "Train[2/3][38/240] Loss: 0.6357519626617432, BER : 0.149\n",
            "Train[2/3][39/240] Loss: 0.6577205061912537, BER : 0.15668359375\n",
            "Train[2/3][40/240] Loss: 0.6340043544769287, BER : 0.14865625\n",
            "Train[2/3][41/240] Loss: 0.6635693311691284, BER : 0.155328125\n",
            "Train[2/3][42/240] Loss: 0.6594493985176086, BER : 0.15636328125\n",
            "Train[2/3][43/240] Loss: 0.6428748965263367, BER : 0.14872265625\n",
            "Train[2/3][44/240] Loss: 0.648074209690094, BER : 0.15234375\n",
            "Train[2/3][45/240] Loss: 0.6268559694290161, BER : 0.144859375\n",
            "Train[2/3][46/240] Loss: 0.6473603844642639, BER : 0.15248046875\n",
            "Train[2/3][47/240] Loss: 0.6342021226882935, BER : 0.148265625\n",
            "Train[2/3][48/240] Loss: 0.6404074430465698, BER : 0.14877734375\n",
            "Train[2/3][49/240] Loss: 0.6467450857162476, BER : 0.15185546875\n",
            "Train[2/3][50/240] Loss: 0.6377968192100525, BER : 0.151\n",
            "Train[2/3][51/240] Loss: 0.6336169242858887, BER : 0.1487890625\n",
            "Train[2/3][52/240] Loss: 0.6495065689086914, BER : 0.15298828125\n",
            "Train[2/3][53/240] Loss: 0.6537252068519592, BER : 0.156515625\n",
            "Train[2/3][54/240] Loss: 0.644929051399231, BER : 0.15198828125\n",
            "Train[2/3][55/240] Loss: 0.642090380191803, BER : 0.151078125\n",
            "Train[2/3][56/240] Loss: 0.645683765411377, BER : 0.15134765625\n",
            "Train[2/3][57/240] Loss: 0.638642430305481, BER : 0.147375\n",
            "Train[2/3][58/240] Loss: 0.6229159832000732, BER : 0.1431640625\n",
            "Train[2/3][59/240] Loss: 0.6425074338912964, BER : 0.15067578125\n",
            "Train[2/3][60/240] Loss: 0.638611912727356, BER : 0.14838671875\n",
            "Train[2/3][61/240] Loss: 0.6526256799697876, BER : 0.154546875\n",
            "Train[2/3][62/240] Loss: 0.6359813213348389, BER : 0.14974609375\n",
            "Train[2/3][63/240] Loss: 0.6480966806411743, BER : 0.15122265625\n",
            "Train[2/3][64/240] Loss: 0.6277483701705933, BER : 0.14707421875\n",
            "Train[2/3][65/240] Loss: 0.6436720490455627, BER : 0.15080078125\n",
            "Train[2/3][66/240] Loss: 0.6173574328422546, BER : 0.14578515625\n",
            "Train[2/3][67/240] Loss: 0.650327205657959, BER : 0.15355078125\n",
            "Train[2/3][68/240] Loss: 0.632937490940094, BER : 0.14881640625\n",
            "Train[2/3][69/240] Loss: 0.6344163417816162, BER : 0.15068359375\n",
            "Train[2/3][70/240] Loss: 0.6261991858482361, BER : 0.14812890625\n",
            "Train[2/3][71/240] Loss: 0.6205260753631592, BER : 0.14710546875\n",
            "Train[2/3][72/240] Loss: 0.638763964176178, BER : 0.150984375\n",
            "Train[2/3][73/240] Loss: 0.6294033527374268, BER : 0.14850390625\n",
            "Train[2/3][74/240] Loss: 0.6359608173370361, BER : 0.1505703125\n",
            "Train[2/3][75/240] Loss: 0.6417328119277954, BER : 0.1523046875\n",
            "Train[2/3][76/240] Loss: 0.6525915861129761, BER : 0.15603125\n",
            "Train[2/3][77/240] Loss: 0.6332365274429321, BER : 0.1494453125\n",
            "Train[2/3][78/240] Loss: 0.6349292993545532, BER : 0.15017578125\n",
            "Train[2/3][79/240] Loss: 0.6257768869400024, BER : 0.14728125\n",
            "Train[2/3][80/240] Loss: 0.6342857480049133, BER : 0.1504453125\n",
            "Train[2/3][81/240] Loss: 0.6320534944534302, BER : 0.14828515625\n",
            "Train[2/3][82/240] Loss: 0.6209409832954407, BER : 0.14505078125\n",
            "Train[2/3][83/240] Loss: 0.633198082447052, BER : 0.14918359375\n",
            "Train[2/3][84/240] Loss: 0.6381081342697144, BER : 0.15112890625\n",
            "Train[2/3][85/240] Loss: 0.6258584260940552, BER : 0.14878515625\n",
            "Train[2/3][86/240] Loss: 0.6201205849647522, BER : 0.14648828125\n",
            "Train[2/3][87/240] Loss: 0.62604820728302, BER : 0.148265625\n",
            "Train[2/3][88/240] Loss: 0.6230960488319397, BER : 0.14725\n",
            "Train[2/3][89/240] Loss: 0.6485015153884888, BER : 0.15305078125\n",
            "Train[2/3][90/240] Loss: 0.6348123550415039, BER : 0.14946875\n",
            "Train[2/3][91/240] Loss: 0.6346241235733032, BER : 0.15020703125\n",
            "Train[2/3][92/240] Loss: 0.6384131908416748, BER : 0.1504453125\n",
            "Train[2/3][93/240] Loss: 0.6467064619064331, BER : 0.153578125\n",
            "Train[2/3][94/240] Loss: 0.6337828040122986, BER : 0.150890625\n",
            "Train[2/3][95/240] Loss: 0.6282995939254761, BER : 0.14839453125\n",
            "Train[2/3][96/240] Loss: 0.6376188397407532, BER : 0.15099609375\n",
            "Train[2/3][97/240] Loss: 0.6422301530838013, BER : 0.15099609375\n",
            "Train[2/3][98/240] Loss: 0.6434912085533142, BER : 0.152765625\n",
            "Train[2/3][99/240] Loss: 0.6387643814086914, BER : 0.15210546875\n",
            "Train[2/3][100/240] Loss: 0.6395763754844666, BER : 0.15161328125\n",
            "Train[2/3][101/240] Loss: 0.628810465335846, BER : 0.14786328125\n",
            "Train[2/3][102/240] Loss: 0.6268749237060547, BER : 0.14741015625\n",
            "Train[2/3][103/240] Loss: 0.6406864523887634, BER : 0.1508984375\n",
            "Train[2/3][104/240] Loss: 0.6479045748710632, BER : 0.15290625\n",
            "Train[2/3][105/240] Loss: 0.6467941999435425, BER : 0.15366015625\n",
            "Train[2/3][106/240] Loss: 0.6276499032974243, BER : 0.14637890625\n",
            "Train[2/3][107/240] Loss: 0.6381949186325073, BER : 0.15026953125\n",
            "Train[2/3][108/240] Loss: 0.6263983845710754, BER : 0.14691015625\n",
            "Train[2/3][109/240] Loss: 0.6331549882888794, BER : 0.1486796875\n",
            "Train[2/3][110/240] Loss: 0.6367065906524658, BER : 0.14831640625\n",
            "Train[2/3][111/240] Loss: 0.6210366487503052, BER : 0.1428359375\n",
            "Train[2/3][112/240] Loss: 0.6205015182495117, BER : 0.1468671875\n",
            "Train[2/3][113/240] Loss: 0.6329128742218018, BER : 0.15083984375\n",
            "Train[2/3][114/240] Loss: 0.6402406096458435, BER : 0.15233203125\n",
            "Train[2/3][115/240] Loss: 0.6446086764335632, BER : 0.15273828125\n",
            "Train[2/3][116/240] Loss: 0.6259313225746155, BER : 0.14756640625\n",
            "Train[2/3][117/240] Loss: 0.6379204392433167, BER : 0.15162109375\n",
            "Train[2/3][118/240] Loss: 0.6277710795402527, BER : 0.149453125\n",
            "Train[2/3][119/240] Loss: 0.6324643492698669, BER : 0.1485703125\n",
            "Train[2/3][120/240] Loss: 0.6358522176742554, BER : 0.1505390625\n",
            "Train[2/3][121/240] Loss: 0.6262161731719971, BER : 0.14740234375\n",
            "Train[2/3][122/240] Loss: 0.6344653367996216, BER : 0.14957421875\n",
            "Train[2/3][123/240] Loss: 0.6242151260375977, BER : 0.1483046875\n",
            "Train[2/3][124/240] Loss: 0.6232330799102783, BER : 0.1473828125\n",
            "Train[2/3][125/240] Loss: 0.6397238373756409, BER : 0.152234375\n",
            "Train[2/3][126/240] Loss: 0.6303967237472534, BER : 0.1486171875\n",
            "Train[2/3][127/240] Loss: 0.6288033723831177, BER : 0.148\n",
            "Train[2/3][128/240] Loss: 0.6336832046508789, BER : 0.15015625\n",
            "Train[2/3][129/240] Loss: 0.6216965913772583, BER : 0.14490234375\n",
            "Train[2/3][130/240] Loss: 0.6249790191650391, BER : 0.14680078125\n",
            "Train[2/3][131/240] Loss: 0.6163484454154968, BER : 0.1454140625\n",
            "Train[2/3][132/240] Loss: 0.6244205832481384, BER : 0.1489609375\n",
            "Train[2/3][133/240] Loss: 0.6238999366760254, BER : 0.14762109375\n",
            "Train[2/3][134/240] Loss: 0.6196840405464172, BER : 0.14564453125\n",
            "Train[2/3][135/240] Loss: 0.6241905689239502, BER : 0.14759765625\n",
            "Train[2/3][136/240] Loss: 0.617684006690979, BER : 0.14661328125\n",
            "Train[2/3][137/240] Loss: 0.630562424659729, BER : 0.149890625\n",
            "Train[2/3][138/240] Loss: 0.6187775731086731, BER : 0.145625\n",
            "Train[2/3][139/240] Loss: 0.6282520294189453, BER : 0.1470390625\n",
            "Train[2/3][140/240] Loss: 0.6183695197105408, BER : 0.14457421875\n",
            "Train[2/3][141/240] Loss: 0.6347799301147461, BER : 0.1495625\n",
            "Train[2/3][142/240] Loss: 0.6238592267036438, BER : 0.14805859375\n",
            "Train[2/3][143/240] Loss: 0.6180326342582703, BER : 0.1461953125\n",
            "Train[2/3][144/240] Loss: 0.6180053353309631, BER : 0.14682421875\n",
            "Train[2/3][145/240] Loss: 0.6296992301940918, BER : 0.1498671875\n",
            "Train[2/3][146/240] Loss: 0.6183273196220398, BER : 0.1471015625\n",
            "Train[2/3][147/240] Loss: 0.624235212802887, BER : 0.14849609375\n",
            "Train[2/3][148/240] Loss: 0.6125707626342773, BER : 0.144703125\n",
            "Train[2/3][149/240] Loss: 0.6173006296157837, BER : 0.1435\n",
            "Train[2/3][150/240] Loss: 0.6100795865058899, BER : 0.142484375\n",
            "Train[2/3][151/240] Loss: 0.6167664527893066, BER : 0.14577734375\n",
            "Train[2/3][152/240] Loss: 0.6149803996086121, BER : 0.14414453125\n",
            "Train[2/3][153/240] Loss: 0.6256027221679688, BER : 0.14763671875\n",
            "Train[2/3][154/240] Loss: 0.6212424635887146, BER : 0.14814453125\n",
            "Train[2/3][155/240] Loss: 0.6195467710494995, BER : 0.14649609375\n",
            "Train[2/3][156/240] Loss: 0.6247719526290894, BER : 0.14971484375\n",
            "Train[2/3][157/240] Loss: 0.6090630292892456, BER : 0.1443359375\n",
            "Train[2/3][158/240] Loss: 0.6129724979400635, BER : 0.1451640625\n",
            "Train[2/3][159/240] Loss: 0.6255198121070862, BER : 0.14861328125\n",
            "Train[2/3][160/240] Loss: 0.612004280090332, BER : 0.1441484375\n",
            "Train[2/3][161/240] Loss: 0.6096737384796143, BER : 0.145453125\n",
            "Train[2/3][162/240] Loss: 0.617927074432373, BER : 0.146359375\n",
            "Train[2/3][163/240] Loss: 0.6118995547294617, BER : 0.14535546875\n",
            "Train[2/3][164/240] Loss: 0.6281228065490723, BER : 0.150234375\n",
            "Train[2/3][165/240] Loss: 0.6131327748298645, BER : 0.1440234375\n",
            "Train[2/3][166/240] Loss: 0.6128045916557312, BER : 0.14508203125\n",
            "Train[2/3][167/240] Loss: 0.6128578782081604, BER : 0.1438359375\n",
            "Train[2/3][168/240] Loss: 0.6214307546615601, BER : 0.14715625\n",
            "Train[2/3][169/240] Loss: 0.6174680590629578, BER : 0.1463046875\n",
            "Train[2/3][170/240] Loss: 0.6209730505943298, BER : 0.1485234375\n",
            "Train[2/3][171/240] Loss: 0.6185733079910278, BER : 0.14546875\n",
            "Train[2/3][172/240] Loss: 0.6156865358352661, BER : 0.14556640625\n",
            "Train[2/3][173/240] Loss: 0.6031649708747864, BER : 0.14210546875\n",
            "Train[2/3][174/240] Loss: 0.6087508201599121, BER : 0.1414296875\n",
            "Train[2/3][175/240] Loss: 0.6030214428901672, BER : 0.14083984375\n",
            "Train[2/3][176/240] Loss: 0.6096065044403076, BER : 0.1433125\n",
            "Train[2/3][177/240] Loss: 0.6099874973297119, BER : 0.14548828125\n",
            "Train[2/3][178/240] Loss: 0.6205195188522339, BER : 0.14894140625\n",
            "Train[2/3][179/240] Loss: 0.6056596040725708, BER : 0.14372265625\n",
            "Train[2/3][180/240] Loss: 0.6131983995437622, BER : 0.14612890625\n",
            "Train[2/3][181/240] Loss: 0.6204640865325928, BER : 0.148765625\n",
            "Train[2/3][182/240] Loss: 0.6242877840995789, BER : 0.14913671875\n",
            "Train[2/3][183/240] Loss: 0.6128520369529724, BER : 0.14625390625\n",
            "Train[2/3][184/240] Loss: 0.6192050576210022, BER : 0.1459765625\n",
            "Train[2/3][185/240] Loss: 0.610767126083374, BER : 0.1438203125\n",
            "Train[2/3][186/240] Loss: 0.6143031120300293, BER : 0.14616796875\n",
            "Train[2/3][187/240] Loss: 0.6171175837516785, BER : 0.1475859375\n",
            "Train[2/3][188/240] Loss: 0.6031110286712646, BER : 0.14406640625\n",
            "Train[2/3][189/240] Loss: 0.6160844564437866, BER : 0.14908984375\n",
            "Train[2/3][190/240] Loss: 0.6051923632621765, BER : 0.1432890625\n",
            "Train[2/3][191/240] Loss: 0.5999331474304199, BER : 0.14129296875\n",
            "Train[2/3][192/240] Loss: 0.6087344884872437, BER : 0.14536328125\n",
            "Train[2/3][193/240] Loss: 0.6136069297790527, BER : 0.14697265625\n",
            "Train[2/3][194/240] Loss: 0.6199606657028198, BER : 0.1484453125\n",
            "Train[2/3][195/240] Loss: 0.6091081500053406, BER : 0.14553125\n",
            "Train[2/3][196/240] Loss: 0.6175645589828491, BER : 0.14701953125\n",
            "Train[2/3][197/240] Loss: 0.6030665636062622, BER : 0.14116015625\n",
            "Train[2/3][198/240] Loss: 0.5967974066734314, BER : 0.14038671875\n",
            "Train[2/3][199/240] Loss: 0.5957447290420532, BER : 0.14020703125\n",
            "Train[2/3][200/240] Loss: 0.6078563928604126, BER : 0.14407421875\n",
            "Train[2/3][201/240] Loss: 0.6091474294662476, BER : 0.14602734375\n",
            "Train[2/3][202/240] Loss: 0.6114819645881653, BER : 0.14712109375\n",
            "Train[2/3][203/240] Loss: 0.6136980652809143, BER : 0.1458671875\n",
            "Train[2/3][204/240] Loss: 0.6018726825714111, BER : 0.1425390625\n",
            "Train[2/3][205/240] Loss: 0.6002039313316345, BER : 0.14235546875\n",
            "Train[2/3][206/240] Loss: 0.5985753536224365, BER : 0.14156640625\n",
            "Train[2/3][207/240] Loss: 0.60760098695755, BER : 0.14414453125\n",
            "Train[2/3][208/240] Loss: 0.6013367176055908, BER : 0.14282421875\n",
            "Train[2/3][209/240] Loss: 0.6005634069442749, BER : 0.14390234375\n",
            "Train[2/3][210/240] Loss: 0.6215934753417969, BER : 0.15096875\n",
            "Train[2/3][211/240] Loss: 0.594330906867981, BER : 0.13930859375\n",
            "Train[2/3][212/240] Loss: 0.6052289009094238, BER : 0.14351171875\n",
            "Train[2/3][213/240] Loss: 0.5981180667877197, BER : 0.14312890625\n",
            "Train[2/3][214/240] Loss: 0.6018329858779907, BER : 0.14359765625\n",
            "Train[2/3][215/240] Loss: 0.6045665144920349, BER : 0.145\n",
            "Train[2/3][216/240] Loss: 0.603987455368042, BER : 0.1437109375\n",
            "Train[2/3][217/240] Loss: 0.5945483446121216, BER : 0.14066796875\n",
            "Train[2/3][218/240] Loss: 0.6024172902107239, BER : 0.1434609375\n",
            "Train[2/3][219/240] Loss: 0.597026526927948, BER : 0.14294921875\n",
            "Train[2/3][220/240] Loss: 0.6033042669296265, BER : 0.14373828125\n",
            "Train[2/3][221/240] Loss: 0.5976191163063049, BER : 0.1414921875\n",
            "Train[2/3][222/240] Loss: 0.5906503200531006, BER : 0.14033203125\n",
            "Train[2/3][223/240] Loss: 0.5985492467880249, BER : 0.1437421875\n",
            "Train[2/3][224/240] Loss: 0.6024924516677856, BER : 0.14484375\n",
            "Train[2/3][225/240] Loss: 0.5967972278594971, BER : 0.1423515625\n",
            "Train[2/3][226/240] Loss: 0.5939041376113892, BER : 0.14258984375\n",
            "Train[2/3][227/240] Loss: 0.5999575257301331, BER : 0.14330859375\n",
            "Train[2/3][228/240] Loss: 0.5985572338104248, BER : 0.1443046875\n",
            "Train[2/3][229/240] Loss: 0.598366379737854, BER : 0.1426484375\n",
            "Train[2/3][230/240] Loss: 0.5839493274688721, BER : 0.13825390625\n",
            "Train[2/3][231/240] Loss: 0.5923851728439331, BER : 0.1419375\n",
            "Train[2/3][232/240] Loss: 0.5932609438896179, BER : 0.1421171875\n",
            "Train[2/3][233/240] Loss: 0.5905542373657227, BER : 0.14140625\n",
            "Train[2/3][234/240] Loss: 0.5898106098175049, BER : 0.14248046875\n",
            "Train[2/3][235/240] Loss: 0.5813236236572266, BER : 0.13757421875\n",
            "Train[2/3][236/240] Loss: 0.5794357061386108, BER : 0.13819140625\n",
            "Train[2/3][237/240] Loss: 0.5813997387886047, BER : 0.14046875\n",
            "Train[2/3][238/240] Loss: 0.5792759656906128, BER : 0.13781640625\n",
            "Train[2/3][239/240] Loss: 0.5842242240905762, BER : 0.14076171875\n",
            "Test[2/3][0/40]  Loss: 1.055401086807251, BER (test): 0.2214296875\n",
            "Test[2/3][1/40]  Loss: 1.0485260486602783, BER (test): 0.21975\n",
            "Test[2/3][2/40]  Loss: 1.0546913146972656, BER (test): 0.22060546875\n",
            "Test[2/3][3/40]  Loss: 1.0576937198638916, BER (test): 0.22253515625\n",
            "Test[2/3][4/40]  Loss: 1.0500015020370483, BER (test): 0.22049609375\n",
            "Test[2/3][5/40]  Loss: 1.0551010370254517, BER (test): 0.2204140625\n",
            "Test[2/3][6/40]  Loss: 1.0527641773223877, BER (test): 0.22028125\n",
            "Test[2/3][7/40]  Loss: 1.0573465824127197, BER (test): 0.21967578125\n",
            "Test[2/3][8/40]  Loss: 1.0511146783828735, BER (test): 0.22042578125\n",
            "Test[2/3][9/40]  Loss: 1.0516713857650757, BER (test): 0.22133984375\n",
            "Test[2/3][10/40]  Loss: 1.0547754764556885, BER (test): 0.2221484375\n",
            "Test[2/3][11/40]  Loss: 1.0494353771209717, BER (test): 0.2218125\n",
            "Test[2/3][12/40]  Loss: 1.054872751235962, BER (test): 0.222609375\n",
            "Test[2/3][13/40]  Loss: 1.0569677352905273, BER (test): 0.22067578125\n",
            "Test[2/3][14/40]  Loss: 1.0528593063354492, BER (test): 0.21966015625\n",
            "Test[2/3][15/40]  Loss: 1.0577651262283325, BER (test): 0.2219296875\n",
            "Test[2/3][16/40]  Loss: 1.0543038845062256, BER (test): 0.22028515625\n",
            "Test[2/3][17/40]  Loss: 1.0416322946548462, BER (test): 0.21758203125\n",
            "Test[2/3][18/40]  Loss: 1.0565600395202637, BER (test): 0.22175\n",
            "Test[2/3][19/40]  Loss: 1.0538229942321777, BER (test): 0.2204296875\n",
            "Test[2/3][20/40]  Loss: 1.0557903051376343, BER (test): 0.221625\n",
            "Test[2/3][21/40]  Loss: 1.0546931028366089, BER (test): 0.2209921875\n",
            "Test[2/3][22/40]  Loss: 1.0547205209732056, BER (test): 0.2208671875\n",
            "Test[2/3][23/40]  Loss: 1.0452271699905396, BER (test): 0.21869140625\n",
            "Test[2/3][24/40]  Loss: 1.0558642148971558, BER (test): 0.22222265625\n",
            "Test[2/3][25/40]  Loss: 1.0512909889221191, BER (test): 0.2227890625\n",
            "Test[2/3][26/40]  Loss: 1.0554749965667725, BER (test): 0.22128515625\n",
            "Test[2/3][27/40]  Loss: 1.0363821983337402, BER (test): 0.21737890625\n",
            "Test[2/3][28/40]  Loss: 1.0542000532150269, BER (test): 0.22065234375\n",
            "Test[2/3][29/40]  Loss: 1.0547406673431396, BER (test): 0.21997265625\n",
            "Test[2/3][30/40]  Loss: 1.0456886291503906, BER (test): 0.21987109375\n",
            "Test[2/3][31/40]  Loss: 1.047318696975708, BER (test): 0.21831640625\n",
            "Test[2/3][32/40]  Loss: 1.0541179180145264, BER (test): 0.219875\n",
            "Test[2/3][33/40]  Loss: 1.0482943058013916, BER (test): 0.2188046875\n",
            "Test[2/3][34/40]  Loss: 1.0572867393493652, BER (test): 0.22211328125\n",
            "Test[2/3][35/40]  Loss: 1.049699068069458, BER (test): 0.2213515625\n",
            "Test[2/3][36/40]  Loss: 1.046293020248413, BER (test): 0.2200546875\n",
            "Test[2/3][37/40]  Loss: 1.0555880069732666, BER (test): 0.22213671875\n",
            "Test[2/3][38/40]  Loss: 1.0515638589859009, BER (test): 0.22090625\n",
            "Test[2/3][39/40]  Loss: 1.0524494647979736, BER (test): 0.22126953125\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"Saved_Trained_Checkpoints/\"\n",
        "Output_Spikes = \"Output_Spikes/\"\n",
        "Enc_syn_Spikes = \"Enc_syn_Spikes/\"\n",
        "Intermediate_Lyrs = \"Intermediate_Lyrs/\"\n",
        "epoch_activations_list = [] # Create a list to store activations for each epoch\n",
        "epoch_activations = {}\n",
        "\n",
        "\n",
        "\n",
        "# # Run training and testing\n",
        "# for e in range(epochs):\n",
        "#     train_loss = train(net, train_loader, optimizer, e)\n",
        "#     train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "#     test_loss = test(net, test_loader, optimizer, e)\n",
        "#     test_avg_loss_rec.append(sum(test_loss_rec) / (len(test_loader)))\n",
        "\n",
        "\n",
        "#     #----------Save the model every 10 epochs\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#         model_path = checkpoint_path + f\"model_epoch_{e + 1}.pt\"\n",
        "#         torch.save(net.state_dict(), model_path)\n",
        "\n",
        "#     # # ---------------------------------------------- Add hooks for specific layers\n",
        "\n",
        "#     hook_layers = [net.encoder[2], net.encoder[5], net.encoder[8], net.encoder[11], net.decoder[1], net.decoder[4], net.decoder[7], net.decoder[9]]  # You can choose the layers you want to capture activations from\n",
        "#     hook_names = [\"Enc_Lk1\",\"Enc_syn1\", \"Enc_syn2\", \"Enc_Lk2\", \"Dec_Lk1\", \"Dec_syn1\", \"Dec_syn2\", \"Dec_Lk2\"]  # Names for the captured activations\n",
        "#     hooks = []\n",
        "#     for i, layer in enumerate(hook_layers):\n",
        "#       hook_fn = get_activation(hook_names[i])\n",
        "#       hooks.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "#     epoch_activations[e] = {}\n",
        "#     for i, name in enumerate(hook_names):\n",
        "#       epoch_activations[e][name] = activation[name]\n",
        "\n",
        "#     # Check if the current epoch is a multiple of 10\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#       # Save the epoch_activations dictionary to a file\n",
        "#       activations_path = Intermediate_Lyrs + f\"Intermediate_Lyrs_epoch_{e + 1}.pkl\"\n",
        "#       with open(activations_path, 'wb') as file:\n",
        "#         pickle.dump(epoch_activations, file)\n",
        "\n",
        "\n",
        "\n",
        "# ///////////////////////////////////\n",
        "\n",
        "# Define hook_layers and hook_names\n",
        "hook_layers = [net.encoder[2], net.encoder[5], net.encoder[8], net.encoder[11], net.decoder[1], net.decoder[4], net.decoder[7], net.decoder[9]]\n",
        "hook_names = [\"Enc_Lk1\", \"Enc_syn1\", \"Enc_syn2\", \"Enc_Lk2\", \"Dec_Lk1\", \"Dec_syn1\", \"Dec_syn2\", \"Dec_Lk2\"]\n",
        "\n",
        "# Create an empty dictionary to store activations\n",
        "epoch_activations = {}\n",
        "\n",
        "# Register hooks for capturing activations\n",
        "hooks = []\n",
        "for i, layer in enumerate(hook_layers):\n",
        "    hook_fn = get_activation(hook_names[i])\n",
        "    hooks.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "# Run training and testing\n",
        "for e in range(epochs):\n",
        "    train_loss = train(net, train_loader, optimizer, e)\n",
        "    train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "    test_loss = test(net, test_loader, optimizer, e)\n",
        "    test_avg_loss_rec.append(sum(test_loss_rec) / (len(test_loader)))\n",
        "\n",
        "    # #---------------------------------------------------------- Access the out_en tensor\n",
        "    out_en = net.out_en\n",
        "    out_en_numpy = out_en.cpu().detach().numpy()\n",
        "    # Save with a different name for each epoch\n",
        "    out_en_filename = Output_Spikes + f\"out_en_epoch_{e + 1}.npy\"\n",
        "    np.save(out_en_filename, out_en_numpy)\n",
        "\n",
        "    # #-----------------------------------------------------------Access the out tensor\n",
        "    out = net.out\n",
        "    out_numpy = out.cpu().detach().numpy()\n",
        "    # Save with a different name for each epoch\n",
        "    out_filename = Output_Spikes + f\"out_epoch_{e + 1}.npy\"\n",
        "    np.save(out_filename, out_numpy)\n",
        "    # -------------------------------------------------------------------- Intermediate Layers\n",
        "# Check if the current epoch is a multiple of 10\n",
        "    if (e + 1) % 10 == 0:\n",
        "        # Save the epoch_activations dictionary to a file\n",
        "        activations_path = Intermediate_Lyrs +  f\"activations_epoch_{e + 1}.pkl\"\n",
        "        with open(activations_path, 'wb') as file:\n",
        "            pickle.dump(epoch_activations, file)\n",
        "    # Capture activations for the current epoch\n",
        "    epoch_activations[e] = {}\n",
        "    for i, name in enumerate(hook_names):\n",
        "        epoch_activations[e][name] = activation.get(name, None)  # Use get to avoid KeyError\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# beta_syn=0.0001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To load the saved data"
      ],
      "metadata": {
        "id": "3r8IS1WVqDdV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6FpdQtAyhAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print available epoch numbers\n",
        "available_epochs = list(epoch_activations.keys())\n",
        "print(\"Available Epochs:\", available_epochs)\n",
        "\n",
        "# Choose a valid epoch number from the list\n",
        "valid_epoch = 2  # Replace with a valid epoch number from the list of available_epochs\n",
        "\n",
        "# Load activations for the chosen epoch\n",
        "activations_path = Intermediate_Lyrs + f\"activations_epoch_{valid_epoch}.pkl\"\n",
        "with open(activations_path, 'rb') as file:\n",
        "    epoch_activations = pickle.load(file)\n",
        "\n",
        "# Access the activations for Enc_syn1\n",
        "Enc_syn1_activations = epoch_activations[valid_epoch][\"Enc_syn1\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "Uux-gnqAqG8i",
        "outputId": "2ea75728-4a5e-4918-af3c-f454d277b0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0b0559759ecd>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mactivations_path\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0;34mf\"activations_epoch_{valid_epoch}.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mepoch_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Access the activations for Enc_syn1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                 typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m--> 980\u001b[0;31m                     \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                     _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    167\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gcl3d4RtyMRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming activation[\"Enc_syn1\"] is of shape (250, 64, 8, 8)\n",
        "data = activation[\"Enc_syn1\"][0, 0, :, :].cpu().numpy()  # Transfer to CPU and convert to NumPy\n",
        "\n",
        "# Create meshgrid for x and y dimensions\n",
        "x, y = np.meshgrid(np.arange(data.shape[1]), np.arange(data.shape[0]))\n",
        "\n",
        "# Create a figure and a 3D axis\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot the 3D surface\n",
        "ax.plot_surface(x, y, data, cmap='viridis')\n",
        "\n",
        "# Set axis labels\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "\n",
        "# Show the 3D plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "bQvnihqXys4P",
        "outputId": "275dfb56-ba47-4037-8c03-205c3411f324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-128ee26f263e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Assuming activation[\"Enc_syn1\"] is of shape (250, 64, 8, 8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Enc_syn1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Transfer to CPU and convert to NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create meshgrid for x and y dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Enc_syn1'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## to access the saved files"
      ],
      "metadata": {
        "id": "nyjYtMx3p1r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access activations for a specific epoch and layer\n",
        "epoch_number = 4  # Replace with the epoch number you're interested in\n",
        "layer_name = \"Enc_syn1\"  # Replace with the layer name you're interested in\n",
        "\n",
        "# Access the activations for the specified epoch and layer\n",
        "activations = epoch_activations[epoch_number][layer_name]\n",
        "\n",
        "# Now, you can use the 'activations' variable for analysi"
      ],
      "metadata": {
        "id": "vdHk7pO9p0pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_path = \"Saved_Trained_Checkpoints/\"\n",
        "# Output_Spikes = \"Output_Spikes/\"\n",
        "# Enc_syn_Spikes = \"Enc_syn_Spikes/\"\n",
        "\n",
        "# # Run training and testing\n",
        "# for e in range(epochs):\n",
        "#     train_loss = train(net, train_loader, optimizer, e)\n",
        "#     train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "#     test_loss = test(net, test_loader, optimizer, e)\n",
        "#     test_avg_loss_rec.append(sum(test_loss_rec) / (len(test_loader)))\n",
        "\n",
        "#     # Save the model every 10 epochs\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#         model_path = checkpoint_path + f\"model_epoch_{e + 1}.pt\"\n",
        "#         torch.save(net.state_dict(), model_path)\n",
        "\n",
        "\n",
        "#     # Add hooks for specific layers\n",
        "#     hook_layers = [net.encoder[5], net.encoder[8]]  # You can choose the layers you want to capture activations from\n",
        "#     hook_names = [\"Enc_syn1\", \"Enc_syn2\"]  # Names for the captured activations\n",
        "#     hooks = []\n",
        "#     for i, layer in enumerate(hook_layers):\n",
        "#       hook_fn = get_activation(hook_names[i])\n",
        "#       hooks.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "\n",
        "activation[\"Enc_syn1\"]\n",
        "\n",
        "print(activation[\"Enc_Lk1\"].size())\n",
        "print(activation[\"Enc_syn1\"].size())\n",
        "print(activation[\"Enc_syn2\"].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y4DleYOlf2L",
        "outputId": "e43db961-b696-45fb-90c4-5a8cd4fbba5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([250, 32, 16, 16])\n",
            "torch.Size([250, 64, 8, 8])\n",
            "torch.Size([250, 128, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjvlZfSt4T8L"
      },
      "outputs": [],
      "source": [
        "[[=]]\n",
        "# # Set the path for saving checkpoints\n",
        "# checkpoint_path = \"Saved_Trained_Checkpoints/\"\n",
        "\n",
        "#    #Run training and testing\n",
        "# for e in range(epochs):\n",
        "#   train_loss = train(net, train_loader, optimizer, e)\n",
        "#   train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "\n",
        "#   test_loss = test(net,test_loader, optimizer, e)\n",
        "#   test_avg_loss_rec.append(sum(test_loss_rec)/(len(test_loader)))\n",
        "\n",
        "# # /////////////////////////////////////////////////////////////////////\n",
        "#       # Save the model every 10 epochs\n",
        "#   if (e + 1) % 10 == 0:\n",
        "#     model_path = checkpoint_path + f\"model_epoch_{e + 1}.pt\"\n",
        "#     torch.save(net.state_dict(), model_path)\n",
        "\n",
        "# # After training\n",
        "# # Access the out_en tensor\n",
        "# out_en = net.out_en\n",
        "# out_en_numpy = out_en.cpu().detach().numpy()\n",
        "# np.save(\"out_en.npy\", out_en_numpy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YCKZr773q_b"
      },
      "outputs": [],
      "source": [
        "# spike_data = np.load(\"out_en.npy\")\n",
        "\n",
        "# # cmap = plt.get_cmap(\"binary\")\n",
        "\n",
        "# plt.imshow(spike_data, aspect=\"auto\")\n",
        "# # plt.xlabel(\"Time Steps\")\n",
        "# # plt.ylabel(\"Neurons\")\n",
        "# plt.title(\"Spike Activity in out_en\")\n",
        "# # plt.colorbar(label=\"Spikes\")\n",
        "\n",
        "# filename = f\"(out_en)_spike_activity_epoch_{epochs}_b_syn_{beta_syn}.pdf\"\n",
        "# plt.savefig(filename, format=\"pdf\")\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p6wqnnBH4OH"
      },
      "outputs": [],
      "source": [
        "# spike_data = np.load(\"out_en.npy\")\n",
        "\n",
        "# cmap = plt.get_cmap(\"binary\")\n",
        "# plt.imshow(spike_data, cmap=cmap, aspect=\"auto\")\n",
        "# # plt.xlabel(\"Time Steps\")\n",
        "# # plt.ylabel(\"Neurons\")\n",
        "# plt.title(\"Spike Activity in out_en\")\n",
        "# # plt.colorbar(label=\"Spikes\")\n",
        "\n",
        "# filename = f\"(out_en)_spike_activity_epoch_{epochs}_b_syn_{beta_syn}_binary.pdf\"\n",
        "# plt.savefig(filename, format=\"pdf\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the npy files"
      ],
      "metadata": {
        "id": "Q6L-Y5IU83Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "ax2_epoch = 50\n",
        "spike_data_bench = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "ax1_epoch = 1\n",
        "spike_data1_0001 = np.load(f\"out_en_epoch_{ax1_epoch}.npy\")\n",
        "ax2_epoch = 50\n",
        "spike_data_over = np.load(f\"out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mHP4qGr_82my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anaylizing the spike rates"
      ],
      "metadata": {
        "id": "IFSePS4l_IYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_rel\n",
        "spike_rate1=spike_data_bench\n",
        "spike_rate2=spike_data_over\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# Create histograms\n",
        "ax1.hist(spike_rate1, bins=20, label='Epoch 50', alpha=0.75)\n",
        "ax1.set_xlabel('Spike Rate')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Distribution of Spike Rates (benchmark)')\n",
        "# ax1.legend(\"benchmark\")\n",
        "ax2.hist(spike_rate2, bins=20, label='Epoch 50_0001', alpha=0.75)\n",
        "ax2.set_xlabel('Spike Rate')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('Distribution of Spike Rates (over-firing)')\n",
        "# ax2.legend(\"over-firing\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "nDtmu8zx_MAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Correlation over-firing and benchmark"
      ],
      "metadata": {
        "id": "peeUdW-1DlPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ax1_epoch = 1\n",
        "# spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "# ax2_epoch = 50\n",
        "# spike_data2 = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "spike_rate1=spike_data_bench\n",
        "spike_rate2=spike_data_over\n",
        "\n",
        "\n",
        "# Calculate cross-correlation between corresponding neurons in two epochs\n",
        "cross_correlations = []\n",
        "for i in range(spike_rate1.shape[1]):  # Assuming spike_data1 and spike_data2 have the same number of neurons\n",
        "    cross_corr = correlate(spike_rate1[:, i], spike_rate2[:, i], mode='same')\n",
        "    cross_correlations.append(cross_corr)\n",
        "\n",
        "# Plot cross-correlations for a few neurons (e.g., the first 5)\n",
        "for i in range(5):\n",
        "    plt.plot(cross_correlations[i], label=f'Timestep {i + 1}')\n",
        "\n",
        "plt.xlabel('Latent dim')\n",
        "plt.ylabel('Cross-Correlation')\n",
        "plt.legend()\n",
        "plt.title('Cross-Correlation Between Timesteps (benchmark) vs (overfiring)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BaJPD68bHLAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Calculate cross-correlation between corresponding neurons in two epochs\n",
        "cross_correlations = []\n",
        "for i in range(spike_rate1.shape[1]):  # Assuming spike_rate1 and spike_rate2 have the same number of neurons\n",
        "    cross_corr = correlate(spike_rate1[:, i], spike_rate2[:, i], mode='same')\n",
        "\n",
        "    # Normalize the cross-correlation\n",
        "    cross_corr /= np.sqrt(np.sum(spike_rate1[:, i] ** 2) * np.sum(spike_rate2[:, i] ** 2))\n",
        "\n",
        "    cross_correlations.append(cross_corr)\n",
        "\n",
        "# # Plot normalized cross-correlations for a few timestep (e.g., the first 5)\n",
        "for i in range(5):\n",
        "    plt.plot(cross_correlations[i], label=f'Timestep {i + 1}')\n",
        "\n",
        "\n",
        "# plt.plot(cross_correlations[4], label=f'Timestep {4 + 1}')\n",
        "\n",
        "plt.xlabel('Latent dim')\n",
        "plt.ylabel('Normalized Cross-Correlation')\n",
        "plt.legend()\n",
        "plt.title('Normalized Cross-Correlation Between Timesteps (benchmark) vs (overfiring)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9rJSOHgmZd0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming spike_data_bench and spike_data_over are loaded as numpy arrays with shape (32, 5)\n",
        "\n",
        "# Calculate the cross-correlation\n",
        "cross_corr = correlate(spike_data_bench, spike_data_over, mode='same')\n",
        "\n",
        "# Calculate the denominators for each row separately\n",
        "denominator = np.sqrt(np.sum(spike_data_bench ** 2, axis=1, keepdims=True) * np.sum(spike_data_over ** 2, axis=1, keepdims=True))\n",
        "\n",
        "# Avoid division by zero by setting denominator to 1 where it's zero\n",
        "denominator[denominator == 0] = 1\n",
        "\n",
        "# Normalize the cross-correlation to [-1, 1]\n",
        "cross_corr /= denominator\n",
        "\n",
        "# Calculate time lags for plotting\n",
        "time_lags = np.arange(-2, 3)  # Assuming 5 time steps in your data\n",
        "\n",
        "# Plot the cross-correlation\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(len(cross_corr)):\n",
        "    plt.plot(time_lags, cross_corr[i], label=f'Latent Dim {i + 1}')\n",
        "\n",
        "plt.xlabel('Time Lag')\n",
        "plt.ylabel('Normalized Cross-Correlation')\n",
        "plt.title('Cross-Correlation Between spike_data_bench and spike_data_over')\n",
        "# plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# - The normalized cross-correlation ranges from -1 to 1.\n",
        "# - A value of 1 indicates perfect positive correlation (identical patterns).\n",
        "# - A value of -1 indicates perfect negative correlation (completely opposite patterns).\n",
        "# - A value around 0 indicates no significant correlation (random or dissimilar patterns).\n",
        "\n",
        "# You can print or analyze the 'cross_corr' array to see how correlated the spike patterns are for each latent dimension.\n"
      ],
      "metadata": {
        "id": "ks8ZcC3Zdj2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ax1_epoch = 1\n",
        "# spike_data1_0001 = np.load(f\"out_en_epoch_{ax1_epoch}.npy\")\n",
        "# ax2_epoch = 50\n",
        "# spike_data2_0001 = np.load(f\"out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "# cross_correlations = []\n",
        "# for i in range(spike_data1_0001.shape[1]):\n",
        "#     cross_corr = correlate(spike_data1_0001[:, i], spike_data2_0001[:, i], mode='same')\n",
        "#     cross_correlations.append(cross_corr)\n",
        "\n",
        "# for i in range(5):\n",
        "#     plt.plot(cross_correlations[i], label=f'Timestep {i + 1}')\n",
        "\n",
        "# plt.xlabel('Latent dim')\n",
        "# plt.ylabel('Cross-Correlation')\n",
        "# plt.legend()\n",
        "# plt.title('Cross-Correlation Between Timesteps (overfiring)')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "8l_pQFiVHtDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Theory"
      ],
      "metadata": {
        "id": "wHhK-1jAa64_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine how much information is gained or lost when transitioning from spike_data1 to spike_data2.  You should analyze how spike patterns change from from spike_data1 to spike_data2 and calculate the information gain.\n",
        "# please provide me the code based on each instruction below, wisely:\n",
        "# 1. Data Normalization:\n",
        "# Ensure that both spike_data1 and spike_data2 have the same scale or normalization. You may need to rescale or normalize the data so that they are directly comparable.\n",
        "# 2. Data Discretization:\n",
        "# Convert the spike data into discrete probability distributions. You can do this by discretizing the data into time bins and assigning probabilities to each bin based on spike counts.\n",
        "# 3. Calculate Probability Distributions:\n",
        "# Calculate probability distributions for both spike_data1 and spike_data2. These distributions represent the likelihood of observing spikes in each time bin.\n",
        "# 4. Compute KL Divergence:\n",
        "# Use the KL divergence formula to calculate the divergence between the two distributions.\n",
        "# 5. Summation:\n",
        "# Sum up the KL divergence values across all time steps and latent dimensions to obtain a single value representing the total information gain or loss when transitioning from spike_data1 to spike_data2.\n",
        "\n",
        "spike_rate1=spike_data_bench\n",
        "spike_rate2=spike_data_over\n",
        "\n",
        "# # Step 1: Data Normalization\n",
        "# spike_data1_normalized = spike_rate1 / np.max(spike_rate1)\n",
        "# spike_data2_normalized = spike_rate2 / np.max(spike_rate2)\n",
        "\n",
        "# Step 2: Data Discretization\n",
        "num_bins = 10\n",
        "spike_data1_discretized = np.histogramdd(np.where(spike_rate1 > 0), bins=num_bins)[0]\n",
        "spike_data2_discretized = np.histogramdd(np.where(spike_rate2 > 0), bins=num_bins)[0]\n",
        "\n",
        "# Step 3: Calculate Probability Distributions\n",
        "epsilon = 1e-10  # Small constant to avoid division by zero\n",
        "spike_data1_prob = (spike_data1_discretized + epsilon) / np.sum(spike_data1_discretized + epsilon)\n",
        "spike_data2_prob = (spike_data2_discretized + epsilon) / np.sum(spike_data2_discretized + epsilon)\n",
        "\n",
        "# Step 4: Compute KL Divergence\n",
        "kl_divergence = kl_div(spike_data2_prob.flatten(), spike_data1_prob.flatten())\n",
        "print(\"kl_divergence:\", kl_divergence.shape)\n",
        "\n",
        "# Step 5: Summation\n",
        "total_information_gain = np.sum(kl_divergence)\n",
        "\n",
        "print(\"Total Information Gain:\", total_information_gain)\n"
      ],
      "metadata": {
        "id": "zMPPO4Ija5V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plot probability densities\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(spike_data1_prob, cmap='viridis', aspect='auto', origin='lower')\n",
        "plt.colorbar(label='Probability Density')\n",
        "plt.xlabel(\"Latent Dimension\")\n",
        "plt.ylabel(\"Time Step\")\n",
        "plt.title(\"Probability Density - benchmark\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(spike_data2_prob, cmap='viridis', aspect='auto', origin='lower')\n",
        "plt.colorbar(label='Probability Density')\n",
        "plt.xlabel(\"Latent Dimension\")\n",
        "plt.ylabel(\"Time Step\")\n",
        "plt.title(\"Probability Density - overfiring\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YdeVP2VBYnNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of bins for the histograms\n",
        "num_bins = 20\n",
        "\n",
        "# Create a figure with 5 subplots (one for each time step)\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "# Iterate through time steps\n",
        "for i in range(5):\n",
        "    # Create a histogram for spike_data_bench at the current time step\n",
        "    axes[i].hist(spike_data_bench[:, i], bins=num_bins, density=True, alpha=0.5, label='Benchmark')\n",
        "\n",
        "    # Create a histogram for spike_data2 at the current time step\n",
        "    axes[i].hist(spike_data_over[:, i], bins=num_bins, density=True, alpha=0.5, label='Over-firing')\n",
        "\n",
        "    # Set labels and title for the subplot\n",
        "    axes[i].set_xlabel(\"Latent Dimension\")\n",
        "    axes[i].set_ylabel(\"Probability Density\")\n",
        "    axes[i].set_title(f\"Time Step {i}\")\n",
        "    axes[i].legend()\n",
        "\n",
        "# Add a title for the entire figure\n",
        "plt.suptitle(\"Probability Density of Spike Patterns for Different Time Steps\")\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.85)  # Adjust the position of the main title\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# spike_rate1=spike_data_bench\n",
        "# spike_rate2=spike_data_over\n",
        "\n",
        "# # Define the number of bins for the histograms\n",
        "# num_bins = 10\n",
        "\n",
        "# # Create histograms for spike_data1\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for i in range(5):  # 5 time steps\n",
        "#     plt.subplot(2, 5, i + 1)\n",
        "#     plt.hist(spike_rate1[:, i], bins=num_bins, density=True, alpha=0.5)\n",
        "#     plt.title(f\"Benchmark, Time Step {i}\")\n",
        "#     plt.xlabel(\"Spikes (0 or 1)\")\n",
        "#     plt.ylabel(\"Probability Density\")\n",
        "\n",
        "# # Create histograms for spike_data2\n",
        "# for i in range(5):  # 5 time steps\n",
        "#     plt.subplot(2, 5, i + 6)\n",
        "#     plt.hist(spike_rate2[:, i], bins=num_bins, density=True, alpha=0.5)\n",
        "#     plt.title(f\"Over-firing, Time Step {i}\")\n",
        "#     plt.xlabel(\"Spikes (0 or 1)\")\n",
        "#     plt.ylabel(\"Probability Density\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "Jy48gHX2Dkhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an array to store KL Divergence values for each time step\n",
        "kl_divergence_per_step = []\n",
        "\n",
        "# Iterate through time steps\n",
        "for i in range(5):\n",
        "    # Calculate probability distributions for the two datasets at the current time step\n",
        "    prob_dist_bench, _ = np.histogram(spike_data_bench[:, i], bins=num_bins, density=True)\n",
        "    prob_dist_over, _ = np.histogram(spike_data_over[:, i], bins=num_bins, density=True)\n",
        "\n",
        "    # Ensure that the two distributions have the same length\n",
        "    min_len = min(len(prob_dist_bench), len(prob_dist_over))\n",
        "    prob_dist_bench = prob_dist_bench[:min_len]\n",
        "    prob_dist_over = prob_dist_over[:min_len]\n",
        "\n",
        "    # Calculate KL Divergence for the current time step\n",
        "    kl_divergence = entropy(prob_dist_bench, prob_dist_over)\n",
        "\n",
        "    kl_divergence_per_step.append(kl_divergence)\n",
        "\n",
        "# Plot the KL Divergence values for each time step\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(5), kl_divergence_per_step, marker='o')\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"KL Divergence\")\n",
        "plt.title(\"KL Divergence for Each Time Step\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print KL Divergence values\n",
        "for i, kl_divergence in enumerate(kl_divergence_per_step):\n",
        "    print(f\"Time Step {i}: KL Divergence = {kl_divergence}\")\n",
        "\n",
        "# ------------------------------ information gain or loss\n",
        "# Flatten the kl_divergence_per_step list to include all time steps\n",
        "kl_divergence_flat = np.array(kl_divergence_per_step)\n",
        "\n",
        "# Sum up the KL divergence values across all time steps and latent dimensions\n",
        "total_kl_divergence = np.sum(kl_divergence_flat)\n",
        "\n",
        "# Print the total KL Divergence\n",
        "print(f\"Total KL Divergence: {total_kl_divergence}\")\n"
      ],
      "metadata": {
        "id": "4YGd17N-UJi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # 2. Probability Distributions\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# for i in range(4):\n",
        "#     plt.subplot(2, 2, i + 1)\n",
        "#     sns.histplot(spike_data1[i], bins=20, label=\"Benchmark\", color='blue', kde=True)\n",
        "#     sns.histplot(spike_data2[i], bins=20, label=\"Over-firing\", color='red', kde=True)\n",
        "#     plt.xlabel(f\"Latent Dimension {i}\")\n",
        "#     plt.ylabel(\"Probability Density\")\n",
        "#     plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # 3. Heatmap\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.heatmap(kl_divergence, cmap=\"YlGnBu\", annot=True, fmt=\".3f\")\n",
        "# plt.xlabel(\"Latent Dimension\")\n",
        "# plt.ylabel(\"Time Step\")\n",
        "# plt.title(\"KL Divergence Heatmap\")\n",
        "# plt.show()\n",
        "\n",
        "# # 4. Time Series Plot\n",
        "# time_steps = np.arange(4)\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.plot(time_steps, kl_divergence.mean(axis=1), marker='o')\n",
        "# plt.xlabel(\"Time Step\")\n",
        "# plt.ylabel(\"Mean KL Divergence\")\n",
        "# plt.title(\"KL Divergence Over Time\")\n",
        "# plt.xticks(time_steps)\n",
        "# plt.show()\n",
        "\n",
        "# # 5. Latent Dimension Comparison\n",
        "# latent_dim_to_compare = 0  # Choose a specific latent dimension to compare\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# sns.histplot(spike_data1[:, latent_dim_to_compare], label=\"Benchmark\", color='blue', kde=True)\n",
        "# sns.histplot(spike_data2[:, latent_dim_to_compare], label=\"Over-firing\", color='red', kde=True)\n",
        "# plt.xlabel(f\"Latent Dimension {latent_dim_to_compare}\")\n",
        "# plt.ylabel(\"Probability Density\")\n",
        "# plt.legend()\n",
        "# plt.title(f\"Latent Dimension {latent_dim_to_compare} Comparison\")\n",
        "# plt.show()\n",
        "\n",
        "# # 7. Summary Statistics\n",
        "# mean_kl_divergence = kl_divergence.mean()\n",
        "# median_kl_divergence = np.median(kl_divergence)\n",
        "# std_kl_divergence = kl_divergence.std()\n",
        "# print(f\"Mean KL Divergence: {mean_kl_divergence:.3f}\")\n",
        "# print(f\"Median KL Divergence: {median_kl_divergence:.3f}\")\n",
        "# print(f\"Standard Deviation of KL Divergence: {std_kl_divergence:.3f}\")\n",
        "\n",
        "# # 8. Anomaly Detection Plot (set a threshold)\n",
        "# threshold = 0.1  # Adjust the threshold as needed\n",
        "# anomalies = np.where(kl_divergence > threshold, 1, 0)\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.heatmap(anomalies, cmap=\"Reds\", annot=True, fmt=\".0f\")\n",
        "# plt.xlabel(\"Latent Dimension\")\n",
        "# plt.ylabel(\"Time Step\")\n",
        "# plt.title(f\"Anomalies (Threshold = {threshold})\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "54vwHGfJr4qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpL7zyJXQvyE"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Encoder plotting each epochs output of out_en:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6JBtVnNX5GU"
      },
      "source": [
        "## Heatmap or Cmap plot of the Endoer spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ7gLK-kIOeu"
      },
      "outputs": [],
      "source": [
        "# ax1_epoch = 1\n",
        "# spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "# ax2_epoch = 50\n",
        "# spike_data2 = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "ax1_epoch = 50\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "cmap = plt.get_cmap(\"binary\")\n",
        "ax1.imshow(spike_data1, cmap=cmap, aspect=\"auto\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "cmap = plt.get_cmap(\"binary\")\n",
        "ax2.imshow(spike_data2, cmap=cmap, aspect=\"auto\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWyA4kYyX1CQ"
      },
      "source": [
        "## Raster plot of the Encoder spikes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax1_epoch = 50\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "# Find the coordinates of spike events\n",
        "coords = np.argwhere(spike_data1)\n",
        "ax1.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "coords = np.argwhere(spike_data2)\n",
        "ax2.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KdTScRM4EQqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqPaeRmVJvJo"
      },
      "outputs": [],
      "source": [
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "# Find the coordinates of spike events\n",
        "coords = np.argwhere(spike_data1)\n",
        "ax1.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "coords = np.argwhere(spike_data2)\n",
        "ax2.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxp-zd_SXux4"
      },
      "source": [
        "## 3D plot of the Encoder sppikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIY7x0Z9WutH"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create the first subplot for spike_data1\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "x1, y1 = np.meshgrid(np.arange(spike_data1.shape[1]), np.arange(spike_data1.shape[0]))\n",
        "z1 = spike_data1.flatten()\n",
        "ax1.scatter(x1.flatten(), y1.flatten(), z1, s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlabel(\"Time Steps\")\n",
        "ax1.set_ylabel(\"Latent dim\")\n",
        "ax1.set_zlabel(\"Spike Intensity\", labelpad=2)\n",
        "ax1.set_title(f\"Spike Activity in out_en (Epoch {ax1_epoch})\")\n",
        "\n",
        "# Create the second subplot for spike_data2\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "x2, y2 = np.meshgrid(np.arange(spike_data2.shape[1]), np.arange(spike_data2.shape[0]))\n",
        "z2 = spike_data2.flatten()\n",
        "ax2.scatter(x2.flatten(), y2.flatten(), z2, s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlabel(\"Time Steps\")\n",
        "ax2.set_ylabel(\"Latent dim\")\n",
        "ax2.set_zlabel(\"Spike Intensity\", labelpad=0.1)\n",
        "ax2.set_title(f\"Spike Activity in out_en (Epoch {ax2_epoch})\")\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "# Manually adjust the position of the second subplot\n",
        "# pos = ax2.get_position()\n",
        "# pos.x0 += 0.1  # Adjust the left position\n",
        "# pos.x1 += 0.1  # Adjust the right position\n",
        "# ax2.set_position(pos)\n",
        "\n",
        "# Save and display the plot\n",
        "filename = f\"Enc_b_syn_{beta_syn}_spike_epochs_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_3D.pdf\"\n",
        "plt.savefig(filename, format=\"pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G40wCzd5tqd0"
      },
      "source": [
        "# Decoder plotting each epochs output of out:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-c7Bkw7Zp7U"
      },
      "source": [
        "## cmap plotting Decoder of (for one digit at the last time t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXZ80BPRSgoT"
      },
      "outputs": [],
      "source": [
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_epoch_{ax1_epoch}.npy\")\n",
        "spike_data1 = spike_data1[0,0,:, :]\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_epoch_{ax2_epoch}.npy\")\n",
        "spike_data2 = spike_data2[0,0,:, :]\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "cmap = plt.get_cmap(\"binary\")\n",
        "ax1.imshow(spike_data1, cmap=cmap, aspect=\"auto\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"width\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"length\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out (for one digit at the last time t)\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "cmap = plt.get_cmap(\"binary\")\n",
        "ax2.imshow(spike_data2, cmap=cmap, aspect=\"auto\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"width\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"length\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out (for one digit at the last time t)\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Dec_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBZS5QZ7Z0Kc"
      },
      "source": [
        "## Raster plot Decoder of (for one digit at the last time t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV9x0uMSTqJm"
      },
      "outputs": [],
      "source": [
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_epoch_{ax1_epoch}.npy\")\n",
        "spike_data1 = spike_data1[0,0,:, :]\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_epoch_{ax2_epoch}.npy\")\n",
        "spike_data2 = spike_data2[0,0,:, :]\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "coords = np.argwhere(spike_data1)\n",
        "ax1.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"width\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"length\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out (for one digit at the last time t)\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "coords = np.argwhere(spike_data2)\n",
        "ax2.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"width\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"length\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out (for one digit at the last time t)\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Dec_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f6gFpA1Z5Nu"
      },
      "source": [
        "## 3D plotting Decoder of (for one digit at the last time t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSbkKPrvZ7my"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_epoch_{ax1_epoch}.npy\")\n",
        "spike_data1 = spike_data1[0,0,:, :]\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_epoch_{ax2_epoch}.npy\")\n",
        "spike_data2 = spike_data2[0,0,:, :]\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create the first subplot for spike_data1\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "x1, y1 = np.meshgrid(np.arange(spike_data1.shape[1]), np.arange(spike_data1.shape[0]))\n",
        "z1 = spike_data1.flatten()\n",
        "ax1.scatter(x1.flatten(), y1.flatten(), z1, s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlabel(\"width\")#, fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"length\")#, fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_zlabel(\"Spike Intensity\", labelpad=2)#, fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(f\"Spike Activity in out_en (Epoch {ax1_epoch})\")\n",
        "\n",
        "# Create the second subplot for spike_data2\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "x2, y2 = np.meshgrid(np.arange(spike_data2.shape[1]), np.arange(spike_data2.shape[0]))\n",
        "z2 = spike_data2.flatten()\n",
        "ax2.scatter(x2.flatten(), y2.flatten(), z2, s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlabel(\"width\")#, fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"length\")#, fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_zlabel(\"Spike Intensity\", labelpad=0.1)#, fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(f\"Spike Activity in out_en (Epoch {ax2_epoch})\")\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "# Manually adjust the position of the second subplot\n",
        "# pos = ax2.get_position()\n",
        "# pos.x0 += 0.1  # Adjust the left position\n",
        "# pos.x1 += 0.1  # Adjust the right position\n",
        "# ax2.set_position(pos)\n",
        "\n",
        "# Save and display the plot\n",
        "filename = f\"Dec_b_syn_{beta_syn}_spike_epochs_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_3D.pdf\"\n",
        "plt.savefig(filename, format=\"pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwXV7LJv5pOk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the 'out' tensor for a specific epoch\n",
        "epoch_number = 50  # Change this to the epoch you want to visualize\n",
        "out_filename = f\"out_epoch_{epoch_number}.npy\"\n",
        "out = np.load(out_filename)\n",
        "print(\"out:\",out.shape)  # Using .shape attribute\n",
        "\n",
        "# Reshape the data if needed\n",
        "# For example, if 'out' is a 4D tensor and you want to visualize a specific slice\n",
        "# along one of the dimensions, you can reshape it to a 2D matrix for plotting.\n",
        "\n",
        "# Example reshape (adjust dimensions as needed)\n",
        "out = out.reshape(-1, out.shape[-1])  # Flatten all dimensions except the last one\n",
        "print(\"out.reshape(-1, out.shape[-1]): \",out.shape)  # Using .shape attribute\n",
        "# or\n",
        "# print(out.size())  # Using .size() method\n",
        "# Create a plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(out, cmap='viridis', aspect='auto', interpolation='nearest')\n",
        "plt.xlabel(\"Dimension\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.title(\"Decoder Representations (for one digit at the last time t)\" + \" [epoch: \" + str(epoch_number) + \"]\")\n",
        "\n",
        "plt.colorbar(label='Spike Count')\n",
        "\n",
        "filename = f\"Dec_b_syn_{beta_syn}_spike_epochs_{epoch_number}_of_{epochs}_(8000x32).pdf\"\n",
        "plt.savefig(filename, format=\"pdf\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4z3jtT4MQaS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrHvc2tE/bfi+HzPW2u0JB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}