{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrezakhodashenas/PhD-NN/blob/output-spikes/OutputSpikes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6HiiAfjcTIpU"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8LJRXXN6gygC"
      },
      "outputs": [],
      "source": [
        "# !pip install brian2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOq6G-QBejwM",
        "outputId": "3f0544c1-8ef1-4e32-dae4-448ff350d444"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czXuC1x8el5J",
        "outputId": "3d554dca-c37d-4023-8ae6-f461f41e5ddd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.9-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.1.2-py3-none-any.whl (764 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.8/764.8 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.9.0 pytorch-lightning-2.0.9 torchmetrics-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTEDV8wqLZXr",
        "outputId": "2215bdaa-67ae-4637-b7ef-6b8cb27b0030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.7.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.23.5)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-0.2.0-py3-none-any.whl (21 kB)\n",
            "Collecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-0.2.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->snntorch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1.0->snntorch) (16.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Installing collected packages: nir, nirtorch, snntorch\n",
            "Successfully installed nir-0.2.0 nirtorch-0.2.1 snntorch-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGPDVXvjdGsl",
        "outputId": "f6e6906f-f337-40f2-a072-e45ed730e487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7xJifRLMc2Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KehrXuVuPiAt"
      },
      "source": [
        "## set seeds for PyTorch and Numpy to ensure reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NbSadcLWPedr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for Python, Numpy, and Torch for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Additional steps if you're using GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UyXSF4IqLFND"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pickle\n",
        "import matplotlib.animation as animation\n",
        "from scipy.integrate import simps\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import os, sys, time, datetime, json, random\n",
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import utils as utls\n",
        "from snntorch import utils\n",
        "from snntorch import surrogate\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import auc\n",
        "from torchsummary import summary\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn as nn\n",
        "from scipy.stats import entropy\n",
        "from scipy.special import kl_div\n",
        "from torch.autograd import Variable\n",
        "# import spikeflow as snn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.autograd.set_detect_anomaly(True)"
      ],
      "metadata": {
        "id": "ZDOWUXysMxFD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nOyTY0dE50vG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bdf3af-aac2-4e3b-cc8c-656bbbb4fe43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 346351696.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 12618301.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 138606128.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1349569.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# /////////////////////# Building the Autoencoder\n",
        "#-------------------DataLoaders.  using the MNIST dataset\n",
        "\n",
        "# dataloader arguments\n",
        "batch_size = 250\n",
        "data_path='/data/mnist'\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# /////////////////////////////////# Define a transform\n",
        "input_size = 32 # resizing the original MNIST from 28 to 32\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "#------------------------------------------- Load MNIST\n",
        "# Training data\n",
        "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Testing data\n",
        "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Ha5yn2VjO39Q"
      },
      "outputs": [],
      "source": [
        "# creating directories where we can save the original and reconstructed images for training and testing:\n",
        "# create training/ and testing/ folders in the chosen path\n",
        "if not os.path.isdir('figures/training'):\n",
        "    os.makedirs('figures/training')\n",
        "if not os.path.isdir('figures/binarytraining'):\n",
        "    os.makedirs('figures/binarytraining')\n",
        "\n",
        "if not os.path.isdir('figures/testing'):\n",
        "    os.makedirs('figures/testing')\n",
        "if not os.path.isdir('figures/binarytesting'):\n",
        "    os.makedirs('figures/binarytesting')\n",
        "\n",
        "\n",
        "if not os.path.isdir('Saved_Trained_Checkpoints/'):\n",
        "    os.makedirs('Saved_Trained_Checkpoints/')\n",
        "\n",
        "if not os.path.isdir('Output_Spikes/'):\n",
        "    os.makedirs('Output_Spikes/')\n",
        "\n",
        "if not os.path.isdir('Enc_syn_Spikes/'):\n",
        "    os.makedirs('Enc_syn_Spikes/')\n",
        "\n",
        "\n",
        "if not os.path.isdir('Intermediate_Lyrs/'):\n",
        "    os.makedirs('Intermediate_Lyrs/')\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcZMZrCoKFaq",
        "outputId": "fb1a2fea-4ac4-4f85-f81e-48433487bf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Tesla T4 (cuda)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyQvxnL5umDu"
      },
      "source": [
        "### To manipulate and test (Working for Encoder and Decoder outputs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2VQHpS2xufu0"
      },
      "outputs": [],
      "source": [
        "# class SAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "\n",
        "#         # Encoder\n",
        "#         self.encoder = nn.Sequential(\n",
        "#                             nn.Conv2d(1, 32, 3,padding = 1,stride=2), # Conv Layer 1\n",
        "#                             # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n",
        "#                                 # bias=True, padding_mode='zeros',  device=None, dtype=None)\n",
        "#                             nn.BatchNorm2d(32),\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             # snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             nn.Conv2d(32, 64, 3,padding = 1,stride=2), # Conv Layer 2\n",
        "#                             nn.BatchNorm2d(64),\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             # snn.Alpha(alpha=alpha1, beta=beta1, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.Conv2d(64, 128, 3,padding = 1,stride=2), # Conv Layer 3\n",
        "#                             nn.BatchNorm2d(128),\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             # snn.Alpha(alpha=alpha11, beta=beta11, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.Flatten(start_dim = 1, end_dim = 3), #Flatten convolutional output\n",
        "#                             nn.Linear(128*4*4, latent_dim), # Fully connected linear layer\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "#                             # snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "#                             )\n",
        "\n",
        "\n",
        "#         self.latent_dim = latent_dim #dimensions of the encoded z-space data\n",
        "\n",
        "#         #ve from the flattened encoded representation (latent_dim) back to a tensor representation to\n",
        "#             # use in transposed convolution.\n",
        "#           # To do so, we need to run an additional fully-connected linear layer transforming the data back into a tensor of 128 x 4 x 4:\n",
        "\n",
        "#         self.linearNet= nn.Sequential(\n",
        "#                                       nn.Linear(latent_dim,128*4*4),\n",
        "#                                       snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "#                                       #snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#       #  The decoder, with three transposed convolutional (nn.ConvTranspose2d) layers and one linear output layer.\n",
        "#       # Although we converted the latent data back into tensor form for convolution, we still need to Unflatten it to a tensor of 128 x 4 x 4,\n",
        "#         # as the input to the network is 1 dimensional.  This is done using nn.Unflatten in the first line of the Decoder:\n",
        "#         # Decoder:\n",
        "#         self.decoder = nn.Sequential(\n",
        "#                             nn.Unflatten(1,(128,4,4)), #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             # snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             nn.BatchNorm2d(64),\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             # snn.Alpha(alpha=alpha2, beta=beta2, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             nn.BatchNorm2d(32),\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             # snn.Alpha(alpha=alpha22, beta=beta22, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#                             # snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=thresh) #---------------------------------------------- ADDED\n",
        "#                             # snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#                             )\n",
        "#         # final Leaky layer, our spiking threshold (thresh) is set extremely high. This is a neat trick in snnTorch, which allows the neuron\n",
        "#         # membrane in the final layer to continuously be updated, without ever reaching a spiking threshold.\n",
        "\n",
        "#         # using the membrane potential output from the final layer for the image reconstruction.\n",
        "#             # snnTorch allows us to use either the spikes or membrane potential of each neuron in training.\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         utils.reset(self.encoder) #need to reset the hidden states of LIF\n",
        "#         utils.reset(self.decoder)\n",
        "#         utils.reset(self.linearNet)\n",
        "\n",
        "#     #-----------------------------encode\n",
        "#         spk_mem=[];\n",
        "#         spk_rec=[];\n",
        "#         spk_rec_syn=[];\n",
        "#         encoder_mem=[];\n",
        "#         spk_rec_dec=[];\n",
        "#         spk_mem_dec=[];\n",
        "#         for step in range(num_steps): #for t in time\n",
        "#             spk_x, mem_x = self.encode(x) #Output spike trains and neuron membrane states\n",
        "#             # print(\"Size of = self.encode(x):\", self.encode(x).size())\n",
        "#             # print(\"len of = self.encode(x):\", len(self.encode(x)))\n",
        "#             spk_rec.append(spk_x)\n",
        "#             spk_mem.append(mem_x)\n",
        "\n",
        "#         spk_rec=torch.stack(spk_rec,dim=2) # stack spikes in second tensor dimension\n",
        "#         spk_mem=torch.stack(spk_mem,dim=2) # stack membranes in second tensor dimension\n",
        "# # torch.stack joins (concatenates) a sequence of tensors (two or more tensors) along a new dimension.\n",
        "#         # print(\"Size of spk_rec:\", spk_rec.size())   # Size of spk_rec: torch.Size([250, 32, 5])\n",
        "#         # print(\"Size of spk_mem:\", spk_mem.size()) # Size of spk_mem: torch.Size([250, 32, 5])\n",
        "#         # out_en = spk_rec[:,:,-1]     #//////////////////////////////////////ADDED---------------- shows  batch size (250) different examples on (32) channel number\n",
        "#         out_en = spk_rec[0, :,:] #//////////////////////////////////////ADDED------------ latent dim (32) and time (num_steps)(5)\n",
        "#         # print(\"Size of out_en:\", out_en.size()) #Size of out_en: torch.Size([250, 32])\n",
        "\n",
        "#     #------------------------------decode\n",
        "#         spk_mem2=[];\n",
        "#         spk_rec2=[];\n",
        "#         decoded_x=[];\n",
        "#         spk_x_dec=[];\n",
        "#         mem_x_dec=[];\n",
        "#         for step in range(num_steps): #for t in time\n",
        "#             # spk_x_dec, mem_x_dec = self.decode(spk_rec[...,step]) #//////////////////////////ADDED\n",
        "#             x_recon, x_mem_recon = self.decode(spk_rec[...,step])\n",
        "\n",
        "#             spk_rec2.append(x_recon)\n",
        "#             spk_mem2.append(x_mem_recon)\n",
        "\n",
        "#             # spk_rec_dec.append(spk_x_dec)   #//////////////////////////ADDED Size of spk_rec_dec: torch.Size([250, 1, 5, 32, 32])\n",
        "#             # spk_mem_dec.append(mem_x_dec)   #//////////////////////////ADDED\n",
        "\n",
        "#         # spk_rec_dec=torch.stack(spk_rec_dec,dim=2) #//////////////////////////ADDED\n",
        "#         # spk_mem_dec=torch.stack(spk_mem_dec,dim=2) #//////////////////////////ADDED\n",
        "#         spk_rec2=torch.stack(spk_rec2,dim=4)\n",
        "#         spk_mem2=torch.stack(spk_mem2,dim=4)\n",
        "\n",
        "#         # out = spk_mem2[:,:,:,:,-1] #return the membrane potential of the output neuron at t = -1 (last t)\n",
        "#         # out = spk_rec2[:,:,:,:,-1]  #//////////////////////////ADDED #  Size of out: torch.Size([250, 1, 32, 32])\n",
        "#         # out = spk_rec_dec[:,:,-1] #//////////////////////////ADDED  # Size of out: torch.Size([250, 1, 32, 32])\n",
        "#         # out = spk_rec_dec[:,0,:,-1] #//////////////////////////ADDED  # Size of out: torch.Size([250, 5, 32])\n",
        "#         # out = spk_rec_dec[:,0,0,:,-1] #//////////////////////////ADDED  # Size of out: torch.Size([250, 32])\n",
        "#         # out = spk_rec_dec[:,-1] #//////////////////////////ADDED  Size of out: torch.Size([250, 5, 32, 32])\n",
        "#         # out = spk_rec2[0,0,:,:] #//////////////////////////ADDED Now working with [250, 1, 32, 32]) because it is (torch.Size([32, 32, 5]))\n",
        "#         out = spk_mem2[:,:,:,:,-1]  #//////////////////////////ADDED #  Size of out: torch.Size([250, 1, 32, 32])--------------- for one digit at the last time t\n",
        "\n",
        "#         # print(\"Size of out:\", out.size())\n",
        "\n",
        "#         # Save the out_en tensor\n",
        "#         self.out_en = out_en\n",
        "#         self.out = out\n",
        "\n",
        "#         return out, out_en\n",
        "\n",
        "\n",
        "#     def encode(self,x):\n",
        "#       spk_latent_x, mem_latent_x = self.encoder(x)\n",
        "#       return spk_latent_x, mem_latent_x, syn1, memsyn1\n",
        "\n",
        "#     def decode(self,x):\n",
        "#         spk_x, mem_x = self.linearNet(x) #convert latent dimension back to total size of features in encoder final layer\n",
        "#         spk_x2, mem_x2 = self.decoder(spk_x)\n",
        "#         return spk_x2, mem_x2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with \"out\" and \"out_en\":\n"
      ],
      "metadata": {
        "id": "vIXf5IFdcTt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         # Encoder\n",
        "#         self.encoder = nn.Sequential(\n",
        "#                             nn.Conv2d(1, 32, 3,padding = 1,stride=2), # Conv Layer 1\n",
        "#                             nn.BatchNorm2d(32),\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.Conv2d(32, 64, 3,padding = 1,stride=2), # Conv Layer 2\n",
        "#                             nn.BatchNorm2d(64),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             nn.Conv2d(64, 128, 3,padding = 1,stride=2), # Conv Layer 3\n",
        "#                             nn.BatchNorm2d(128),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "#                             nn.Flatten(start_dim = 1, end_dim = 3), #Flatten convolutional output\n",
        "#                             nn.Linear(128*4*4, latent_dim), # Fully connected linear layer\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "#                             )\n",
        "#         self.latent_dim = latent_dim #dimensions of the encoded z-space data\n",
        "#         self.linearNet= nn.Sequential(\n",
        "#                                       nn.Linear(latent_dim,128*4*4),\n",
        "#                                       snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "\n",
        "#         # Decoder:\n",
        "#         self.decoder = nn.Sequential(\n",
        "#                             nn.Unflatten(1,(128,4,4)), #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             nn.BatchNorm2d(64),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             nn.BatchNorm2d(32),\n",
        "#                             snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "#                             nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "#                             snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#                             )\n",
        "#     def forward(self, x):\n",
        "#         utils.reset(self.encoder) #need to reset the hidden states of LIF\n",
        "#         utils.reset(self.decoder)\n",
        "#         utils.reset(self.linearNet)\n",
        "\n",
        "#     #-----------------------------encode\n",
        "#         spk_mem=[];\n",
        "#         spk_rec=[];\n",
        "#         spk_rec_syn=[];\n",
        "#         encoder_mem=[];\n",
        "#         spk_rec_dec=[];\n",
        "#         spk_mem_dec=[];\n",
        "#         for step in range(num_steps): #for t in time\n",
        "#             spk_x, mem_x = self.encode(x) #Output spike trains and neuron membrane states\n",
        "#             # print(\"spk_x in spk_x, mem_x = self.encode(x): \",spk_x.size())    #                       torch.Size([250, 32])\n",
        "#             # print(\"mem_x in spk_x, mem_x = self.encode(x): \",mem_x.size())    #                       torch.Size([250, 32])\n",
        "#             spk_rec.append(spk_x)\n",
        "#             spk_mem.append(mem_x)\n",
        "#             # print(\"len of spk_rec in spk_rec.append(spk_x): \",len(spk_rec))\n",
        "#             # print(\"len of spk_mem in spk_rec.append(mem_x): \",len(spk_mem))\n",
        "\n",
        "#         spk_rec=torch.stack(spk_rec,dim=2) # stack spikes in second tensor dimension # ----------------spk_rec in torch.stack(spk_rec,dim=2):  torch.Size([250, 32, 5])\n",
        "#         # print(\"----------------spk_rec in torch.stack(spk_rec,dim=2): \",spk_rec.size())\n",
        "#         spk_mem=torch.stack(spk_mem,dim=2) # stack membranes in second tensor dimension # ----------------spk_mem in torch.stack(spk_mem,dim=2):  torch.Size([250, 32, 5])\n",
        "#         # print(\"----------------spk_mem in torch.stack(spk_mem,dim=2): \",spk_mem.size())\n",
        "#         # out_en = spk_rec[0, :,:] #//////////////////////////////////////ADDED------------ latent dim (32) and time (num_steps)(5) # out_en in spk_rec[0, :,:]:----------- torch.Size([32, 5])\n",
        "#         # print(\"out_en in spk_rec[0, :,:]:-----------\" , out_en.size())\n",
        "#         out_en = spk_rec[...,step]\n",
        "#         # print(\"spk_rec[...,step]:-----------\" , spk_rec[...,step].size()) # spk_rec[...,step]:----------- torch.Size([250, 32])       input of the latent and then decoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     #------------------------------decode\n",
        "#         spk_mem2=[];\n",
        "#         spk_rec2=[];\n",
        "#         decoded_x=[];\n",
        "#         spk_x_dec=[];\n",
        "#         mem_x_dec=[];\n",
        "#         for step in range(num_steps): #for t in time                                   from decoder: ([250, 1, 32, 32])\n",
        "#             x_recon, x_mem_recon = self.decode(spk_rec[...,step])                             # x_recon in self.decode(spk_rec[...,step]): torch.Size([250, 1, 32, 32])\n",
        "#             # print(\"x_recon in self.decode(spk_rec[...,step]):\" , x_recon.size())            # x_mem_recon in self.decode(spk_rec[...,step]): torch.Size([250, 1, 32, 32])\n",
        "#             # print(\"x_mem_recon in self.decode(spk_rec[...,step]):\" , x_mem_recon.size())\n",
        "\n",
        "#             spk_rec2.append(x_recon)\n",
        "#             spk_mem2.append(x_mem_recon)\n",
        "#             # print(\"len of spk_rec2.append(x_recon)\",len(spk_rec2))\n",
        "#             # print(\"len of spk_mem2.append(x_recon)\",len(spk_mem2))\n",
        "\n",
        "#         spk_rec2=torch.stack(spk_rec2,dim=4)\n",
        "#         spk_mem2=torch.stack(spk_mem2,dim=4)\n",
        "#         # print(\"spk_rec2 in torch.stack(spk_rec2,dim=4):\", spk_rec2.size())          # spk_rec2 in torch.stack(spk_rec2,dim=4): torch.Size([250, 1, 32, 32, 5])\n",
        "#         # print(\"spk_mem2 in torch.stack(spk_rec2,dim=4):\", spk_mem2.size())          # spk_mem2 in torch.stack(spk_rec2,dim=4): torch.Size([250, 1, 32, 32, 5])\n",
        "\n",
        "#         out = spk_mem2[:,:,:,:,-1]  #//////////////////////////ADDED #  Size of out: torch.Size([250, 1, 32, 32])--------------- for one digit at the last time t\n",
        "\n",
        "#         # self.out_en = out_en\n",
        "#         # self.out = out\n",
        "\n",
        "#         return out, out_en\n",
        "\n",
        "\n",
        "#     def encode(self,x):\n",
        "#       spk_latent_x, mem_latent_x = self.encoder(x)\n",
        "#       return spk_latent_x, mem_latent_x\n",
        "\n",
        "#     def decode(self,x):\n",
        "#         spk_x, mem_x = self.linearNet(x) #convert latent dimension back to total size of features in encoder final layer\n",
        "#         spk_x2, mem_x2 = self.decoder(spk_x)\n",
        "#         return spk_x2, mem_x2"
      ],
      "metadata": {
        "id": "MuOth1ZabvgL"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # encoder_layers = [\n",
        "        #     ('conv1', nn.Conv2d(1, 32, 3, padding=1, stride=2)),  # Conv Layer 1\n",
        "        #     ('batchnorm1', nn.BatchNorm2d(32)),\n",
        "        #     ('leaky1', snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),\n",
        "        #     ('conv2', nn.Conv2d(32, 64, 3, padding=1, stride=2)),  # Conv Layer 2\n",
        "        #     ('batchnorm2', nn.BatchNorm2d(64)),\n",
        "        #     ('synaptic1', snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),  # SNN TORCH LIF NEURON\n",
        "        #     ('conv3', nn.Conv2d(64, 128, 3, padding=1, stride=2)),  # Conv Layer 3\n",
        "        #     ('batchnorm3', nn.BatchNorm2d(128)),\n",
        "        #     ('synaptic2', snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),  # SNN TORCH LIF NEURON\n",
        "        #     ('flatten', nn.Flatten(start_dim=1, end_dim=3)),  # Flatten convolutional output\n",
        "        #     ('linear', nn.Linear(128 * 4 * 4, latent_dim)),  # Fully connected linear layer\n",
        "        #     ('leaky2', snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh))\n",
        "        # ]\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "                            nn.Conv2d(1, 32, 3,padding = 1,stride=2), # Conv Layer 1\n",
        "                            nn.BatchNorm2d(32),\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.Conv2d(32, 64, 3,padding = 1,stride=2), # Conv Layer 2\n",
        "                            nn.BatchNorm2d(64),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, threshold=thresh), #SNN TORCH LIF NEURON\n",
        "                            nn.Conv2d(64, 128, 3,padding = 1,stride=2), # Conv Layer 3\n",
        "                            nn.BatchNorm2d(128),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "                            nn.Flatten(start_dim = 1, end_dim = 3), #Flatten convolutional output\n",
        "                            nn.Linear(128*4*4, latent_dim), # Fully connected linear layer\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "                            )\n",
        "\n",
        "\n",
        "        self.latent_dim = latent_dim #dimensions of the encoded z-space data\n",
        "        self.linearNet= nn.Sequential(\n",
        "                                      nn.Linear(latent_dim,128*4*4),\n",
        "                                      snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "\n",
        "        # Decoder:\n",
        "        self.decoder = nn.Sequential(\n",
        "                            nn.Unflatten(1,(128,4,4)), #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            nn.BatchNorm2d(64),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            nn.BatchNorm2d(32),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "                            )\n",
        "    def forward(self, x):\n",
        "        utils.reset(self.encoder) #need to reset the hidden states of LIF\n",
        "        utils.reset(self.decoder)\n",
        "        utils.reset(self.linearNet)\n",
        "\n",
        "    #-----------------------------encode\n",
        "        spk_mem=[];\n",
        "        spk_rec=[];\n",
        "        spk_rec_syn=[];\n",
        "        encoder_mem=[];\n",
        "        spk_rec_dec=[];\n",
        "        spk_mem_dec=[];\n",
        "        enc5_rec = [];\n",
        "\n",
        "\n",
        "     #------------------------------ intermediate layers\n",
        "\n",
        "        # for step in range(num_steps):\n",
        "        #     enc5 = self.encoder[5](x)             #  enc5 shape: torch.Size([250, 1, 32, 32])\n",
        "        #     enc5_rec.append(enc5)\n",
        "        # enc5_rec = torch.stack(enc5_rec, dim=2)            #   enc5_rec size: torch.Size([250, 1, 5, 32, 32])\n",
        "        # Enc_syn_1 = enc5_rec[:, :, -1]                      # #   torch.Size([250, 1, 32, 32])\n",
        "\n",
        "     #------------------------------ encode\n",
        "        for step in range(num_steps):\n",
        "            spk_x, mem_x = self.encoder(x)              # spk_x size: ([250, 32])  ,   mem_x size: ([250, 32])  , x.shape : torch.Size([250, 1, 32, 32])\n",
        "            spk_rec.append(spk_x)\n",
        "            spk_mem.append(mem_x)\n",
        "\n",
        "        spk_rec=torch.stack(spk_rec,dim=2) # stack spikes in second tensor dimension # ----------------spk_rec in torch.stack(spk_rec,dim=2):  torch.Size([250, 32, 5])\n",
        "        spk_mem=torch.stack(spk_mem,dim=2) # stack membranes in second tensor dimension # ----------------spk_mem in torch.stack(spk_mem,dim=2):  torch.Size([250, 32, 5])\n",
        "        out_en = spk_rec[...,step]\n",
        "\n",
        "        # print(\"out_en= spk_rec[...,step]:-----------\" , spk_rec[...,step].size()) # spk_rec[...,step]:----------- torch.Size([250, 32])       input of the latent and then decoder\n",
        "\n",
        "     #------------------------------decode\n",
        "        spk_mem2=[];\n",
        "        spk_rec2=[];\n",
        "        decoded_x=[];\n",
        "        spk_x_dec=[];\n",
        "        mem_x_dec=[];\n",
        "        for step in range(num_steps): #for t in time                           #        from decoder: ([250, 1, 32, 32])\n",
        "            x_recon, x_mem_recon = self.decode(spk_rec[...,step])\n",
        "            spk_rec2.append(x_recon)\n",
        "            spk_mem2.append(x_mem_recon)\n",
        "\n",
        "        spk_rec2=torch.stack(spk_rec2,dim=4)\n",
        "        spk_mem2=torch.stack(spk_mem2,dim=4)\n",
        "\n",
        "        out = spk_mem2[:,:,:,:,-1]\n",
        "\n",
        "        self.out_en = out_en\n",
        "        self.out = out\n",
        "\n",
        "        return out, out_en\n",
        "        # return out, Enc_syn_1\n",
        "\n",
        "    def encode(self,x):\n",
        "      spk_latent_x, mem_latent_x = self.encoder(x)\n",
        "      return spk_latent_x, mem_latent_x\n",
        "\n",
        "\n",
        "    def decode(self,x):\n",
        "        spk_x, mem_x = self.linearNet(x) #convert latent dimension back to total size of features in encoder final layer\n",
        "        spk_x2, mem_x2 = self.decoder(spk_x)\n",
        "        return spk_x2, mem_x2\n",
        "\n"
      ],
      "metadata": {
        "id": "U_D8NuZnxpXG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape = (250, 1, 32, 32)\n",
        "\n",
        "# # Create a random tensor with the specified shape\n",
        "# x = torch.randn(shape)\n",
        "\n",
        "# print(x.size())\n",
        "# print(x.shape)\n",
        "\n",
        "# print(x.size(0))\n",
        "\n",
        "# print(x.view(x.size(0), -1).size())\n",
        "# x=x.view(x.size(0), -1)\n",
        "# print(\"x new.      \",x.size())\n",
        "\n",
        "# print(\"len(x)----------\",len(x))\n",
        "\n",
        "\n",
        "# # Assuming x is your tensor of size [250, 1024]\n",
        "# new_shape = (-1, 32)  # -1 means \"whatever is needed to keep the total number of elements the same\"\n",
        "# reshaped_x = x.view(new_shape)\n",
        "\n",
        "# print(\"x new.      \",x.size())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d3HCr9tn2JIi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALL LEAKY"
      ],
      "metadata": {
        "id": "NQ7EsTgaHefc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class SAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#              # Encoder\n",
        "#         self.enc0=    nn.Conv2d(1, 32, 3, padding=1, stride=2)\n",
        "#         self.enc1=    nn.BatchNorm2d(32)\n",
        "#         self.enc2=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.enc3=    nn.Conv2d(32, 64, 3, padding=1, stride=2)\n",
        "#         self.enc4=    nn.BatchNorm2d(64)\n",
        "#         self.enc5=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.enc6=    nn.Conv2d(64, 128, 3, padding=1, stride=2)\n",
        "#         self.enc7=    nn.BatchNorm2d(128)\n",
        "#         self.enc8=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.enc9=    nn.Flatten(start_dim=1, end_dim=3)\n",
        "#         self.enc10=   nn.Linear(128 * 4 * 4, latent_dim)\n",
        "#         self.enc11=   snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "\n",
        "#         self.latent_dim = latent_dim\n",
        "\n",
        "#         self.linearNet0= nn.Linear(latent_dim,128*4*4)\n",
        "#         self.linearNet1=snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "\n",
        "\n",
        "#         # Decoder:\n",
        "#         self.dec0=    nn.Unflatten(1,(128,4,4)) #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "#         self.dec1=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "#         self.dec2=    nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec3=    nn.BatchNorm2d(64)\n",
        "#         self.dec4=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.dec5=    nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec6=    nn.BatchNorm2d(32)\n",
        "#         self.dec7=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "#         self.dec8=    nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec9=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#        # self.dec9=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=thresh) #---------------------------------------------- ADDED\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         utils.reset(self.enc0)\n",
        "#         utils.reset(self.enc1)\n",
        "#         utils.reset(self.enc2)\n",
        "#         utils.reset(self.enc3)\n",
        "#         utils.reset(self.enc4)\n",
        "#         utils.reset(self.enc5)\n",
        "#         utils.reset(self.enc6)\n",
        "#         utils.reset(self.enc7)\n",
        "#         utils.reset(self.enc8)\n",
        "#         utils.reset(self.enc9)\n",
        "#         utils.reset(self.enc10)\n",
        "#         utils.reset(self.enc11)\n",
        "#         utils.reset(self.linearNet0)\n",
        "#         utils.reset(self.linearNet1)\n",
        "#         utils.reset(self.dec0)\n",
        "#         utils.reset(self.dec1)\n",
        "#         utils.reset(self.dec2)\n",
        "#         utils.reset(self.dec3)\n",
        "#         utils.reset(self.dec4)\n",
        "#         utils.reset(self.dec5)\n",
        "#         utils.reset(self.dec6)\n",
        "#         utils.reset(self.dec7)\n",
        "#         utils.reset(self.dec8)\n",
        "#         utils.reset(self.dec9)\n",
        "\n",
        "#         # # -------Initialize hidden states at t=0\n",
        "\n",
        "#         x2_rec = []\n",
        "#         x5_rec = []\n",
        "#         x8_rec = []\n",
        "#         x11_rec = []\n",
        "\n",
        "#         d1_rec = []\n",
        "#         d4_rec = []\n",
        "#         d7_rec = []\n",
        "#         d9_rec = []\n",
        "#         d9_rec_mem = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             #------------------------------ encoder:\n",
        "#             x0=self.enc0(x)\n",
        "#             x1=self.enc1(x0)\n",
        "#             x2=self.enc2(x1) #Leaky\n",
        "#             # print(\"len x2: \", len(x2))\n",
        "#             # print(\"x2.size: \", x2.size())\n",
        "#             x3=self.enc3(x2)\n",
        "#             x4=self.enc4(x3)\n",
        "#             x5=self.enc5(x4) #Leaky\n",
        "#             x6=self.enc6(x5)\n",
        "#             x7=self.enc7(x6)\n",
        "#             x8=self.enc8(x7) #Leaky\n",
        "#             x9=self.enc9(x8)\n",
        "#             x10=self.enc10(x9)\n",
        "#             x11=self.enc11(x10) #Leaky\n",
        "#             # print(\"len x11: \", len(x11))\n",
        "#             # print(\"x11[0].size: \", x11[0].size())         torch.Size([250, 32])\n",
        "#             # print(\"x11[1].size: \", x11[1].size())         torch.Size([250, 32])\n",
        "\n",
        "\n",
        "#             x2_rec.append(x2)              #    x2 size:  torch.Size([250, 32, 16, 16])\n",
        "#             x5_rec.append(x5)                 #x5 size:  torch.Size([250, 64, 8, 8])\n",
        "#             x8_rec.append(x8)              #  x8 size:  torch.Size([250, 128, 4, 4])\n",
        "#             x11_rec.append(x11[0])\n",
        "#             # print(\"x11 length: \",len(x11))                        x11 length:  2\n",
        "#             # print(\"x11[1] length: \",len(x11[1]))               #   x11[1] length:  250\n",
        "#             # print(\"x11[0] length: \",len(x11[0]))               #   x11[0] length:  250\n",
        "#             # print(\"x11_rec length: \",len(x11_rec))                x11_rec length:  1 >> 2 >> 3 >> 4 >> 5\n",
        "\n",
        "#             #-------------------------------------------------\n",
        "#         x2_rec = torch.stack(x2_rec, dim=2)\n",
        "#         x5_rec = torch.stack(x5_rec, dim=2)\n",
        "#         x8_rec = torch.stack(x8_rec, dim=2)\n",
        "#         x11_rec = torch.stack(x11_rec, dim=2)                         #       torch.Size([250, 32, 5])\n",
        "\n",
        "#         x2_rec = x2_rec[:, :, -1]                                               # =========  x2_rec[:, :, -1].size() torch.Size([250, 32, 16, 16])\n",
        "#         x5_rec = x5_rec[:, :, -1]                                               # =========  x5_rec[:, :, -1].size() torch.Size([250, 64, 8, 8])\n",
        "#         # print(\"=========  x5_rec[:, :, -1].size()\", x5_rec.size())\n",
        "#         x8_rec = x8_rec[:, :, -1]                                               #  ========= x8_rec[:, :, -1].size() torch.Size([250, 128, 4, 4])\n",
        "#         # print(\"=========  x8_rec[:, :, -1].size()\", x8_rec.size())\n",
        "#         out_en = x11_rec[...,step]\n",
        "#         # print(\"========= out_en: x11_rec[...,step] size: \", out_en.size())\n",
        "\n",
        "#         # x11_rec = x11_rec[:, :, -1]             #  =========  x11_rec[:, :, -1].size() torch.Size([250, 32])\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             #------------------------------ latent:\n",
        "#             # LN0=self.linearNet0(x11[...,step])         # means keep all dimensions of x11 up to the last one (which is usually the time dimension), and then selecting the data at the step position along that dimension.\n",
        "#             LN0=self.linearNet0(x11_rec[...,step])\n",
        "#             # print(\" LN0=self.linearNet0(x11_rec[...,step])======\", LN0.size())       # torch.Size([250, 2048])\n",
        "#             # print(\"type of LN0\",type(LN0))\n",
        "#             # print(\"length of LN0\",len(LN0))\n",
        "#             LN1=self.linearNet1(LN0)\n",
        "#             # print(\"LN1=self.linearNet1(LN0) size:\", len(LN1))\n",
        "#             # print(\"LN1[0].size()\", LN1[0].size())          #  LN1[0].size() torch.Size([250, 2048])\n",
        "#             # print(\"LN1[1].size()\", LN1[1].size())          #  LN1[1].size() torch.Size([250, 2048])\n",
        "\n",
        "#             # print(\"type of LN1\",type(LN1))\n",
        "#             # print(\"---------------length of LN1\",len(LN1))                      # length of LN1: 2\n",
        "#             # print(\"length of LN1\",len(LN1[0]))                                  # length of LN1[0]: 250\n",
        "#             # print(\"length of LN1\",len(LN1[1]))                                  # length of LN1[1]: 250\n",
        "#             #------------------------------ decoder:\n",
        "#             # d0=self.dec0(LN1[...,step])\n",
        "#             d0=self.dec0(LN1[0])\n",
        "#             # print(\"type of d0\",type(d0))\n",
        "#             # print(\"length of d0\",len(d0))\n",
        "#             # print(\"Size of d0\", LN1[0].size())              # Size of d0 torch.Size([250, 128, 4, 4])\n",
        "#             # print(\"Size of d0\", LN1[1].size())              # Size of d0 torch.Size([250, 128, 4, 4])\n",
        "\n",
        "#             d1=self.dec1(d0)     #Leaky\n",
        "#             d2=self.dec2(d1)\n",
        "#             d3=self.dec3(d2)\n",
        "#             d4=self.dec4(d3)     #Leaky\n",
        "#             d5=self.dec5(d4)\n",
        "#             d6=self.dec6(d5)\n",
        "#             d7=self.dec7(d6)     #Leaky\n",
        "#             d8=self.dec8(d7)\n",
        "#             d9=self.dec9(d8)     #Leaky\n",
        "#             # d9,d9_mem =self.dec9(d8)     #Leaky\n",
        "#             # print(\"            d9[0]=self.dec9(d8)\", d9[0].size())  #   torch.Size([250, 1, 32, 32])\n",
        "#             # print(\"            d9[1]=self.dec9(d8)\", d9[1].size())  #   torch.Size([250, 1, 32, 32])\n",
        "\n",
        "#             # x_recon=self.dec9(d8)\n",
        "#             #----------------------------------------\n",
        "\n",
        "#             #-----------decoder:\n",
        "#             d1_rec.append(d1)\n",
        "#             # print(\"length of d1_rec.append(d1)\",len(d1_rec))\n",
        "#             d4_rec.append(d4)\n",
        "#             # print(\"length of d4_rec.append(d4)\",len(d4_rec))\n",
        "#             d7_rec.append(d7)\n",
        "#             # print(\"length of d7_rec.append(d7)\",len(d7_rec))\n",
        "#             d9_rec.append(d9[0])\n",
        "#             # print(\"-----------d9 length: \", len(d9))                                   # length of d9: 2\n",
        "#             # print(\"d9[0] length: \", len(d9[0]))                                   # length of d9[0]: 250\n",
        "#             # print(\"d9[1] length: \", len(d9[1]))                                   # length of d9[1]: 250\n",
        "#             # print(\"length of d9_rec.append(d9)\",len(d9_rec))\n",
        "#             d9_rec_mem.append(d9[1])\n",
        "\n",
        "\n",
        "#        # decoder\n",
        "#         d1_rec = torch.stack(d1_rec, dim=4)                                              # torch.Size([250, 128, 4, 4, 5])\n",
        "#         # print(\"-----------------length of torch.stack(d1_rec, dim=4)\",len(d1_rec))\n",
        "#         # print(\"------size of torch.stack(d1_rec, dim=4)\",d1_rec.size())\n",
        "#         d4_rec = torch.stack(d4_rec, dim=4)                                              #torch.Size([250, 64, 8, 8, 5])\n",
        "#         # print(\"-----------------length of torch.stack(d4_rec, dim=4)\",len(d4_rec))\n",
        "#         # print(\"------size of torch.stack(d4_rec, dim=4)\",d4_rec.size())\n",
        "\n",
        "#         d7_rec = torch.stack(d7_rec, dim=4)                                              #torch.Size([250, 32, 16, 16, 5])\n",
        "#         # print(\"-----------------length of torch.stack(d7_rec, dim=4)\",len(d7_rec))\n",
        "#         # print(\"------size of torch.stack(d7_rec, dim=4)\",d7_rec.size())\n",
        "\n",
        "#         d9_rec = torch.stack(d9_rec, dim=4)\n",
        "#         # print(\"-----------------length of torch.stack(d9_rec, dim=4)\",len(d9_rec))\n",
        "#         # print(\"------size of torch.stack(d9_rec, dim=4)\",d9_rec.size())                  #size of torch.stack(d9_rec, dim=4) torch.Size([250, 1, 32, 32, 5])\n",
        "#         d9_rec_mem = torch.stack(d9_rec_mem, dim=4)\n",
        "\n",
        "#        # decoder\n",
        "#         d1_rec = d1_rec[:, :, :, :, -1]\n",
        "#         # print(\"=========  d1_rec[:, :, :, :, -1].size()\", d1_rec.size())              #   d1_rec[:, :, :, :, -1].size() torch.Size([250, 128, 4, 4])\n",
        "#         d4_rec = d4_rec[:, :, :, :, -1]\n",
        "#         # print(\"=========  d4_rec[:, :, :, :, -1].size()\", d4_rec.size())              #   d4_rec[:, :, :, :, -1].size() torch.Size([250, 64, 8, 8])\n",
        "#         d7_rec = d7_rec[:, :, :, :, -1]\n",
        "#         # print(\"=========  d7_rec[:, :, :, :, -1].size()\", d7_rec.size())              #   d7_rec[:, :, :, :, -1].size() torch.Size([250, 32, 16, 16])\n",
        "#         d9_rec = d9_rec[:, :, :, :, -1]\n",
        "#         # print(\"=========  d9_rec[:, :, :, :, -1].size()\", d9_rec.size())              #   d9_rec[:, :, :, :, -1].size() torch.Size([250, 1, 32, 32])\n",
        "#         out = d9_rec_mem[:, :, :, :, -1]\n",
        "\n",
        "#         # self.out_en = out_en\n",
        "#         # self.out = out\n",
        "\n",
        "#         return d9_rec, out_en\n",
        "\n",
        "#     def get_activation(self, name):\n",
        "#         def hook(module, input, output):\n",
        "#             setattr(self, name, output)  # Store the output as an attribute of the model\n",
        "#         return hook\n",
        "\n",
        "\n",
        "# # activation = {}\n",
        "# #     def get_activation(self, name):\n",
        "# #         def hook(model, input, output):\n",
        "# #             activation[name] = output.detach()\n",
        "# #         return hook\n"
      ],
      "metadata": {
        "id": "AIVR3KS_XFcu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SAE(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#              # Encoder\n",
        "#         self.enc0=    nn.Conv2d(1, 32, 3, padding=1, stride=2)\n",
        "#         self.enc1=    nn.BatchNorm2d(32)\n",
        "#         self.enc2=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, threshold=thresh)\n",
        "#         self.enc3=    nn.Conv2d(32, 64, 3, padding=1, stride=2)\n",
        "#         self.enc4=    nn.BatchNorm2d(64)\n",
        "#         self.enc5=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True , threshold=thresh)\n",
        "#         self.enc6=    nn.Conv2d(64, 128, 3, padding=1, stride=2)\n",
        "#         self.enc7=    nn.BatchNorm2d(128)\n",
        "#         self.enc8=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, threshold=thresh)\n",
        "#         self.enc9=    nn.Flatten(start_dim=1, end_dim=3)\n",
        "#         self.enc10=   nn.Linear(128 * 4 * 4, latent_dim)\n",
        "#         self.enc11=   snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)\n",
        "\n",
        "#         self.latent_dim = latent_dim\n",
        "\n",
        "#         self.linearNet0= nn.Linear(latent_dim,128*4*4)\n",
        "#         self.linearNet1=snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "\n",
        "\n",
        "#         # Decoder:\n",
        "#         self.dec0=    nn.Unflatten(1,(128,4,4)) #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "#         self.dec1=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh)\n",
        "#         self.dec2=    nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec3=    nn.BatchNorm2d(64)\n",
        "#         self.dec4=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, threshold=thresh)\n",
        "#         self.dec5=    nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec6=    nn.BatchNorm2d(32)\n",
        "#         self.dec7=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, threshold=thresh)\n",
        "#         self.dec8=    nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1)\n",
        "#         self.dec9=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "#        # self.dec9=    snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=thresh) #---------------------------------------------- ADDED\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         utils.reset(self.enc0)\n",
        "#         utils.reset(self.enc1)\n",
        "#         utils.reset(self.enc2)\n",
        "#         utils.reset(self.enc3)\n",
        "#         utils.reset(self.enc4)\n",
        "#         utils.reset(self.enc5)\n",
        "#         utils.reset(self.enc6)\n",
        "#         utils.reset(self.enc7)\n",
        "#         utils.reset(self.enc8)\n",
        "#         utils.reset(self.enc9)\n",
        "#         utils.reset(self.enc10)\n",
        "#         utils.reset(self.enc11)\n",
        "#         utils.reset(self.linearNet0)\n",
        "#         utils.reset(self.linearNet1)\n",
        "#         utils.reset(self.dec0)\n",
        "#         utils.reset(self.dec1)\n",
        "#         utils.reset(self.dec2)\n",
        "#         utils.reset(self.dec3)\n",
        "#         utils.reset(self.dec4)\n",
        "#         utils.reset(self.dec5)\n",
        "#         utils.reset(self.dec6)\n",
        "#         utils.reset(self.dec7)\n",
        "#         utils.reset(self.dec8)\n",
        "#         utils.reset(self.dec9)\n",
        "\n",
        "\n",
        "#         x2_rec = []\n",
        "#         x5_rec = []\n",
        "#         x8_rec = []\n",
        "#         x11_rec = []\n",
        "\n",
        "#         d1_rec = []\n",
        "#         d4_rec = []\n",
        "#         d7_rec = []\n",
        "#         d9_rec = []\n",
        "#         d9_rec_mem = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             #------------------------------ encoder:\n",
        "#             x0=self.enc0(x)\n",
        "#             x1=self.enc1(x0)\n",
        "#             x2=self.enc2(x1) #Leaky\n",
        "#             x3=self.enc3(x2)\n",
        "#             x4=self.enc4(x3)\n",
        "#             x5, x5mem =self.enc5(x4) #Leaky\n",
        "#             x6=self.enc6(x5)\n",
        "#             x7=self.enc7(x6)\n",
        "#             x8=self.enc8(x7) #Leaky\n",
        "#             x9=self.enc9(x8)\n",
        "#             x10=self.enc10(x9)\n",
        "#             x11=self.enc11(x10) #Leaky\n",
        "\n",
        "#             x2_rec.append(x2)\n",
        "#             x5_rec.append(x5)\n",
        "#             x8_rec.append(x8)\n",
        "#             x11_rec.append(x11[0])\n",
        "\n",
        "#             #-------------------------------------------------\n",
        "#         x2_rec = torch.stack(x2_rec, dim=2)\n",
        "#         x5_rec = torch.stack(x5_rec, dim=2)\n",
        "#         x8_rec = torch.stack(x8_rec, dim=2)\n",
        "#         x11_rec = torch.stack(x11_rec, dim=2)                         #\n",
        "#         x2_rec = x2_rec[:, :, -1]\n",
        "#         x5_rec = x5_rec[:, :, -1]\n",
        "#         x8_rec = x8_rec[:, :, -1]\n",
        "#         out_en = x11_rec[...,step]\n",
        "#         # x11_rec = x11_rec[:, :, -1]             #  =========  x11_rec[:, :, -1].size() torch.Size([250, 32])\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             #------------------------------ latent:\n",
        "#             LN0=self.linearNet0(x11_rec[...,step])\n",
        "#             LN1=self.linearNet1(LN0)\n",
        "#             #------------------------------ decoder:\n",
        "#             # d0=self.dec0(LN1[...,step])\n",
        "#             d0=self.dec0(LN1[0])\n",
        "#             d1=self.dec1(d0)     #Leaky\n",
        "#             d2=self.dec2(d1)\n",
        "#             d3=self.dec3(d2)\n",
        "#             d4=self.dec4(d3)     #Leaky\n",
        "#             d5=self.dec5(d4)\n",
        "#             d6=self.dec6(d5)\n",
        "#             d7=self.dec7(d6)     #Leaky\n",
        "#             d8=self.dec8(d7)\n",
        "#             d9=self.dec9(d8)     #Leaky\n",
        "#             #----------------------------------------\n",
        "\n",
        "#             #-----------decoder:\n",
        "#             d1_rec.append(d1)\n",
        "#             d4_rec.append(d4)\n",
        "#             d7_rec.append(d7)\n",
        "#             d9_rec.append(d9[0])\n",
        "#             d9_rec_mem.append(d9[1])\n",
        "\n",
        "\n",
        "#        # decoder\n",
        "#         d1_rec = torch.stack(d1_rec, dim=4)\n",
        "#         d4_rec = torch.stack(d4_rec, dim=4)\n",
        "#         d7_rec = torch.stack(d7_rec, dim=4)\n",
        "#         d9_rec = torch.stack(d9_rec, dim=4)\n",
        "#         d9_rec_mem = torch.stack(d9_rec_mem, dim=4)\n",
        "\n",
        "#        # decoder\n",
        "#         d1_rec = d1_rec[:, :, :, :, -1]\n",
        "#         d4_rec = d4_rec[:, :, :, :, -1]\n",
        "#         d7_rec = d7_rec[:, :, :, :, -1]\n",
        "#         d9_rec = d9_rec[:, :, :, :, -1]\n",
        "#         out = d9_rec_mem[:, :, :, :, -1]\n",
        "\n",
        "#         # self.out_en = out_en\n",
        "#         # self.out = out\n",
        "\n",
        "#         return d9_rec, out_en\n",
        "\n",
        "#     # def get_activation(self, name):\n",
        "#     #     def hook(module, input, output):\n",
        "#     #         setattr(self, name, output)  # Store the output as an attribute of the model\n",
        "#     #     return hook"
      ],
      "metadata": {
        "id": "_C76N8tQJmcT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## for each layer"
      ],
      "metadata": {
        "id": "9q4mUUpWdF_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training and Testing\n",
        "# from torchvision.utils import save_image\n",
        "\n",
        "# spike_recordings = []\n",
        "# train_ber_rec = []\n",
        "# test_ber_rec = []\n",
        "# threshold_Real = 0.5\n",
        "# threshold_Recon = 0.5\n",
        "\n",
        "# def train(network, trainloader, opti, epoch):\n",
        "#     network=network.train()\n",
        "#     train_loss_hist=torch.zeros((1), dtype=dtype, device=device)\n",
        "#     train_avg_loss_rec=[]\n",
        "\n",
        "#     for batch_idx, (real_img, labels) in enumerate(trainloader):\n",
        "#         opti.zero_grad()\n",
        "#         real_img = real_img.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         # print(\"real_img size\", real_img.size())      #    real_img size                # -------------------------------------------------ADDED\n",
        "#         out, out_en = network(real_img)\n",
        "#         x_recon, out_en = network(real_img)\n",
        "#         # print(\"out_en size\", out_en.size())      #                  # -------------------------------------------------ADDED\n",
        "#         # print(\"x_recon size\", x_recon.size())      #                  # -------------------------------------------------ADDED\n",
        "\n",
        "\n",
        "#         #Calculate loss\n",
        "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "#         for step in range(num_steps):\n",
        "#           loss_val += F.mse_loss(x_recon, real_img)                  #.view(1, -1)\n",
        "\n",
        "#         train_loss_hist += (loss_val.item())/num_steps\n",
        "#         avg_loss=train_loss_hist.mean()\n",
        "\n",
        "#         # # # ---------------------\n",
        "#         print(f'Train[{epoch}/{max_epoch}][{batch_idx}/{len(trainloader)}] Loss: {loss_val.item()}')\n",
        "\n",
        "#         loss_val.backward()        #\n",
        "\n",
        "#         opti.step()\n",
        "#         train_loss_rec.append(loss_val.item())\n",
        "\n",
        "#         #Save reconstructed images every at the end of the epoch\n",
        "#         if batch_idx == len(trainloader)-1:\n",
        "#             utls.save_image((real_img+1)/2, f'figures/training/epoch{epoch}_finalbatch_inputs.png')\n",
        "#             utls.save_image((x_recon+1)/2, f'figures/training/epoch{epoch}_finalbatch_recon.png')\n",
        "#             train_auc = auc(np.arange(len(train_loss_rec)), train_loss_rec)\n",
        "\n",
        "#     # return loss_val, train_loss_rec, train_auc, d9_rec, out_en  #              # -------------------------------------------------ADDED\n",
        "#     return loss_val, train_loss_rec, train_auc, out, out_en  #              # -------------------------------------------------ADDED\n",
        "\n",
        "\n",
        "# # For Testing, not doing backpropagate, therefore no gradients are required and we use torch.no_grad():\n",
        "# #Testing Loop\n",
        "# def test(network, testloader, opti, epoch):\n",
        "#     network=network.eval()\n",
        "#     test_loss_hist=[]\n",
        "#     test_avg_loss_rec=[]\n",
        "#     test_avg_loss_hist = []\n",
        "\n",
        "#     spk_rec_test = []\n",
        "#     with torch.no_grad(): #no gradient this time\n",
        "#         for batch_idx, (real_img, labels) in enumerate(testloader):\n",
        "#             real_img = real_img.to(device)#\n",
        "#             labels = labels.to(device)\n",
        "#             # x11_rec, d9_rec = network(real_img)             # -------------------------------------------------ADDED\n",
        "#             out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#             x_recon, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#             # average Loss:\n",
        "#             loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "#             for step in range(num_steps):\n",
        "#               loss_val += F.mse_loss(x_recon, real_img)\n",
        "#             avg_loss=loss_val/num_steps\n",
        "#             test_loss_hist.append(loss_val.item())\n",
        "\n",
        "#             real_img_binary = (real_img > threshold_Real).float()\n",
        "#             x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "#             bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "#             total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "#             bit_error_rate = bit_errors.item() / total_pixels\n",
        "#             test_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "#             # Save binary images\n",
        "#             Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "#             if batch_idx == len(testloader)-1:\n",
        "#               if epoch in [0, 25, 49]:\n",
        "#                 save_image(real_img_binary, f'figures/binarytesting/ep{epoch}_inputs_binary.png')\n",
        "#                 save_image(x_recon_binary, f'figures/binarytesting/ep{epoch}_recon_binary.png')\n",
        "#                 save_image(Error_bin, f'figures/binarytesting/ep{epoch}_Error_bin.png')\n",
        "#             # -------------------------------------------------------------------------------------------------\n",
        "\n",
        "#             print(f'Test[{epoch}/{max_epoch}][{batch_idx}/{len(testloader)}]  Loss: {loss_val.item()}, '  f'BER (test): {bit_error_rate}')\n",
        "\n",
        "#             test_loss_rec.append(loss_val.item())\n",
        "\n",
        "#             if batch_idx == len(testloader)-1:\n",
        "#                 utls.save_image((real_img+1)/2, f'figures/testing/epoch{epoch}_finalbatch_inputs.png')\n",
        "#                 utls.save_image((x_recon+1)/2, f'figures/testing/epoch{epoch}_finalbatch_recons.png')\n",
        "#                 test_auc = auc(np.arange(len(test_loss_rec)), test_loss_rec)\n",
        "\n",
        "#     # return loss_val, test_loss_rec, test_auc, out, out_en                     # -------------------------------------------------ADDED\n",
        "#     # return loss_val, test_loss_rec, test_auc, x11_rec, d9_rec  #              # -------------------------------------------------ADDED\n",
        "#     return loss_val, test_loss_rec, test_auc, out, out_en\n",
        "\n"
      ],
      "metadata": {
        "id": "PGWPHsrmPW9v"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training and Testing\n",
        "# # using MSE loss to compare the reconstructed image (x_recon) with the original image (real_img)\n",
        "# from torchvision.utils import save_image\n",
        "\n",
        "# spike_recordings = []\n",
        "# train_ber_rec = []\n",
        "# test_ber_rec = []\n",
        "# threshold_Real = 0.5\n",
        "# threshold_Recon = 0.5\n",
        "\n",
        "# def train(network, trainloader, opti, epoch):\n",
        "#     network=network.train()\n",
        "#     train_loss_hist=torch.zeros((1), dtype=dtype, device=device)\n",
        "#     train_avg_loss_rec=[]\n",
        "\n",
        "#     for batch_idx, (real_img, labels) in enumerate(trainloader):\n",
        "#         opti.zero_grad()\n",
        "#         real_img = real_img.to(device)\n",
        "#         labels = labels.to(device)\n",
        "#         # d9_rec, out_en = network(real_img)             # -------------------------------------------------ADDED\n",
        "#         # print(\"real_img size\", real_img.size())      #    real_img size torch.Size([250, 1, 32, 32])                # -------------------------------------------------ADDED\n",
        "\n",
        "#         # out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#         x_recon, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#         # print(\"x_recon size\", x_recon.size())      #       x_recon size torch.Size([250, 1, 32, 32])\n",
        "#         # print(\"out size\", out.size())             #       out size torch.Size([250, 32])\n",
        "\n",
        "#         #Calculate loss\n",
        "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "#         for step in range(num_steps):\n",
        "#           loss_val += F.mse_loss(x_recon, real_img)                  #.view(1, -1)\n",
        "\n",
        "#             # Clone the loss_val tensor to avoid in-place modification\n",
        "\n",
        "#         train_loss_hist += (loss_val.item())/num_steps\n",
        "#         avg_loss=train_loss_hist.mean()\n",
        "\n",
        "#         # # ---------------------------- Calculate Bit Error Rate (BER)\n",
        "#         real_img_binary = (real_img > threshold_Real).float()\n",
        "#         x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "#         bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "#         total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "#         bit_error_rate = bit_errors.item() / total_pixels\n",
        "#         train_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "#         # Save binary images\n",
        "#         Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "#         if batch_idx == len(trainloader)-1:\n",
        "#           if epoch in [0, 25, 49]:\n",
        "#             utls.save_image(real_img_binary, f'figures/binarytraining/ep{epoch}_inputs_binary.png')\n",
        "#             utls.save_image(x_recon_binary, f'figures/binarytraining/ep{epoch}_recon_binary.png')\n",
        "#             utls.save_image(Error_bin, f'figures/binarytraining/ep{epoch}_Error_bin.png')\n",
        "#         print(f'Train[{epoch}/{max_epoch}][{batch_idx}/{len(trainloader)}] Loss: {loss_val.item()}, ' f'BER : {bit_error_rate}')\n",
        "\n",
        "#         # loss_val += torch.mean(loss_val)  # Accumulate the loss             # -------------------------------------------------ADDED\n",
        "#         loss_val.backward()         #\n",
        "#         # loss_val.backward(retain_graph=True)        #\n",
        "\n",
        "#         opti.step()\n",
        "#         train_loss_rec.append(loss_val.item())\n",
        "\n",
        "#         #Save reconstructed images every at the end of the epoch\n",
        "#         if batch_idx == len(trainloader)-1:\n",
        "#             utls.save_image((real_img+1)/2, f'figures/training/epoch{epoch}_finalbatch_inputs.png')\n",
        "#             utls.save_image((x_recon+1)/2, f'figures/training/epoch{epoch}_finalbatch_recon.png')\n",
        "#             train_auc = auc(np.arange(len(train_loss_rec)), train_loss_rec)\n",
        "\n",
        "#     # loss_val.backward()                                            # -------------------------------------------------ADDED\n",
        "\n",
        "#     # return loss_val, train_loss_rec, train_auc , out, out_en  #, spk_rec_batches#, train_avg_loss_rec, #avg_loss #, train_loss_hist\n",
        "#     # return loss_val, train_loss_rec, train_auc   #\n",
        "#     return loss_val, train_loss_rec, train_auc, d9_rec, out_en  #              # -------------------------------------------------ADDED\n",
        "\n",
        "\n",
        "# # For Testing, not doing backpropagate, therefore no gradients are required and we use torch.no_grad():\n",
        "# #Testing Loop\n",
        "# def test(network, testloader, opti, epoch):\n",
        "#     network=network.eval()\n",
        "#     test_loss_hist=[]\n",
        "#     test_avg_loss_rec=[]\n",
        "#     test_avg_loss_hist = []\n",
        "\n",
        "#     spk_rec_test = []\n",
        "#     with torch.no_grad(): #no gradient this time\n",
        "#         for batch_idx, (real_img, labels) in enumerate(testloader):\n",
        "#             real_img = real_img.to(device)#\n",
        "#             labels = labels.to(device)\n",
        "#             # x11_rec, d9_rec = network(real_img)             # -------------------------------------------------ADDED\n",
        "#             # out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#             x_recon, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "#             # average Loss:\n",
        "#             loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "#             for step in range(num_steps):\n",
        "#               loss_val += F.mse_loss(x_recon, real_img)\n",
        "#             avg_loss=loss_val/num_steps\n",
        "#             test_loss_hist.append(loss_val.item())\n",
        "\n",
        "#             real_img_binary = (real_img > threshold_Real).float()\n",
        "#             x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "#             bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "#             total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "#             bit_error_rate = bit_errors.item() / total_pixels\n",
        "#             test_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "#             # Save binary images\n",
        "#             Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "#             if batch_idx == len(testloader)-1:\n",
        "#               if epoch in [0, 25, 49]:\n",
        "#                 save_image(real_img_binary, f'figures/binarytesting/ep{epoch}_inputs_binary.png')\n",
        "#                 save_image(x_recon_binary, f'figures/binarytesting/ep{epoch}_recon_binary.png')\n",
        "#                 save_image(Error_bin, f'figures/binarytesting/ep{epoch}_Error_bin.png')\n",
        "#             # -------------------------------------------------------------------------------------------------\n",
        "\n",
        "#             print(f'Test[{epoch}/{max_epoch}][{batch_idx}/{len(testloader)}]  Loss: {loss_val.item()}, '  f'BER (test): {bit_error_rate}')\n",
        "\n",
        "#             test_loss_rec.append(loss_val.item())\n",
        "\n",
        "#             if batch_idx == len(testloader)-1:\n",
        "#                 utls.save_image((real_img+1)/2, f'figures/testing/epoch{epoch}_finalbatch_inputs.png')\n",
        "#                 utls.save_image((x_recon+1)/2, f'figures/testing/epoch{epoch}_finalbatch_recons.png')\n",
        "#                 test_auc = auc(np.arange(len(test_loss_rec)), test_loss_rec)\n",
        "\n",
        "#     # return loss_val, test_loss_rec, test_auc, out, out_en                     # -------------------------------------------------ADDED\n",
        "#     # return loss_val, test_loss_rec, test_auc, x11_rec, d9_rec  #              # -------------------------------------------------ADDED\n",
        "#     return loss_val, test_loss_rec, test_auc  #\n",
        "\n"
      ],
      "metadata": {
        "id": "HmvBPb95dEju"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5HbcSNNp6vK2"
      },
      "outputs": [],
      "source": [
        "# Training and Testing\n",
        "# using MSE loss to compare the reconstructed image (x_recon) with the original image (real_img)\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "spike_recordings = []\n",
        "train_ber_rec = []\n",
        "test_ber_rec = []\n",
        "threshold_Real = 0.5\n",
        "threshold_Recon = 0.5\n",
        "\n",
        "def train(network, trainloader, opti, epoch):\n",
        "    network=network.train()\n",
        "    train_loss_hist=torch.zeros((1), dtype=dtype, device=device)\n",
        "    train_avg_loss_rec=[]\n",
        "\n",
        "    for batch_idx, (real_img, labels) in enumerate(trainloader):\n",
        "        opti.zero_grad()\n",
        "        real_img = real_img.to(device)\n",
        "        labels = labels.to(device)\n",
        "        out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "        x_recon, out = network(real_img)  # Pass data into network and return reconstructed image and spk_rec.  #        x_recon size torch.Size([250, 1, 32, 32]) ,  #        out size torch.Size([250, 32])\n",
        "        #Calculate loss\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "        for step in range(num_steps):\n",
        "          loss_val += F.mse_loss(x_recon, real_img)                  #.view(1, -1)\n",
        "        train_loss_hist += (loss_val.item())/num_steps\n",
        "        avg_loss=train_loss_hist.mean()\n",
        "\n",
        "        # # ---------------------------- Calculate Bit Error Rate (BER)\n",
        "        real_img_binary = (real_img > threshold_Real).float()\n",
        "        x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "        bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "        total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "        bit_error_rate = bit_errors.item() / total_pixels\n",
        "        train_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "        # Save binary images\n",
        "        Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "        if batch_idx == len(trainloader)-1:\n",
        "          if epoch in [0, 25, 49]:\n",
        "            utls.save_image(real_img_binary, f'figures/binarytraining/ep{epoch}_inputs_binary.png')\n",
        "            utls.save_image(x_recon_binary, f'figures/binarytraining/ep{epoch}_recon_binary.png')\n",
        "            utls.save_image(Error_bin, f'figures/binarytraining/ep{epoch}_Error_bin.png')\n",
        "        print(f'Train[{epoch}/{max_epoch}][{batch_idx}/{len(trainloader)}] Loss: {loss_val.item()}, ' f'BER : {bit_error_rate}')\n",
        "\n",
        "        loss_val.backward()\n",
        "        opti.step()\n",
        "        train_loss_rec.append(loss_val.item())\n",
        "\n",
        "        #Save reconstructed images every at the end of the epoch\n",
        "        if batch_idx == len(trainloader)-1:\n",
        "            utls.save_image((real_img+1)/2, f'figures/training/epoch{epoch}_finalbatch_inputs.png')\n",
        "            utls.save_image((x_recon+1)/2, f'figures/training/epoch{epoch}_finalbatch_recon.png')\n",
        "            train_auc = auc(np.arange(len(train_loss_rec)), train_loss_rec)\n",
        "\n",
        "    return loss_val, train_loss_rec, train_auc , out, out_en  #, spk_rec_batches#, train_avg_loss_rec, #avg_loss #, train_loss_hist\n",
        "    # return loss_val, train_loss_rec, train_auc   #\n",
        "\n",
        "\n",
        "# For Testing, not doing backpropagate, therefore no gradients are required and we use torch.no_grad():\n",
        "#Testing Loop\n",
        "def test(network, testloader, opti, epoch):\n",
        "    network=network.eval()\n",
        "    test_loss_hist=[]\n",
        "    test_avg_loss_rec=[]\n",
        "    test_avg_loss_hist = []\n",
        "\n",
        "    spk_rec_test = []\n",
        "    with torch.no_grad(): #no gradient this time\n",
        "        for batch_idx, (real_img, labels) in enumerate(testloader):\n",
        "            real_img = real_img.to(device)#\n",
        "            labels = labels.to(device)\n",
        "            out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "            x_recon , out = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "            # average Loss:\n",
        "            loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "            for step in range(num_steps):\n",
        "              loss_val += F.mse_loss(x_recon, real_img)\n",
        "            avg_loss=loss_val/num_steps\n",
        "            test_loss_hist.append(loss_val.item())\n",
        "\n",
        "            real_img_binary = (real_img > threshold_Real).float()\n",
        "            x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "            bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "            total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "            bit_error_rate = bit_errors.item() / total_pixels\n",
        "            test_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "            # Save binary images\n",
        "            Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "            if batch_idx == len(testloader)-1:\n",
        "              if epoch in [0, 25, 49]:\n",
        "                save_image(real_img_binary, f'figures/binarytesting/ep{epoch}_inputs_binary.png')\n",
        "                save_image(x_recon_binary, f'figures/binarytesting/ep{epoch}_recon_binary.png')\n",
        "                save_image(Error_bin, f'figures/binarytesting/ep{epoch}_Error_bin.png')\n",
        "            # -------------------------------------------------------------------------------------------------\n",
        "\n",
        "            print(f'Test[{epoch}/{max_epoch}][{batch_idx}/{len(testloader)}]  Loss: {loss_val.item()}, '  f'BER (test): {bit_error_rate}')\n",
        "\n",
        "            test_loss_rec.append(loss_val.item())\n",
        "\n",
        "            if batch_idx == len(testloader)-1:\n",
        "                utls.save_image((real_img+1)/2, f'figures/testing/epoch{epoch}_finalbatch_inputs.png')\n",
        "                utls.save_image((x_recon+1)/2, f'figures/testing/epoch{epoch}_finalbatch_recons.png')\n",
        "                test_auc = auc(np.arange(len(test_loss_rec)), test_loss_rec)\n",
        "\n",
        "    return loss_val, test_loss_rec, test_auc, out, out_en                     # -------------------------------------------------ADDED\n",
        "    # return loss_val, test_loss_rec, test_auc  #\n",
        "\n",
        "for batch_spikes in spike_recordings:\n",
        "    print(batch_spikes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lF2cwM20PKAL"
      },
      "outputs": [],
      "source": [
        "input_size = 32 #resize of mnist data (optional)\n",
        "\n",
        "#setup GPU\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# neuron and simulation parameters\n",
        "spike_grad = surrogate.atan(alpha=2.0)  # alternate surrogate gradient fast_sigmoid(slope=25)\n",
        "\n",
        "train_loss_rec = []\n",
        "test_loss_rec = []\n",
        "train_loss_record = []\n",
        "test_loss_record = []\n",
        "train_avg_loss_rec=[]\n",
        "test_avg_loss_rec=[]\n",
        "\n",
        "  # Synaptic current and membrane potential decay exponentially with rates of alpha and beta\n",
        "alpha=0.9\n",
        "# beta_syn=0.0001\n",
        "beta_syn=0.9\n",
        "\n",
        "beta =0.9\n",
        "\n",
        "num_steps=5\n",
        "latent_dim = 32 #dimension of latent layer (how compressed we want the information)\n",
        "thresh=1    #spiking threshold (lower = more spikes are let through)\n",
        "# epochs=50\n",
        "epochs=3\n",
        "max_epoch=epochs\n",
        "\n",
        "  #Define Network and optimizer\n",
        "net=SAE()\n",
        "net = net.to(device)\n",
        "optimizer = torch.optim.AdamW(net.parameters(),\n",
        "                            lr=0.0001,\n",
        "                            betas=(0.9, 0.999),\n",
        "                            weight_decay=0.001)\n",
        "\n",
        "\n",
        "\n",
        "activation = {}\n",
        "# def get_activation(name):\n",
        "#     def hook(model, input, output):\n",
        "#         activation[name] = output.detach()\n",
        "#     return hook\n",
        "\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        if isinstance(output, tuple):\n",
        "            activation[name] = [out.detach() for out in output]\n",
        "        else:\n",
        "            activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "\n",
        "# net.encoder[5].register_forward_hook(get_activation('encoder[5]'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2hQ0wiOxUr"
      },
      "source": [
        "## for saving the out_en after each epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUOjPTrBOjmt",
        "outputId": "e9f1311a-ea0d-4a12-c622-df4c8723174b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train[0/3][0/240] Loss: 0.6051520109176636, BER : 0.1281484375\n",
            "Train[0/3][1/240] Loss: 0.6221926212310791, BER : 0.13150390625\n",
            "Train[0/3][2/240] Loss: 0.6240261197090149, BER : 0.1310859375\n",
            "Train[0/3][3/240] Loss: 0.6404139995574951, BER : 0.13375390625\n",
            "Train[0/3][4/240] Loss: 0.7125463485717773, BER : 0.13743359375\n",
            "Train[0/3][5/240] Loss: 0.8500621318817139, BER : 0.1411875\n",
            "Train[0/3][6/240] Loss: 1.0110933780670166, BER : 0.15303515625\n",
            "Train[0/3][7/240] Loss: 1.021156907081604, BER : 0.150640625\n",
            "Train[0/3][8/240] Loss: 0.997146725654602, BER : 0.14925\n",
            "Train[0/3][9/240] Loss: 0.8551381826400757, BER : 0.1416015625\n",
            "Train[0/3][10/240] Loss: 0.7665982246398926, BER : 0.13877734375\n",
            "Train[0/3][11/240] Loss: 0.7022441029548645, BER : 0.132296875\n",
            "Train[0/3][12/240] Loss: 0.7101943492889404, BER : 0.13644921875\n",
            "Train[0/3][13/240] Loss: 0.7256504893302917, BER : 0.1354296875\n",
            "Train[0/3][14/240] Loss: 0.8732196092605591, BER : 0.14524609375\n",
            "Train[0/3][15/240] Loss: 0.8227789998054504, BER : 0.14209765625\n",
            "Train[0/3][16/240] Loss: 0.7733208537101746, BER : 0.1362578125\n",
            "Train[0/3][17/240] Loss: 0.7862025499343872, BER : 0.144359375\n",
            "Train[0/3][18/240] Loss: 0.8485956192016602, BER : 0.147390625\n",
            "Train[0/3][19/240] Loss: 0.8253930807113647, BER : 0.1429140625\n",
            "Train[0/3][20/240] Loss: 0.9236584901809692, BER : 0.151796875\n",
            "Train[0/3][21/240] Loss: 0.9230853319168091, BER : 0.14973046875\n",
            "Train[0/3][22/240] Loss: 0.8710893988609314, BER : 0.1498828125\n",
            "Train[0/3][23/240] Loss: 0.9076521396636963, BER : 0.14809765625\n",
            "Train[0/3][24/240] Loss: 0.9563690423965454, BER : 0.15311328125\n",
            "Train[0/3][25/240] Loss: 0.8959437608718872, BER : 0.15114453125\n",
            "Train[0/3][26/240] Loss: 0.9483879208564758, BER : 0.15052734375\n",
            "Train[0/3][27/240] Loss: 0.9293634295463562, BER : 0.14788671875\n",
            "Train[0/3][28/240] Loss: 0.9152786731719971, BER : 0.14733203125\n",
            "Train[0/3][29/240] Loss: 0.8994038105010986, BER : 0.1481328125\n",
            "Train[0/3][30/240] Loss: 0.8545111417770386, BER : 0.14659375\n",
            "Train[0/3][31/240] Loss: 0.8261399269104004, BER : 0.142234375\n",
            "Train[0/3][32/240] Loss: 0.8984435796737671, BER : 0.14731640625\n",
            "Train[0/3][33/240] Loss: 0.95444256067276, BER : 0.1549296875\n",
            "Train[0/3][34/240] Loss: 1.0281624794006348, BER : 0.1601953125\n",
            "Train[0/3][35/240] Loss: 1.0900875329971313, BER : 0.16446484375\n",
            "Train[0/3][36/240] Loss: 0.982711672782898, BER : 0.152921875\n",
            "Train[0/3][37/240] Loss: 0.905221164226532, BER : 0.1516015625\n",
            "Train[0/3][38/240] Loss: 0.8848900198936462, BER : 0.15215625\n",
            "Train[0/3][39/240] Loss: 1.0053637027740479, BER : 0.15993359375\n",
            "Train[0/3][40/240] Loss: 0.9845548272132874, BER : 0.15628515625\n",
            "Train[0/3][41/240] Loss: 0.9279965758323669, BER : 0.1494765625\n",
            "Train[0/3][42/240] Loss: 0.9788092970848083, BER : 0.154796875\n",
            "Train[0/3][43/240] Loss: 1.0230929851531982, BER : 0.1626640625\n",
            "Train[0/3][44/240] Loss: 1.0637478828430176, BER : 0.1649375\n",
            "Train[0/3][45/240] Loss: 0.9237703680992126, BER : 0.1535625\n",
            "Train[0/3][46/240] Loss: 0.9118431210517883, BER : 0.15255859375\n",
            "Train[0/3][47/240] Loss: 1.0042439699172974, BER : 0.16555859375\n",
            "Train[0/3][48/240] Loss: 0.93662428855896, BER : 0.15940625\n",
            "Train[0/3][49/240] Loss: 0.9389908313751221, BER : 0.15813671875\n",
            "Train[0/3][50/240] Loss: 0.9149842262268066, BER : 0.15537109375\n",
            "Train[0/3][51/240] Loss: 0.9896566867828369, BER : 0.16184375\n",
            "Train[0/3][52/240] Loss: 0.9417210817337036, BER : 0.15961328125\n",
            "Train[0/3][53/240] Loss: 1.0138365030288696, BER : 0.16691796875\n",
            "Train[0/3][54/240] Loss: 1.012025237083435, BER : 0.16361328125\n",
            "Train[0/3][55/240] Loss: 1.0823372602462769, BER : 0.1713125\n",
            "Train[0/3][56/240] Loss: 1.030984878540039, BER : 0.16888671875\n",
            "Train[0/3][57/240] Loss: 0.9793704748153687, BER : 0.16697265625\n",
            "Train[0/3][58/240] Loss: 0.9467548131942749, BER : 0.16457421875\n",
            "Train[0/3][59/240] Loss: 1.0698809623718262, BER : 0.17059375\n",
            "Train[0/3][60/240] Loss: 1.0079386234283447, BER : 0.16896875\n",
            "Train[0/3][61/240] Loss: 0.9330565929412842, BER : 0.16189453125\n",
            "Train[0/3][62/240] Loss: 0.9733372926712036, BER : 0.16724609375\n",
            "Train[0/3][63/240] Loss: 0.9706946611404419, BER : 0.165359375\n",
            "Train[0/3][64/240] Loss: 1.0041099786758423, BER : 0.1689375\n",
            "Train[0/3][65/240] Loss: 1.083534598350525, BER : 0.17684765625\n",
            "Train[0/3][66/240] Loss: 1.0113885402679443, BER : 0.16960546875\n",
            "Train[0/3][67/240] Loss: 1.0046236515045166, BER : 0.17103515625\n",
            "Train[0/3][68/240] Loss: 0.9909861087799072, BER : 0.1693046875\n",
            "Train[0/3][69/240] Loss: 1.1224453449249268, BER : 0.1872265625\n",
            "Train[0/3][70/240] Loss: 1.104293704032898, BER : 0.1843125\n",
            "Train[0/3][71/240] Loss: 1.0400118827819824, BER : 0.17846484375\n",
            "Train[0/3][72/240] Loss: 1.0681087970733643, BER : 0.18130078125\n",
            "Train[0/3][73/240] Loss: 1.0651049613952637, BER : 0.1831328125\n",
            "Train[0/3][74/240] Loss: 1.0458627939224243, BER : 0.18070703125\n",
            "Train[0/3][75/240] Loss: 1.0319973230361938, BER : 0.179390625\n",
            "Train[0/3][76/240] Loss: 1.1285239458084106, BER : 0.18932421875\n",
            "Train[0/3][77/240] Loss: 1.052410364151001, BER : 0.18495703125\n",
            "Train[0/3][78/240] Loss: 1.1242209672927856, BER : 0.1938515625\n",
            "Train[0/3][79/240] Loss: 1.0443774461746216, BER : 0.1854296875\n",
            "Train[0/3][80/240] Loss: 1.07275390625, BER : 0.1881328125\n",
            "Train[0/3][81/240] Loss: 1.0882152318954468, BER : 0.1888515625\n",
            "Train[0/3][82/240] Loss: 1.0949645042419434, BER : 0.1915546875\n",
            "Train[0/3][83/240] Loss: 1.190915822982788, BER : 0.20712890625\n",
            "Train[0/3][84/240] Loss: 1.0511133670806885, BER : 0.1863515625\n",
            "Train[0/3][85/240] Loss: 1.0409623384475708, BER : 0.18883203125\n",
            "Train[0/3][86/240] Loss: 1.0535670518875122, BER : 0.19015625\n",
            "Train[0/3][87/240] Loss: 1.1158808469772339, BER : 0.20108984375\n",
            "Train[0/3][88/240] Loss: 1.0727895498275757, BER : 0.19508984375\n",
            "Train[0/3][89/240] Loss: 1.1469736099243164, BER : 0.2040703125\n",
            "Train[0/3][90/240] Loss: 1.0500578880310059, BER : 0.19380078125\n",
            "Train[0/3][91/240] Loss: 1.1062982082366943, BER : 0.199296875\n",
            "Train[0/3][92/240] Loss: 1.162820816040039, BER : 0.2069296875\n",
            "Train[0/3][93/240] Loss: 1.0416808128356934, BER : 0.19004296875\n",
            "Train[0/3][94/240] Loss: 1.0394463539123535, BER : 0.1924453125\n",
            "Train[0/3][95/240] Loss: 0.997074544429779, BER : 0.18561328125\n",
            "Train[0/3][96/240] Loss: 1.0013492107391357, BER : 0.188109375\n",
            "Train[0/3][97/240] Loss: 1.1078245639801025, BER : 0.20040234375\n",
            "Train[0/3][98/240] Loss: 1.1267366409301758, BER : 0.2053828125\n",
            "Train[0/3][99/240] Loss: 1.0852677822113037, BER : 0.20439453125\n",
            "Train[0/3][100/240] Loss: 1.2227638959884644, BER : 0.2164921875\n",
            "Train[0/3][101/240] Loss: 1.2848844528198242, BER : 0.2251640625\n",
            "Train[0/3][102/240] Loss: 1.1449625492095947, BER : 0.20652734375\n",
            "Train[0/3][103/240] Loss: 1.1445071697235107, BER : 0.20961328125\n",
            "Train[0/3][104/240] Loss: 1.0980147123336792, BER : 0.20451953125\n",
            "Train[0/3][105/240] Loss: 1.1273645162582397, BER : 0.2096875\n",
            "Train[0/3][106/240] Loss: 1.035136342048645, BER : 0.194015625\n",
            "Train[0/3][107/240] Loss: 1.1117689609527588, BER : 0.20741015625\n",
            "Train[0/3][108/240] Loss: 1.2624390125274658, BER : 0.2263671875\n",
            "Train[0/3][109/240] Loss: 1.2313451766967773, BER : 0.224109375\n",
            "Train[0/3][110/240] Loss: 1.0952235460281372, BER : 0.20546875\n",
            "Train[0/3][111/240] Loss: 1.1925456523895264, BER : 0.2199140625\n",
            "Train[0/3][112/240] Loss: 1.1396470069885254, BER : 0.213375\n",
            "Train[0/3][113/240] Loss: 1.052977204322815, BER : 0.20366796875\n",
            "Train[0/3][114/240] Loss: 1.0209239721298218, BER : 0.19705859375\n",
            "Train[0/3][115/240] Loss: 1.128737211227417, BER : 0.214875\n",
            "Train[0/3][116/240] Loss: 1.1598252058029175, BER : 0.21742578125\n",
            "Train[0/3][117/240] Loss: 1.0829260349273682, BER : 0.21067578125\n",
            "Train[0/3][118/240] Loss: 1.1034153699874878, BER : 0.21269140625\n",
            "Train[0/3][119/240] Loss: 1.0686668157577515, BER : 0.20912890625\n",
            "Train[0/3][120/240] Loss: 1.1765350103378296, BER : 0.22433203125\n",
            "Train[0/3][121/240] Loss: 1.0481598377227783, BER : 0.20215234375\n",
            "Train[0/3][122/240] Loss: 1.0233824253082275, BER : 0.200125\n",
            "Train[0/3][123/240] Loss: 1.1221948862075806, BER : 0.21316015625\n",
            "Train[0/3][124/240] Loss: 1.1306475400924683, BER : 0.21212109375\n",
            "Train[0/3][125/240] Loss: 1.113524317741394, BER : 0.212578125\n",
            "Train[0/3][126/240] Loss: 1.0294636487960815, BER : 0.20198046875\n",
            "Train[0/3][127/240] Loss: 0.9718173742294312, BER : 0.1918828125\n",
            "Train[0/3][128/240] Loss: 0.9669917821884155, BER : 0.19326953125\n",
            "Train[0/3][129/240] Loss: 1.0332589149475098, BER : 0.2030390625\n",
            "Train[0/3][130/240] Loss: 1.2093855142593384, BER : 0.228515625\n",
            "Train[0/3][131/240] Loss: 1.2146046161651611, BER : 0.229921875\n",
            "Train[0/3][132/240] Loss: 1.2031476497650146, BER : 0.22883984375\n",
            "Train[0/3][133/240] Loss: 1.181908130645752, BER : 0.22649609375\n",
            "Train[0/3][134/240] Loss: 1.0635087490081787, BER : 0.21008984375\n",
            "Train[0/3][135/240] Loss: 1.0419526100158691, BER : 0.2046171875\n",
            "Train[0/3][136/240] Loss: 0.9756988883018494, BER : 0.1949609375\n",
            "Train[0/3][137/240] Loss: 0.9516385793685913, BER : 0.18960546875\n",
            "Train[0/3][138/240] Loss: 0.9734020233154297, BER : 0.19328515625\n",
            "Train[0/3][139/240] Loss: 0.9846464395523071, BER : 0.1954375\n",
            "Train[0/3][140/240] Loss: 1.030383586883545, BER : 0.20439453125\n",
            "Train[0/3][141/240] Loss: 1.1934877634048462, BER : 0.2297734375\n",
            "Train[0/3][142/240] Loss: 1.1004422903060913, BER : 0.214828125\n",
            "Train[0/3][143/240] Loss: 1.1476941108703613, BER : 0.2230859375\n",
            "Train[0/3][144/240] Loss: 1.1077700853347778, BER : 0.21706640625\n",
            "Train[0/3][145/240] Loss: 1.0842934846878052, BER : 0.2138828125\n",
            "Train[0/3][146/240] Loss: 1.0296263694763184, BER : 0.20355078125\n",
            "Train[0/3][147/240] Loss: 0.9752688407897949, BER : 0.1928828125\n",
            "Train[0/3][148/240] Loss: 0.9616135954856873, BER : 0.191765625\n",
            "Train[0/3][149/240] Loss: 0.9544198513031006, BER : 0.190578125\n",
            "Train[0/3][150/240] Loss: 0.9786468148231506, BER : 0.195953125\n",
            "Train[0/3][151/240] Loss: 0.9796619415283203, BER : 0.19534765625\n",
            "Train[0/3][152/240] Loss: 1.0292611122131348, BER : 0.2039921875\n",
            "Train[0/3][153/240] Loss: 1.0534532070159912, BER : 0.2082109375\n",
            "Train[0/3][154/240] Loss: 1.0815587043762207, BER : 0.209953125\n",
            "Train[0/3][155/240] Loss: 1.1293339729309082, BER : 0.2187421875\n",
            "Train[0/3][156/240] Loss: 1.055438756942749, BER : 0.20732421875\n",
            "Train[0/3][157/240] Loss: 1.0582261085510254, BER : 0.209203125\n",
            "Train[0/3][158/240] Loss: 1.0598905086517334, BER : 0.207703125\n",
            "Train[0/3][159/240] Loss: 1.0253630876541138, BER : 0.20394921875\n",
            "Train[0/3][160/240] Loss: 0.9934782981872559, BER : 0.19964453125\n",
            "Train[0/3][161/240] Loss: 1.015708565711975, BER : 0.20290234375\n",
            "Train[0/3][162/240] Loss: 0.9888043999671936, BER : 0.2000859375\n",
            "Train[0/3][163/240] Loss: 0.9979596734046936, BER : 0.19949609375\n",
            "Train[0/3][164/240] Loss: 1.1054892539978027, BER : 0.21829296875\n",
            "Train[0/3][165/240] Loss: 1.233234167098999, BER : 0.24341796875\n",
            "Train[0/3][166/240] Loss: 1.1364309787750244, BER : 0.220046875\n",
            "Train[0/3][167/240] Loss: 1.2243075370788574, BER : 0.24181640625\n",
            "Train[0/3][168/240] Loss: 1.262976050376892, BER : 0.2484375\n",
            "Train[0/3][169/240] Loss: 1.2177802324295044, BER : 0.23875390625\n",
            "Train[0/3][170/240] Loss: 1.2338874340057373, BER : 0.24262890625\n",
            "Train[0/3][171/240] Loss: 1.2389256954193115, BER : 0.24290234375\n",
            "Train[0/3][172/240] Loss: 1.113917589187622, BER : 0.216421875\n",
            "Train[0/3][173/240] Loss: 1.063554286956787, BER : 0.20834765625\n",
            "Train[0/3][174/240] Loss: 1.0397757291793823, BER : 0.20486328125\n",
            "Train[0/3][175/240] Loss: 1.11385977268219, BER : 0.2054296875\n",
            "Train[0/3][176/240] Loss: 1.0055692195892334, BER : 0.1993515625\n",
            "Train[0/3][177/240] Loss: 1.0885136127471924, BER : 0.20289453125\n",
            "Train[0/3][178/240] Loss: 0.9885167479515076, BER : 0.1971484375\n",
            "Train[0/3][179/240] Loss: 0.9759551286697388, BER : 0.1948203125\n",
            "Train[0/3][180/240] Loss: 0.9407681226730347, BER : 0.18933984375\n",
            "Train[0/3][181/240] Loss: 0.9854339957237244, BER : 0.19875390625\n",
            "Train[0/3][182/240] Loss: 0.9463454484939575, BER : 0.19204296875\n",
            "Train[0/3][183/240] Loss: 0.9410131573677063, BER : 0.1903046875\n",
            "Train[0/3][184/240] Loss: 0.9609144926071167, BER : 0.19351171875\n",
            "Train[0/3][185/240] Loss: 0.9374427795410156, BER : 0.1898046875\n",
            "Train[0/3][186/240] Loss: 0.9670485258102417, BER : 0.1968828125\n",
            "Train[0/3][187/240] Loss: 0.9555008411407471, BER : 0.1965\n",
            "Train[0/3][188/240] Loss: 0.9569542407989502, BER : 0.1947890625\n",
            "Train[0/3][189/240] Loss: 0.9588507413864136, BER : 0.19494140625\n",
            "Train[0/3][190/240] Loss: 1.043143391609192, BER : 0.20278125\n",
            "Train[0/3][191/240] Loss: 1.0291885137557983, BER : 0.201328125\n",
            "Train[0/3][192/240] Loss: 1.0427682399749756, BER : 0.20365625\n",
            "Train[0/3][193/240] Loss: 1.068563461303711, BER : 0.22826171875\n",
            "Train[0/3][194/240] Loss: 1.0003427267074585, BER : 0.20246875\n",
            "Train[0/3][195/240] Loss: 1.0083281993865967, BER : 0.206078125\n",
            "Train[0/3][196/240] Loss: 1.0897555351257324, BER : 0.23359765625\n",
            "Train[0/3][197/240] Loss: 1.0741249322891235, BER : 0.230171875\n",
            "Train[0/3][198/240] Loss: 1.1386919021606445, BER : 0.23936328125\n",
            "Train[0/3][199/240] Loss: 1.0171453952789307, BER : 0.211546875\n",
            "Train[0/3][200/240] Loss: 1.0933924913406372, BER : 0.2341171875\n",
            "Train[0/3][201/240] Loss: 1.0131409168243408, BER : 0.21047265625\n",
            "Train[0/3][202/240] Loss: 1.0150470733642578, BER : 0.21079296875\n",
            "Train[0/3][203/240] Loss: 1.010188102722168, BER : 0.21091015625\n",
            "Train[0/3][204/240] Loss: 0.959772527217865, BER : 0.19948046875\n",
            "Train[0/3][205/240] Loss: 0.970461905002594, BER : 0.19944921875\n",
            "Train[0/3][206/240] Loss: 0.9650086164474487, BER : 0.20091796875\n",
            "Train[0/3][207/240] Loss: 0.9698328375816345, BER : 0.20180859375\n",
            "Train[0/3][208/240] Loss: 0.9834993481636047, BER : 0.2002578125\n",
            "Train[0/3][209/240] Loss: 0.9504802227020264, BER : 0.19505859375\n",
            "Train[0/3][210/240] Loss: 0.9704189896583557, BER : 0.19728125\n",
            "Train[0/3][211/240] Loss: 0.9654552340507507, BER : 0.2013828125\n",
            "Train[0/3][212/240] Loss: 0.9584567546844482, BER : 0.20330078125\n",
            "Train[0/3][213/240] Loss: 0.9478158354759216, BER : 0.199171875\n",
            "Train[0/3][214/240] Loss: 0.9521689414978027, BER : 0.19983203125\n",
            "Train[0/3][215/240] Loss: 0.9369453191757202, BER : 0.19909375\n",
            "Train[0/3][216/240] Loss: 0.9462970495223999, BER : 0.19778125\n",
            "Train[0/3][217/240] Loss: 0.9407596588134766, BER : 0.19720703125\n",
            "Train[0/3][218/240] Loss: 0.9878793358802795, BER : 0.2077578125\n",
            "Train[0/3][219/240] Loss: 0.9701132774353027, BER : 0.203015625\n",
            "Train[0/3][220/240] Loss: 0.9417814612388611, BER : 0.193265625\n",
            "Train[0/3][221/240] Loss: 0.951661229133606, BER : 0.19820703125\n",
            "Train[0/3][222/240] Loss: 0.9636384844779968, BER : 0.20398828125\n",
            "Train[0/3][223/240] Loss: 0.9726142883300781, BER : 0.20462109375\n",
            "Train[0/3][224/240] Loss: 0.9584933519363403, BER : 0.2026796875\n",
            "Train[0/3][225/240] Loss: 0.9229719638824463, BER : 0.19546484375\n",
            "Train[0/3][226/240] Loss: 0.925514817237854, BER : 0.1954765625\n",
            "Train[0/3][227/240] Loss: 0.968925952911377, BER : 0.2058515625\n",
            "Train[0/3][228/240] Loss: 0.9365084171295166, BER : 0.19884765625\n",
            "Train[0/3][229/240] Loss: 0.9318103790283203, BER : 0.19191015625\n",
            "Train[0/3][230/240] Loss: 0.9343224167823792, BER : 0.19463671875\n",
            "Train[0/3][231/240] Loss: 0.9309331178665161, BER : 0.1949765625\n",
            "Train[0/3][232/240] Loss: 0.914420485496521, BER : 0.19198046875\n",
            "Train[0/3][233/240] Loss: 0.923960268497467, BER : 0.1930703125\n",
            "Train[0/3][234/240] Loss: 0.9225738644599915, BER : 0.1973203125\n",
            "Train[0/3][235/240] Loss: 0.919721782207489, BER : 0.19880859375\n",
            "Train[0/3][236/240] Loss: 0.9197548031806946, BER : 0.19682421875\n",
            "Train[0/3][237/240] Loss: 0.902479887008667, BER : 0.1932109375\n",
            "Train[0/3][238/240] Loss: 0.8772571086883545, BER : 0.18917578125\n",
            "Train[0/3][239/240] Loss: 0.8767728805541992, BER : 0.19063671875\n",
            "Test[0/3][0/40]  Loss: 2.1680543422698975, BER (test): 0.38207421875\n",
            "Test[0/3][1/40]  Loss: 2.1577584743499756, BER (test): 0.3826328125\n",
            "Test[0/3][2/40]  Loss: 2.179917812347412, BER (test): 0.384359375\n",
            "Test[0/3][3/40]  Loss: 2.1803529262542725, BER (test): 0.3854453125\n",
            "Test[0/3][4/40]  Loss: 2.1798038482666016, BER (test): 0.38593359375\n",
            "Test[0/3][5/40]  Loss: 2.1781046390533447, BER (test): 0.38528125\n",
            "Test[0/3][6/40]  Loss: 2.173433303833008, BER (test): 0.3835859375\n",
            "Test[0/3][7/40]  Loss: 2.1754794120788574, BER (test): 0.3843515625\n",
            "Test[0/3][8/40]  Loss: 2.166886329650879, BER (test): 0.38291015625\n",
            "Test[0/3][9/40]  Loss: 2.167605400085449, BER (test): 0.38184375\n",
            "Test[0/3][10/40]  Loss: 2.1710009574890137, BER (test): 0.3836015625\n",
            "Test[0/3][11/40]  Loss: 2.1828484535217285, BER (test): 0.38609765625\n",
            "Test[0/3][12/40]  Loss: 2.1676337718963623, BER (test): 0.3834375\n",
            "Test[0/3][13/40]  Loss: 2.159735679626465, BER (test): 0.38169140625\n",
            "Test[0/3][14/40]  Loss: 2.166006088256836, BER (test): 0.38166796875\n",
            "Test[0/3][15/40]  Loss: 2.2005114555358887, BER (test): 0.38708203125\n",
            "Test[0/3][16/40]  Loss: 2.1654627323150635, BER (test): 0.38323046875\n",
            "Test[0/3][17/40]  Loss: 2.16896653175354, BER (test): 0.3836953125\n",
            "Test[0/3][18/40]  Loss: 2.1846790313720703, BER (test): 0.38626953125\n",
            "Test[0/3][19/40]  Loss: 2.1788206100463867, BER (test): 0.38399609375\n",
            "Test[0/3][20/40]  Loss: 2.1537680625915527, BER (test): 0.38149609375\n",
            "Test[0/3][21/40]  Loss: 2.1888933181762695, BER (test): 0.3867578125\n",
            "Test[0/3][22/40]  Loss: 2.1800570487976074, BER (test): 0.38305078125\n",
            "Test[0/3][23/40]  Loss: 2.1901700496673584, BER (test): 0.38636328125\n",
            "Test[0/3][24/40]  Loss: 2.1720118522644043, BER (test): 0.3832109375\n",
            "Test[0/3][25/40]  Loss: 2.1688544750213623, BER (test): 0.384\n",
            "Test[0/3][26/40]  Loss: 2.1668107509613037, BER (test): 0.38419140625\n",
            "Test[0/3][27/40]  Loss: 2.173922061920166, BER (test): 0.38333203125\n",
            "Test[0/3][28/40]  Loss: 2.1494832038879395, BER (test): 0.3798515625\n",
            "Test[0/3][29/40]  Loss: 2.1772875785827637, BER (test): 0.38387109375\n",
            "Test[0/3][30/40]  Loss: 2.1826343536376953, BER (test): 0.38623828125\n",
            "Test[0/3][31/40]  Loss: 2.164379119873047, BER (test): 0.381828125\n",
            "Test[0/3][32/40]  Loss: 2.1683201789855957, BER (test): 0.38277734375\n",
            "Test[0/3][33/40]  Loss: 2.170144557952881, BER (test): 0.3828671875\n",
            "Test[0/3][34/40]  Loss: 2.1892385482788086, BER (test): 0.38567578125\n",
            "Test[0/3][35/40]  Loss: 2.1713168621063232, BER (test): 0.3832890625\n",
            "Test[0/3][36/40]  Loss: 2.194444417953491, BER (test): 0.387109375\n",
            "Test[0/3][37/40]  Loss: 2.185018301010132, BER (test): 0.3853828125\n",
            "Test[0/3][38/40]  Loss: 2.1644511222839355, BER (test): 0.38404296875\n",
            "Test[0/3][39/40]  Loss: 2.164525270462036, BER (test): 0.38351171875\n",
            "Train[1/3][0/240] Loss: 0.9073829650878906, BER : 0.1948515625\n",
            "Train[1/3][1/240] Loss: 0.8902662992477417, BER : 0.1863828125\n",
            "Train[1/3][2/240] Loss: 0.8705868721008301, BER : 0.1876015625\n",
            "Train[1/3][3/240] Loss: 0.8833799362182617, BER : 0.1841640625\n",
            "Train[1/3][4/240] Loss: 0.8943164348602295, BER : 0.19430859375\n",
            "Train[1/3][5/240] Loss: 0.9103339314460754, BER : 0.19647265625\n",
            "Train[1/3][6/240] Loss: 0.8478097915649414, BER : 0.18376953125\n",
            "Train[1/3][7/240] Loss: 0.8992456793785095, BER : 0.1950390625\n",
            "Train[1/3][8/240] Loss: 0.8919771909713745, BER : 0.19378515625\n",
            "Train[1/3][9/240] Loss: 0.8985837697982788, BER : 0.19519921875\n",
            "Train[1/3][10/240] Loss: 0.9353480339050293, BER : 0.211125\n",
            "Train[1/3][11/240] Loss: 0.8986091017723083, BER : 0.1946015625\n",
            "Train[1/3][12/240] Loss: 0.8906282186508179, BER : 0.192828125\n",
            "Train[1/3][13/240] Loss: 0.9072993993759155, BER : 0.19748046875\n",
            "Train[1/3][14/240] Loss: 0.8575206995010376, BER : 0.18524609375\n",
            "Train[1/3][15/240] Loss: 0.8804216384887695, BER : 0.1919921875\n",
            "Train[1/3][16/240] Loss: 0.8472139239311218, BER : 0.18499609375\n",
            "Train[1/3][17/240] Loss: 0.8581251502037048, BER : 0.186625\n",
            "Train[1/3][18/240] Loss: 0.8489059805870056, BER : 0.1866640625\n",
            "Train[1/3][19/240] Loss: 0.8363375067710876, BER : 0.17901171875\n",
            "Train[1/3][20/240] Loss: 0.845898449420929, BER : 0.1827890625\n",
            "Train[1/3][21/240] Loss: 0.8668763637542725, BER : 0.19076953125\n",
            "Train[1/3][22/240] Loss: 0.8285736441612244, BER : 0.1827578125\n",
            "Train[1/3][23/240] Loss: 0.8580331802368164, BER : 0.18762890625\n",
            "Train[1/3][24/240] Loss: 0.8588317036628723, BER : 0.191171875\n",
            "Train[1/3][25/240] Loss: 0.8455969095230103, BER : 0.18791796875\n",
            "Train[1/3][26/240] Loss: 0.8541539311408997, BER : 0.1898671875\n",
            "Train[1/3][27/240] Loss: 0.8500543832778931, BER : 0.1880234375\n",
            "Train[1/3][28/240] Loss: 0.8493045568466187, BER : 0.19005859375\n",
            "Train[1/3][29/240] Loss: 0.8473089337348938, BER : 0.18980859375\n",
            "Train[1/3][30/240] Loss: 0.8114917278289795, BER : 0.1816171875\n",
            "Train[1/3][31/240] Loss: 0.8314650654792786, BER : 0.18398046875\n",
            "Train[1/3][32/240] Loss: 0.8071006536483765, BER : 0.1806328125\n",
            "Train[1/3][33/240] Loss: 0.8177310228347778, BER : 0.18366015625\n",
            "Train[1/3][34/240] Loss: 0.8293703198432922, BER : 0.18625390625\n",
            "Train[1/3][35/240] Loss: 0.8066461086273193, BER : 0.18151171875\n",
            "Train[1/3][36/240] Loss: 0.8003190755844116, BER : 0.18040234375\n",
            "Train[1/3][37/240] Loss: 0.793587327003479, BER : 0.17846484375\n",
            "Train[1/3][38/240] Loss: 0.8120166659355164, BER : 0.184140625\n",
            "Train[1/3][39/240] Loss: 0.7864409685134888, BER : 0.1776328125\n",
            "Train[1/3][40/240] Loss: 0.8215197920799255, BER : 0.18463671875\n",
            "Train[1/3][41/240] Loss: 0.8321887254714966, BER : 0.18890234375\n",
            "Train[1/3][42/240] Loss: 0.8187955021858215, BER : 0.18486328125\n",
            "Train[1/3][43/240] Loss: 0.8044612407684326, BER : 0.18334765625\n",
            "Train[1/3][44/240] Loss: 0.8116120100021362, BER : 0.1812734375\n",
            "Train[1/3][45/240] Loss: 0.8181911706924438, BER : 0.1866015625\n",
            "Train[1/3][46/240] Loss: 0.8052457571029663, BER : 0.18293359375\n",
            "Train[1/3][47/240] Loss: 0.815170407295227, BER : 0.18514453125\n",
            "Train[1/3][48/240] Loss: 0.7975894212722778, BER : 0.17896484375\n",
            "Train[1/3][49/240] Loss: 0.8041027784347534, BER : 0.1817109375\n",
            "Train[1/3][50/240] Loss: 0.7957381010055542, BER : 0.180484375\n",
            "Train[1/3][51/240] Loss: 0.8019919395446777, BER : 0.1794765625\n",
            "Train[1/3][52/240] Loss: 0.8143826723098755, BER : 0.1844296875\n",
            "Train[1/3][53/240] Loss: 0.7788982391357422, BER : 0.17925\n",
            "Train[1/3][54/240] Loss: 0.8037828803062439, BER : 0.1816171875\n",
            "Train[1/3][55/240] Loss: 0.7873144745826721, BER : 0.17942578125\n",
            "Train[1/3][56/240] Loss: 0.7895272970199585, BER : 0.1768046875\n",
            "Train[1/3][57/240] Loss: 0.768741250038147, BER : 0.1764296875\n",
            "Train[1/3][58/240] Loss: 0.7505732774734497, BER : 0.17152734375\n",
            "Train[1/3][59/240] Loss: 0.7434594035148621, BER : 0.1701875\n",
            "Train[1/3][60/240] Loss: 0.7489022016525269, BER : 0.1715\n",
            "Train[1/3][61/240] Loss: 0.7373958230018616, BER : 0.1688671875\n",
            "Train[1/3][62/240] Loss: 0.7332338094711304, BER : 0.16703515625\n",
            "Train[1/3][63/240] Loss: 0.760767936706543, BER : 0.17327734375\n",
            "Train[1/3][64/240] Loss: 0.737316370010376, BER : 0.1694921875\n",
            "Train[1/3][65/240] Loss: 0.7423239946365356, BER : 0.17119921875\n",
            "Train[1/3][66/240] Loss: 0.7303419709205627, BER : 0.16612109375\n",
            "Train[1/3][67/240] Loss: 0.7376372814178467, BER : 0.169546875\n",
            "Train[1/3][68/240] Loss: 0.745652437210083, BER : 0.1722578125\n",
            "Train[1/3][69/240] Loss: 0.7320607304573059, BER : 0.16801171875\n",
            "Train[1/3][70/240] Loss: 0.7244763374328613, BER : 0.16612890625\n",
            "Train[1/3][71/240] Loss: 0.7341256737709045, BER : 0.17163671875\n",
            "Train[1/3][72/240] Loss: 0.7591089010238647, BER : 0.1769453125\n",
            "Train[1/3][73/240] Loss: 0.7605605125427246, BER : 0.17533203125\n",
            "Train[1/3][74/240] Loss: 0.7528061270713806, BER : 0.1744453125\n",
            "Train[1/3][75/240] Loss: 0.8180328607559204, BER : 0.19253515625\n",
            "Train[1/3][76/240] Loss: 0.7508337497711182, BER : 0.1731640625\n",
            "Train[1/3][77/240] Loss: 0.768256425857544, BER : 0.17573828125\n",
            "Train[1/3][78/240] Loss: 0.7544766664505005, BER : 0.1702578125\n",
            "Train[1/3][79/240] Loss: 0.7277200222015381, BER : 0.16819140625\n",
            "Train[1/3][80/240] Loss: 0.7361772060394287, BER : 0.16972265625\n",
            "Train[1/3][81/240] Loss: 0.7176306247711182, BER : 0.16567578125\n",
            "Train[1/3][82/240] Loss: 0.7306801676750183, BER : 0.16915625\n",
            "Train[1/3][83/240] Loss: 0.744645357131958, BER : 0.17214453125\n",
            "Train[1/3][84/240] Loss: 0.7081630229949951, BER : 0.1625859375\n",
            "Train[1/3][85/240] Loss: 0.7337491512298584, BER : 0.16885546875\n",
            "Train[1/3][86/240] Loss: 0.7432123422622681, BER : 0.1726640625\n",
            "Train[1/3][87/240] Loss: 0.7402420043945312, BER : 0.1711171875\n",
            "Train[1/3][88/240] Loss: 0.7381289005279541, BER : 0.17271875\n",
            "Train[1/3][89/240] Loss: 0.7304080724716187, BER : 0.16867578125\n",
            "Train[1/3][90/240] Loss: 0.7080105543136597, BER : 0.16400390625\n",
            "Train[1/3][91/240] Loss: 0.7245555520057678, BER : 0.16991015625\n",
            "Train[1/3][92/240] Loss: 0.7198023796081543, BER : 0.16678125\n",
            "Train[1/3][93/240] Loss: 0.7155427932739258, BER : 0.16520703125\n",
            "Train[1/3][94/240] Loss: 0.7309386730194092, BER : 0.16865234375\n",
            "Train[1/3][95/240] Loss: 0.7262224555015564, BER : 0.167921875\n",
            "Train[1/3][96/240] Loss: 0.7150698900222778, BER : 0.16616015625\n",
            "Train[1/3][97/240] Loss: 0.7177606225013733, BER : 0.16721875\n",
            "Train[1/3][98/240] Loss: 0.7082394361495972, BER : 0.1660625\n",
            "Train[1/3][99/240] Loss: 0.746356725692749, BER : 0.16934765625\n",
            "Train[1/3][100/240] Loss: 0.718018651008606, BER : 0.16898828125\n",
            "Train[1/3][101/240] Loss: 0.7410163879394531, BER : 0.17109375\n",
            "Train[1/3][102/240] Loss: 0.7572123408317566, BER : 0.17255078125\n",
            "Train[1/3][103/240] Loss: 0.74457848072052, BER : 0.169078125\n",
            "Train[1/3][104/240] Loss: 0.7523425817489624, BER : 0.1703359375\n",
            "Train[1/3][105/240] Loss: 0.7296854257583618, BER : 0.1679609375\n",
            "Train[1/3][106/240] Loss: 0.7298879623413086, BER : 0.16896484375\n",
            "Train[1/3][107/240] Loss: 0.7130162715911865, BER : 0.16725\n",
            "Train[1/3][108/240] Loss: 0.7123079299926758, BER : 0.1631875\n",
            "Train[1/3][109/240] Loss: 0.7093833684921265, BER : 0.16435546875\n",
            "Train[1/3][110/240] Loss: 0.7065398097038269, BER : 0.1617734375\n",
            "Train[1/3][111/240] Loss: 0.691916823387146, BER : 0.15968359375\n",
            "Train[1/3][112/240] Loss: 0.6846492290496826, BER : 0.15683984375\n",
            "Train[1/3][113/240] Loss: 0.6925559043884277, BER : 0.1600859375\n",
            "Train[1/3][114/240] Loss: 0.702892005443573, BER : 0.16281640625\n",
            "Train[1/3][115/240] Loss: 0.7013019323348999, BER : 0.1614375\n",
            "Train[1/3][116/240] Loss: 0.7164233326911926, BER : 0.16433203125\n",
            "Train[1/3][117/240] Loss: 0.7211411595344543, BER : 0.16675390625\n",
            "Train[1/3][118/240] Loss: 0.7308192253112793, BER : 0.16794921875\n",
            "Train[1/3][119/240] Loss: 0.7443675398826599, BER : 0.17252734375\n",
            "Train[1/3][120/240] Loss: 0.7336812615394592, BER : 0.16908984375\n",
            "Train[1/3][121/240] Loss: 0.7251358032226562, BER : 0.16687890625\n",
            "Train[1/3][122/240] Loss: 0.737464189529419, BER : 0.166890625\n",
            "Train[1/3][123/240] Loss: 0.7122883796691895, BER : 0.162984375\n",
            "Train[1/3][124/240] Loss: 0.6979759931564331, BER : 0.15880078125\n",
            "Train[1/3][125/240] Loss: 0.6963921785354614, BER : 0.16111328125\n",
            "Train[1/3][126/240] Loss: 0.7073910236358643, BER : 0.16098046875\n",
            "Train[1/3][127/240] Loss: 0.6937709450721741, BER : 0.1609375\n",
            "Train[1/3][128/240] Loss: 0.7067042589187622, BER : 0.1628203125\n",
            "Train[1/3][129/240] Loss: 0.698767900466919, BER : 0.16201953125\n",
            "Train[1/3][130/240] Loss: 0.7088569402694702, BER : 0.1630703125\n",
            "Train[1/3][131/240] Loss: 0.7103694081306458, BER : 0.1632734375\n",
            "Train[1/3][132/240] Loss: 0.6862760782241821, BER : 0.155953125\n",
            "Train[1/3][133/240] Loss: 0.6982592344284058, BER : 0.16065234375\n",
            "Train[1/3][134/240] Loss: 0.6902366876602173, BER : 0.16006640625\n",
            "Train[1/3][135/240] Loss: 0.7276272773742676, BER : 0.16768359375\n",
            "Train[1/3][136/240] Loss: 0.7463651299476624, BER : 0.17040234375\n",
            "Train[1/3][137/240] Loss: 0.7364982962608337, BER : 0.1677890625\n",
            "Train[1/3][138/240] Loss: 0.7396104335784912, BER : 0.16775390625\n",
            "Train[1/3][139/240] Loss: 0.7315248250961304, BER : 0.16527734375\n",
            "Train[1/3][140/240] Loss: 0.7068901658058167, BER : 0.1631796875\n",
            "Train[1/3][141/240] Loss: 0.6831262111663818, BER : 0.15623828125\n",
            "Train[1/3][142/240] Loss: 0.6817619800567627, BER : 0.15662109375\n",
            "Train[1/3][143/240] Loss: 0.6734956502914429, BER : 0.15571484375\n",
            "Train[1/3][144/240] Loss: 0.6788098216056824, BER : 0.1556171875\n",
            "Train[1/3][145/240] Loss: 0.671017050743103, BER : 0.15296484375\n",
            "Train[1/3][146/240] Loss: 0.6815993785858154, BER : 0.15818359375\n",
            "Train[1/3][147/240] Loss: 0.6964225769042969, BER : 0.16008203125\n",
            "Train[1/3][148/240] Loss: 0.6957899928092957, BER : 0.15991015625\n",
            "Train[1/3][149/240] Loss: 0.7057501077651978, BER : 0.16269140625\n",
            "Train[1/3][150/240] Loss: 0.7342406511306763, BER : 0.16903515625\n",
            "Train[1/3][151/240] Loss: 0.7378438115119934, BER : 0.1677109375\n",
            "Train[1/3][152/240] Loss: 0.7398884296417236, BER : 0.16786328125\n",
            "Train[1/3][153/240] Loss: 0.7145336866378784, BER : 0.1626015625\n",
            "Train[1/3][154/240] Loss: 0.7017734050750732, BER : 0.15943359375\n",
            "Train[1/3][155/240] Loss: 0.7087818384170532, BER : 0.1621171875\n",
            "Train[1/3][156/240] Loss: 0.6775702238082886, BER : 0.152359375\n",
            "Train[1/3][157/240] Loss: 0.66923588514328, BER : 0.152625\n",
            "Train[1/3][158/240] Loss: 0.65964674949646, BER : 0.151828125\n",
            "Train[1/3][159/240] Loss: 0.6382903456687927, BER : 0.14568359375\n",
            "Train[1/3][160/240] Loss: 0.6523820757865906, BER : 0.14948046875\n",
            "Train[1/3][161/240] Loss: 0.6527633666992188, BER : 0.1501328125\n",
            "Train[1/3][162/240] Loss: 0.680446982383728, BER : 0.15678515625\n",
            "Train[1/3][163/240] Loss: 0.6821617484092712, BER : 0.15803125\n",
            "Train[1/3][164/240] Loss: 0.7199347615242004, BER : 0.16494140625\n",
            "Train[1/3][165/240] Loss: 0.7240433692932129, BER : 0.1680078125\n",
            "Train[1/3][166/240] Loss: 0.7484033107757568, BER : 0.17165234375\n",
            "Train[1/3][167/240] Loss: 0.7445039749145508, BER : 0.169953125\n",
            "Train[1/3][168/240] Loss: 0.7650513052940369, BER : 0.1734140625\n",
            "Train[1/3][169/240] Loss: 0.7477421164512634, BER : 0.168609375\n",
            "Train[1/3][170/240] Loss: 0.7534483671188354, BER : 0.1688828125\n",
            "Train[1/3][171/240] Loss: 0.7474485039710999, BER : 0.16801171875\n",
            "Train[1/3][172/240] Loss: 0.7507596015930176, BER : 0.168078125\n",
            "Train[1/3][173/240] Loss: 0.7085976004600525, BER : 0.16023046875\n",
            "Train[1/3][174/240] Loss: 0.7180532813072205, BER : 0.16259765625\n",
            "Train[1/3][175/240] Loss: 0.7226320505142212, BER : 0.16334765625\n",
            "Train[1/3][176/240] Loss: 0.7073291540145874, BER : 0.15731640625\n",
            "Train[1/3][177/240] Loss: 0.6840583086013794, BER : 0.1544453125\n",
            "Train[1/3][178/240] Loss: 0.6783372759819031, BER : 0.1529296875\n",
            "Train[1/3][179/240] Loss: 0.6936461329460144, BER : 0.15628125\n",
            "Train[1/3][180/240] Loss: 0.6678286790847778, BER : 0.150890625\n",
            "Train[1/3][181/240] Loss: 0.6654196381568909, BER : 0.15228125\n",
            "Train[1/3][182/240] Loss: 0.6956777572631836, BER : 0.15973828125\n",
            "Train[1/3][183/240] Loss: 0.6945254802703857, BER : 0.158265625\n",
            "Train[1/3][184/240] Loss: 0.6787168979644775, BER : 0.15625\n",
            "Train[1/3][185/240] Loss: 0.7218199372291565, BER : 0.1652265625\n",
            "Train[1/3][186/240] Loss: 0.6940773725509644, BER : 0.1596015625\n",
            "Train[1/3][187/240] Loss: 0.7251340746879578, BER : 0.16421875\n",
            "Train[1/3][188/240] Loss: 0.7269636392593384, BER : 0.16422265625\n",
            "Train[1/3][189/240] Loss: 0.6946986317634583, BER : 0.15917578125\n",
            "Train[1/3][190/240] Loss: 0.691307783126831, BER : 0.1557421875\n",
            "Train[1/3][191/240] Loss: 0.7156550884246826, BER : 0.1623203125\n",
            "Train[1/3][192/240] Loss: 0.69596266746521, BER : 0.158796875\n",
            "Train[1/3][193/240] Loss: 0.6950558423995972, BER : 0.158921875\n",
            "Train[1/3][194/240] Loss: 0.6968864798545837, BER : 0.158453125\n",
            "Train[1/3][195/240] Loss: 0.6788502931594849, BER : 0.15317578125\n",
            "Train[1/3][196/240] Loss: 0.6963586211204529, BER : 0.1586953125\n",
            "Train[1/3][197/240] Loss: 0.6753559112548828, BER : 0.1538984375\n",
            "Train[1/3][198/240] Loss: 0.6827107667922974, BER : 0.15530078125\n",
            "Train[1/3][199/240] Loss: 0.6835101842880249, BER : 0.1556328125\n",
            "Train[1/3][200/240] Loss: 0.6815072298049927, BER : 0.15576953125\n",
            "Train[1/3][201/240] Loss: 0.6820621490478516, BER : 0.156265625\n",
            "Train[1/3][202/240] Loss: 0.6872533559799194, BER : 0.15859375\n",
            "Train[1/3][203/240] Loss: 0.6635740995407104, BER : 0.15230859375\n",
            "Train[1/3][204/240] Loss: 0.6893917322158813, BER : 0.15748046875\n",
            "Train[1/3][205/240] Loss: 0.6831247210502625, BER : 0.15937890625\n",
            "Train[1/3][206/240] Loss: 0.6855624914169312, BER : 0.15877734375\n",
            "Train[1/3][207/240] Loss: 0.6843984723091125, BER : 0.15898046875\n",
            "Train[1/3][208/240] Loss: 0.6843962669372559, BER : 0.1589375\n",
            "Train[1/3][209/240] Loss: 0.687651515007019, BER : 0.16128515625\n",
            "Train[1/3][210/240] Loss: 0.6812223792076111, BER : 0.15711328125\n",
            "Train[1/3][211/240] Loss: 0.6676477193832397, BER : 0.154390625\n",
            "Train[1/3][212/240] Loss: 0.6515830159187317, BER : 0.14915234375\n",
            "Train[1/3][213/240] Loss: 0.6714674830436707, BER : 0.155484375\n",
            "Train[1/3][214/240] Loss: 0.6691944003105164, BER : 0.15647265625\n",
            "Train[1/3][215/240] Loss: 0.6502852439880371, BER : 0.15042578125\n",
            "Train[1/3][216/240] Loss: 0.6540205478668213, BER : 0.15244921875\n",
            "Train[1/3][217/240] Loss: 0.6628410816192627, BER : 0.1549453125\n",
            "Train[1/3][218/240] Loss: 0.6692584753036499, BER : 0.15653515625\n",
            "Train[1/3][219/240] Loss: 0.6679186820983887, BER : 0.15658203125\n",
            "Train[1/3][220/240] Loss: 0.6637730598449707, BER : 0.15607421875\n",
            "Train[1/3][221/240] Loss: 0.6617793440818787, BER : 0.1550390625\n",
            "Train[1/3][222/240] Loss: 0.6740666031837463, BER : 0.15751171875\n",
            "Train[1/3][223/240] Loss: 0.6677225232124329, BER : 0.1566328125\n",
            "Train[1/3][224/240] Loss: 0.6674110889434814, BER : 0.1546796875\n",
            "Train[1/3][225/240] Loss: 0.6728084087371826, BER : 0.157140625\n",
            "Train[1/3][226/240] Loss: 0.6505033373832703, BER : 0.14860546875\n",
            "Train[1/3][227/240] Loss: 0.6600584983825684, BER : 0.15379296875\n",
            "Train[1/3][228/240] Loss: 0.6894258260726929, BER : 0.1605546875\n",
            "Train[1/3][229/240] Loss: 0.6751025319099426, BER : 0.158140625\n",
            "Train[1/3][230/240] Loss: 0.6555526256561279, BER : 0.15169921875\n",
            "Train[1/3][231/240] Loss: 0.6707556247711182, BER : 0.1566328125\n",
            "Train[1/3][232/240] Loss: 0.65088951587677, BER : 0.1521875\n",
            "Train[1/3][233/240] Loss: 0.6530088782310486, BER : 0.15134765625\n",
            "Train[1/3][234/240] Loss: 0.6598747968673706, BER : 0.1536796875\n",
            "Train[1/3][235/240] Loss: 0.6578466296195984, BER : 0.1536328125\n",
            "Train[1/3][236/240] Loss: 0.6693208813667297, BER : 0.1567734375\n",
            "Train[1/3][237/240] Loss: 0.6501388549804688, BER : 0.1516875\n",
            "Train[1/3][238/240] Loss: 0.6432663202285767, BER : 0.149359375\n",
            "Train[1/3][239/240] Loss: 0.6676450371742249, BER : 0.15587109375\n",
            "Test[1/3][0/40]  Loss: 1.2194658517837524, BER (test): 0.25494140625\n",
            "Test[1/3][1/40]  Loss: 1.2195043563842773, BER (test): 0.25393359375\n",
            "Test[1/3][2/40]  Loss: 1.2118648290634155, BER (test): 0.25366015625\n",
            "Test[1/3][3/40]  Loss: 1.2311877012252808, BER (test): 0.25588671875\n",
            "Test[1/3][4/40]  Loss: 1.2121498584747314, BER (test): 0.2520390625\n",
            "Test[1/3][5/40]  Loss: 1.2175934314727783, BER (test): 0.25344921875\n",
            "Test[1/3][6/40]  Loss: 1.2163777351379395, BER (test): 0.25325390625\n",
            "Test[1/3][7/40]  Loss: 1.219290018081665, BER (test): 0.25405859375\n",
            "Test[1/3][8/40]  Loss: 1.2290096282958984, BER (test): 0.25328125\n",
            "Test[1/3][9/40]  Loss: 1.2210286855697632, BER (test): 0.2538046875\n",
            "Test[1/3][10/40]  Loss: 1.2165207862854004, BER (test): 0.25158203125\n",
            "Test[1/3][11/40]  Loss: 1.218345046043396, BER (test): 0.253203125\n",
            "Test[1/3][12/40]  Loss: 1.2173371315002441, BER (test): 0.25290234375\n",
            "Test[1/3][13/40]  Loss: 1.216944694519043, BER (test): 0.25369921875\n",
            "Test[1/3][14/40]  Loss: 1.2184041738510132, BER (test): 0.25434375\n",
            "Test[1/3][15/40]  Loss: 1.2172609567642212, BER (test): 0.25346875\n",
            "Test[1/3][16/40]  Loss: 1.2133493423461914, BER (test): 0.25259765625\n",
            "Test[1/3][17/40]  Loss: 1.2158281803131104, BER (test): 0.25448828125\n",
            "Test[1/3][18/40]  Loss: 1.2158451080322266, BER (test): 0.2524296875\n",
            "Test[1/3][19/40]  Loss: 1.2168292999267578, BER (test): 0.25329296875\n",
            "Test[1/3][20/40]  Loss: 1.2173100709915161, BER (test): 0.2536484375\n",
            "Test[1/3][21/40]  Loss: 1.2223857641220093, BER (test): 0.25322265625\n",
            "Test[1/3][22/40]  Loss: 1.2235136032104492, BER (test): 0.2545390625\n",
            "Test[1/3][23/40]  Loss: 1.2157902717590332, BER (test): 0.25340234375\n",
            "Test[1/3][24/40]  Loss: 1.20928955078125, BER (test): 0.25215625\n",
            "Test[1/3][25/40]  Loss: 1.2148927450180054, BER (test): 0.2528046875\n",
            "Test[1/3][26/40]  Loss: 1.221398949623108, BER (test): 0.25407421875\n",
            "Test[1/3][27/40]  Loss: 1.2245546579360962, BER (test): 0.25327734375\n",
            "Test[1/3][28/40]  Loss: 1.2156662940979004, BER (test): 0.2531953125\n",
            "Test[1/3][29/40]  Loss: 1.223934292793274, BER (test): 0.25448828125\n",
            "Test[1/3][30/40]  Loss: 1.2188104391098022, BER (test): 0.25362890625\n",
            "Test[1/3][31/40]  Loss: 1.2200692892074585, BER (test): 0.25650390625\n",
            "Test[1/3][32/40]  Loss: 1.2179895639419556, BER (test): 0.25461328125\n",
            "Test[1/3][33/40]  Loss: 1.220005750656128, BER (test): 0.25354296875\n",
            "Test[1/3][34/40]  Loss: 1.214136004447937, BER (test): 0.25216015625\n",
            "Test[1/3][35/40]  Loss: 1.2223255634307861, BER (test): 0.25537890625\n",
            "Test[1/3][36/40]  Loss: 1.2195230722427368, BER (test): 0.253515625\n",
            "Test[1/3][37/40]  Loss: 1.2199501991271973, BER (test): 0.2547734375\n",
            "Test[1/3][38/40]  Loss: 1.216813564300537, BER (test): 0.25523828125\n",
            "Test[1/3][39/40]  Loss: 1.2141011953353882, BER (test): 0.25500390625\n",
            "Train[2/3][0/240] Loss: 0.6640655994415283, BER : 0.1558203125\n",
            "Train[2/3][1/240] Loss: 0.6463940143585205, BER : 0.15050390625\n",
            "Train[2/3][2/240] Loss: 0.6699647903442383, BER : 0.15730078125\n",
            "Train[2/3][3/240] Loss: 0.6412348747253418, BER : 0.1489453125\n",
            "Train[2/3][4/240] Loss: 0.6623950600624084, BER : 0.1560859375\n",
            "Train[2/3][5/240] Loss: 0.65151447057724, BER : 0.153015625\n",
            "Train[2/3][6/240] Loss: 0.6520528793334961, BER : 0.15185546875\n",
            "Train[2/3][7/240] Loss: 0.6692236661911011, BER : 0.15580078125\n",
            "Train[2/3][8/240] Loss: 0.6648184061050415, BER : 0.156421875\n",
            "Train[2/3][9/240] Loss: 0.652259886264801, BER : 0.152296875\n",
            "Train[2/3][10/240] Loss: 0.6861766576766968, BER : 0.16198046875\n",
            "Train[2/3][11/240] Loss: 0.6567500829696655, BER : 0.15224609375\n",
            "Train[2/3][12/240] Loss: 0.6546511650085449, BER : 0.15253515625\n",
            "Train[2/3][13/240] Loss: 0.6535404920578003, BER : 0.1525703125\n",
            "Train[2/3][14/240] Loss: 0.6565752029418945, BER : 0.15162109375\n",
            "Train[2/3][15/240] Loss: 0.6621119379997253, BER : 0.153078125\n",
            "Train[2/3][16/240] Loss: 0.6729699373245239, BER : 0.15689453125\n",
            "Train[2/3][17/240] Loss: 0.6630775928497314, BER : 0.15414453125\n",
            "Train[2/3][18/240] Loss: 0.6663760542869568, BER : 0.1569609375\n",
            "Train[2/3][19/240] Loss: 0.6450897455215454, BER : 0.1492734375\n",
            "Train[2/3][20/240] Loss: 0.642157793045044, BER : 0.1494296875\n",
            "Train[2/3][21/240] Loss: 0.6444578766822815, BER : 0.14895703125\n",
            "Train[2/3][22/240] Loss: 0.6455525755882263, BER : 0.1516328125\n",
            "Train[2/3][23/240] Loss: 0.6606969237327576, BER : 0.15448046875\n",
            "Train[2/3][24/240] Loss: 0.6451396942138672, BER : 0.15075\n",
            "Train[2/3][25/240] Loss: 0.6407973766326904, BER : 0.15128515625\n",
            "Train[2/3][26/240] Loss: 0.6567685604095459, BER : 0.1560546875\n",
            "Train[2/3][27/240] Loss: 0.6586665511131287, BER : 0.15605078125\n",
            "Train[2/3][28/240] Loss: 0.6435421705245972, BER : 0.1503125\n",
            "Train[2/3][29/240] Loss: 0.6652364134788513, BER : 0.15834765625\n",
            "Train[2/3][30/240] Loss: 0.6461960077285767, BER : 0.15187890625\n",
            "Train[2/3][31/240] Loss: 0.6355141401290894, BER : 0.148296875\n",
            "Train[2/3][32/240] Loss: 0.6564674377441406, BER : 0.15230078125\n",
            "Train[2/3][33/240] Loss: 0.6367523670196533, BER : 0.14869140625\n",
            "Train[2/3][34/240] Loss: 0.6497484445571899, BER : 0.15153515625\n",
            "Train[2/3][35/240] Loss: 0.6344323754310608, BER : 0.14828515625\n",
            "Train[2/3][36/240] Loss: 0.6335592269897461, BER : 0.14913671875\n",
            "Train[2/3][37/240] Loss: 0.6530646085739136, BER : 0.1555078125\n",
            "Train[2/3][38/240] Loss: 0.6357519626617432, BER : 0.149\n",
            "Train[2/3][39/240] Loss: 0.6577205061912537, BER : 0.15668359375\n",
            "Train[2/3][40/240] Loss: 0.6340043544769287, BER : 0.14865625\n",
            "Train[2/3][41/240] Loss: 0.6635693311691284, BER : 0.155328125\n",
            "Train[2/3][42/240] Loss: 0.6594493985176086, BER : 0.15636328125\n",
            "Train[2/3][43/240] Loss: 0.6428748965263367, BER : 0.14872265625\n",
            "Train[2/3][44/240] Loss: 0.648074209690094, BER : 0.15234375\n",
            "Train[2/3][45/240] Loss: 0.6268559694290161, BER : 0.144859375\n",
            "Train[2/3][46/240] Loss: 0.6473603844642639, BER : 0.15248046875\n",
            "Train[2/3][47/240] Loss: 0.6342021226882935, BER : 0.148265625\n",
            "Train[2/3][48/240] Loss: 0.6404074430465698, BER : 0.14877734375\n",
            "Train[2/3][49/240] Loss: 0.6467450857162476, BER : 0.15185546875\n",
            "Train[2/3][50/240] Loss: 0.6377968192100525, BER : 0.151\n",
            "Train[2/3][51/240] Loss: 0.6336169242858887, BER : 0.1487890625\n",
            "Train[2/3][52/240] Loss: 0.6495065689086914, BER : 0.15298828125\n",
            "Train[2/3][53/240] Loss: 0.6537252068519592, BER : 0.156515625\n",
            "Train[2/3][54/240] Loss: 0.644929051399231, BER : 0.15198828125\n",
            "Train[2/3][55/240] Loss: 0.642090380191803, BER : 0.151078125\n",
            "Train[2/3][56/240] Loss: 0.645683765411377, BER : 0.15134765625\n",
            "Train[2/3][57/240] Loss: 0.638642430305481, BER : 0.147375\n",
            "Train[2/3][58/240] Loss: 0.6229159832000732, BER : 0.1431640625\n",
            "Train[2/3][59/240] Loss: 0.6425074338912964, BER : 0.15067578125\n",
            "Train[2/3][60/240] Loss: 0.638611912727356, BER : 0.14838671875\n",
            "Train[2/3][61/240] Loss: 0.6526256799697876, BER : 0.154546875\n",
            "Train[2/3][62/240] Loss: 0.6359813213348389, BER : 0.14974609375\n",
            "Train[2/3][63/240] Loss: 0.6480966806411743, BER : 0.15122265625\n",
            "Train[2/3][64/240] Loss: 0.6277483701705933, BER : 0.14707421875\n",
            "Train[2/3][65/240] Loss: 0.6436720490455627, BER : 0.15080078125\n",
            "Train[2/3][66/240] Loss: 0.6173574328422546, BER : 0.14578515625\n",
            "Train[2/3][67/240] Loss: 0.650327205657959, BER : 0.15355078125\n",
            "Train[2/3][68/240] Loss: 0.632937490940094, BER : 0.14881640625\n",
            "Train[2/3][69/240] Loss: 0.6344163417816162, BER : 0.15068359375\n",
            "Train[2/3][70/240] Loss: 0.6261991858482361, BER : 0.14812890625\n",
            "Train[2/3][71/240] Loss: 0.6205260753631592, BER : 0.14710546875\n",
            "Train[2/3][72/240] Loss: 0.638763964176178, BER : 0.150984375\n",
            "Train[2/3][73/240] Loss: 0.6294033527374268, BER : 0.14850390625\n",
            "Train[2/3][74/240] Loss: 0.6359608173370361, BER : 0.1505703125\n",
            "Train[2/3][75/240] Loss: 0.6417328119277954, BER : 0.1523046875\n",
            "Train[2/3][76/240] Loss: 0.6525915861129761, BER : 0.15603125\n",
            "Train[2/3][77/240] Loss: 0.6332365274429321, BER : 0.1494453125\n",
            "Train[2/3][78/240] Loss: 0.6349292993545532, BER : 0.15017578125\n",
            "Train[2/3][79/240] Loss: 0.6257768869400024, BER : 0.14728125\n",
            "Train[2/3][80/240] Loss: 0.6342857480049133, BER : 0.1504453125\n",
            "Train[2/3][81/240] Loss: 0.6320534944534302, BER : 0.14828515625\n",
            "Train[2/3][82/240] Loss: 0.6209409832954407, BER : 0.14505078125\n",
            "Train[2/3][83/240] Loss: 0.633198082447052, BER : 0.14918359375\n",
            "Train[2/3][84/240] Loss: 0.6381081342697144, BER : 0.15112890625\n",
            "Train[2/3][85/240] Loss: 0.6258584260940552, BER : 0.14878515625\n",
            "Train[2/3][86/240] Loss: 0.6201205849647522, BER : 0.14648828125\n",
            "Train[2/3][87/240] Loss: 0.62604820728302, BER : 0.148265625\n",
            "Train[2/3][88/240] Loss: 0.6230960488319397, BER : 0.14725\n",
            "Train[2/3][89/240] Loss: 0.6485015153884888, BER : 0.15305078125\n",
            "Train[2/3][90/240] Loss: 0.6348123550415039, BER : 0.14946875\n",
            "Train[2/3][91/240] Loss: 0.6346241235733032, BER : 0.15020703125\n",
            "Train[2/3][92/240] Loss: 0.6384131908416748, BER : 0.1504453125\n",
            "Train[2/3][93/240] Loss: 0.6467064619064331, BER : 0.153578125\n",
            "Train[2/3][94/240] Loss: 0.6337828040122986, BER : 0.150890625\n",
            "Train[2/3][95/240] Loss: 0.6282995939254761, BER : 0.14839453125\n",
            "Train[2/3][96/240] Loss: 0.6376188397407532, BER : 0.15099609375\n",
            "Train[2/3][97/240] Loss: 0.6422301530838013, BER : 0.15099609375\n",
            "Train[2/3][98/240] Loss: 0.6434912085533142, BER : 0.152765625\n",
            "Train[2/3][99/240] Loss: 0.6387643814086914, BER : 0.15210546875\n",
            "Train[2/3][100/240] Loss: 0.6395763754844666, BER : 0.15161328125\n",
            "Train[2/3][101/240] Loss: 0.628810465335846, BER : 0.14786328125\n",
            "Train[2/3][102/240] Loss: 0.6268749237060547, BER : 0.14741015625\n",
            "Train[2/3][103/240] Loss: 0.6406864523887634, BER : 0.1508984375\n",
            "Train[2/3][104/240] Loss: 0.6479045748710632, BER : 0.15290625\n",
            "Train[2/3][105/240] Loss: 0.6467941999435425, BER : 0.15366015625\n",
            "Train[2/3][106/240] Loss: 0.6276499032974243, BER : 0.14637890625\n",
            "Train[2/3][107/240] Loss: 0.6381949186325073, BER : 0.15026953125\n",
            "Train[2/3][108/240] Loss: 0.6263983845710754, BER : 0.14691015625\n",
            "Train[2/3][109/240] Loss: 0.6331549882888794, BER : 0.1486796875\n",
            "Train[2/3][110/240] Loss: 0.6367065906524658, BER : 0.14831640625\n",
            "Train[2/3][111/240] Loss: 0.6210366487503052, BER : 0.1428359375\n",
            "Train[2/3][112/240] Loss: 0.6205015182495117, BER : 0.1468671875\n",
            "Train[2/3][113/240] Loss: 0.6329128742218018, BER : 0.15083984375\n",
            "Train[2/3][114/240] Loss: 0.6402406096458435, BER : 0.15233203125\n",
            "Train[2/3][115/240] Loss: 0.6446086764335632, BER : 0.15273828125\n",
            "Train[2/3][116/240] Loss: 0.6259313225746155, BER : 0.14756640625\n",
            "Train[2/3][117/240] Loss: 0.6379204392433167, BER : 0.15162109375\n",
            "Train[2/3][118/240] Loss: 0.6277710795402527, BER : 0.149453125\n",
            "Train[2/3][119/240] Loss: 0.6324643492698669, BER : 0.1485703125\n",
            "Train[2/3][120/240] Loss: 0.6358522176742554, BER : 0.1505390625\n",
            "Train[2/3][121/240] Loss: 0.6262161731719971, BER : 0.14740234375\n",
            "Train[2/3][122/240] Loss: 0.6344653367996216, BER : 0.14957421875\n",
            "Train[2/3][123/240] Loss: 0.6242151260375977, BER : 0.1483046875\n",
            "Train[2/3][124/240] Loss: 0.6232330799102783, BER : 0.1473828125\n",
            "Train[2/3][125/240] Loss: 0.6397238373756409, BER : 0.152234375\n",
            "Train[2/3][126/240] Loss: 0.6303967237472534, BER : 0.1486171875\n",
            "Train[2/3][127/240] Loss: 0.6288033723831177, BER : 0.148\n",
            "Train[2/3][128/240] Loss: 0.6336832046508789, BER : 0.15015625\n",
            "Train[2/3][129/240] Loss: 0.6216965913772583, BER : 0.14490234375\n",
            "Train[2/3][130/240] Loss: 0.6249790191650391, BER : 0.14680078125\n",
            "Train[2/3][131/240] Loss: 0.6163484454154968, BER : 0.1454140625\n",
            "Train[2/3][132/240] Loss: 0.6244205832481384, BER : 0.1489609375\n",
            "Train[2/3][133/240] Loss: 0.6238999366760254, BER : 0.14762109375\n",
            "Train[2/3][134/240] Loss: 0.6196840405464172, BER : 0.14564453125\n",
            "Train[2/3][135/240] Loss: 0.6241905689239502, BER : 0.14759765625\n",
            "Train[2/3][136/240] Loss: 0.617684006690979, BER : 0.14661328125\n",
            "Train[2/3][137/240] Loss: 0.630562424659729, BER : 0.149890625\n",
            "Train[2/3][138/240] Loss: 0.6187775731086731, BER : 0.145625\n",
            "Train[2/3][139/240] Loss: 0.6282520294189453, BER : 0.1470390625\n",
            "Train[2/3][140/240] Loss: 0.6183695197105408, BER : 0.14457421875\n",
            "Train[2/3][141/240] Loss: 0.6347799301147461, BER : 0.1495625\n",
            "Train[2/3][142/240] Loss: 0.6238592267036438, BER : 0.14805859375\n",
            "Train[2/3][143/240] Loss: 0.6180326342582703, BER : 0.1461953125\n",
            "Train[2/3][144/240] Loss: 0.6180053353309631, BER : 0.14682421875\n",
            "Train[2/3][145/240] Loss: 0.6296992301940918, BER : 0.1498671875\n",
            "Train[2/3][146/240] Loss: 0.6183273196220398, BER : 0.1471015625\n",
            "Train[2/3][147/240] Loss: 0.624235212802887, BER : 0.14849609375\n",
            "Train[2/3][148/240] Loss: 0.6125707626342773, BER : 0.144703125\n",
            "Train[2/3][149/240] Loss: 0.6173006296157837, BER : 0.1435\n",
            "Train[2/3][150/240] Loss: 0.6100795865058899, BER : 0.142484375\n",
            "Train[2/3][151/240] Loss: 0.6167664527893066, BER : 0.14577734375\n",
            "Train[2/3][152/240] Loss: 0.6149803996086121, BER : 0.14414453125\n",
            "Train[2/3][153/240] Loss: 0.6256027221679688, BER : 0.14763671875\n",
            "Train[2/3][154/240] Loss: 0.6212424635887146, BER : 0.14814453125\n",
            "Train[2/3][155/240] Loss: 0.6195467710494995, BER : 0.14649609375\n",
            "Train[2/3][156/240] Loss: 0.6247719526290894, BER : 0.14971484375\n",
            "Train[2/3][157/240] Loss: 0.6090630292892456, BER : 0.1443359375\n",
            "Train[2/3][158/240] Loss: 0.6129724979400635, BER : 0.1451640625\n",
            "Train[2/3][159/240] Loss: 0.6255198121070862, BER : 0.14861328125\n",
            "Train[2/3][160/240] Loss: 0.612004280090332, BER : 0.1441484375\n",
            "Train[2/3][161/240] Loss: 0.6096737384796143, BER : 0.145453125\n",
            "Train[2/3][162/240] Loss: 0.617927074432373, BER : 0.146359375\n",
            "Train[2/3][163/240] Loss: 0.6118995547294617, BER : 0.14535546875\n",
            "Train[2/3][164/240] Loss: 0.6281228065490723, BER : 0.150234375\n",
            "Train[2/3][165/240] Loss: 0.6131327748298645, BER : 0.1440234375\n",
            "Train[2/3][166/240] Loss: 0.6128045916557312, BER : 0.14508203125\n",
            "Train[2/3][167/240] Loss: 0.6128578782081604, BER : 0.1438359375\n",
            "Train[2/3][168/240] Loss: 0.6214307546615601, BER : 0.14715625\n",
            "Train[2/3][169/240] Loss: 0.6174680590629578, BER : 0.1463046875\n",
            "Train[2/3][170/240] Loss: 0.6209730505943298, BER : 0.1485234375\n",
            "Train[2/3][171/240] Loss: 0.6185733079910278, BER : 0.14546875\n",
            "Train[2/3][172/240] Loss: 0.6156865358352661, BER : 0.14556640625\n",
            "Train[2/3][173/240] Loss: 0.6031649708747864, BER : 0.14210546875\n",
            "Train[2/3][174/240] Loss: 0.6087508201599121, BER : 0.1414296875\n",
            "Train[2/3][175/240] Loss: 0.6030214428901672, BER : 0.14083984375\n",
            "Train[2/3][176/240] Loss: 0.6096065044403076, BER : 0.1433125\n",
            "Train[2/3][177/240] Loss: 0.6099874973297119, BER : 0.14548828125\n",
            "Train[2/3][178/240] Loss: 0.6205195188522339, BER : 0.14894140625\n",
            "Train[2/3][179/240] Loss: 0.6056596040725708, BER : 0.14372265625\n",
            "Train[2/3][180/240] Loss: 0.6131983995437622, BER : 0.14612890625\n",
            "Train[2/3][181/240] Loss: 0.6204640865325928, BER : 0.148765625\n",
            "Train[2/3][182/240] Loss: 0.6242877840995789, BER : 0.14913671875\n",
            "Train[2/3][183/240] Loss: 0.6128520369529724, BER : 0.14625390625\n",
            "Train[2/3][184/240] Loss: 0.6192050576210022, BER : 0.1459765625\n",
            "Train[2/3][185/240] Loss: 0.610767126083374, BER : 0.1438203125\n",
            "Train[2/3][186/240] Loss: 0.6143031120300293, BER : 0.14616796875\n",
            "Train[2/3][187/240] Loss: 0.6171175837516785, BER : 0.1475859375\n",
            "Train[2/3][188/240] Loss: 0.6031110286712646, BER : 0.14406640625\n",
            "Train[2/3][189/240] Loss: 0.6160844564437866, BER : 0.14908984375\n",
            "Train[2/3][190/240] Loss: 0.6051923632621765, BER : 0.1432890625\n",
            "Train[2/3][191/240] Loss: 0.5999331474304199, BER : 0.14129296875\n",
            "Train[2/3][192/240] Loss: 0.6087344884872437, BER : 0.14536328125\n",
            "Train[2/3][193/240] Loss: 0.6136069297790527, BER : 0.14697265625\n",
            "Train[2/3][194/240] Loss: 0.6199606657028198, BER : 0.1484453125\n",
            "Train[2/3][195/240] Loss: 0.6091081500053406, BER : 0.14553125\n",
            "Train[2/3][196/240] Loss: 0.6175645589828491, BER : 0.14701953125\n",
            "Train[2/3][197/240] Loss: 0.6030665636062622, BER : 0.14116015625\n",
            "Train[2/3][198/240] Loss: 0.5967974066734314, BER : 0.14038671875\n",
            "Train[2/3][199/240] Loss: 0.5957447290420532, BER : 0.14020703125\n",
            "Train[2/3][200/240] Loss: 0.6078563928604126, BER : 0.14407421875\n",
            "Train[2/3][201/240] Loss: 0.6091474294662476, BER : 0.14602734375\n",
            "Train[2/3][202/240] Loss: 0.6114819645881653, BER : 0.14712109375\n",
            "Train[2/3][203/240] Loss: 0.6136980652809143, BER : 0.1458671875\n",
            "Train[2/3][204/240] Loss: 0.6018726825714111, BER : 0.1425390625\n",
            "Train[2/3][205/240] Loss: 0.6002039313316345, BER : 0.14235546875\n",
            "Train[2/3][206/240] Loss: 0.5985753536224365, BER : 0.14156640625\n",
            "Train[2/3][207/240] Loss: 0.60760098695755, BER : 0.14414453125\n",
            "Train[2/3][208/240] Loss: 0.6013367176055908, BER : 0.14282421875\n",
            "Train[2/3][209/240] Loss: 0.6005634069442749, BER : 0.14390234375\n",
            "Train[2/3][210/240] Loss: 0.6215934753417969, BER : 0.15096875\n",
            "Train[2/3][211/240] Loss: 0.594330906867981, BER : 0.13930859375\n",
            "Train[2/3][212/240] Loss: 0.6052289009094238, BER : 0.14351171875\n",
            "Train[2/3][213/240] Loss: 0.5981180667877197, BER : 0.14312890625\n",
            "Train[2/3][214/240] Loss: 0.6018329858779907, BER : 0.14359765625\n",
            "Train[2/3][215/240] Loss: 0.6045665144920349, BER : 0.145\n",
            "Train[2/3][216/240] Loss: 0.603987455368042, BER : 0.1437109375\n",
            "Train[2/3][217/240] Loss: 0.5945483446121216, BER : 0.14066796875\n",
            "Train[2/3][218/240] Loss: 0.6024172902107239, BER : 0.1434609375\n",
            "Train[2/3][219/240] Loss: 0.597026526927948, BER : 0.14294921875\n",
            "Train[2/3][220/240] Loss: 0.6033042669296265, BER : 0.14373828125\n",
            "Train[2/3][221/240] Loss: 0.5976191163063049, BER : 0.1414921875\n",
            "Train[2/3][222/240] Loss: 0.5906503200531006, BER : 0.14033203125\n",
            "Train[2/3][223/240] Loss: 0.5985492467880249, BER : 0.1437421875\n",
            "Train[2/3][224/240] Loss: 0.6024924516677856, BER : 0.14484375\n",
            "Train[2/3][225/240] Loss: 0.5967972278594971, BER : 0.1423515625\n",
            "Train[2/3][226/240] Loss: 0.5939041376113892, BER : 0.14258984375\n",
            "Train[2/3][227/240] Loss: 0.5999575257301331, BER : 0.14330859375\n",
            "Train[2/3][228/240] Loss: 0.5985572338104248, BER : 0.1443046875\n",
            "Train[2/3][229/240] Loss: 0.598366379737854, BER : 0.1426484375\n",
            "Train[2/3][230/240] Loss: 0.5839493274688721, BER : 0.13825390625\n",
            "Train[2/3][231/240] Loss: 0.5923851728439331, BER : 0.1419375\n",
            "Train[2/3][232/240] Loss: 0.5932609438896179, BER : 0.1421171875\n",
            "Train[2/3][233/240] Loss: 0.5905542373657227, BER : 0.14140625\n",
            "Train[2/3][234/240] Loss: 0.5898106098175049, BER : 0.14248046875\n",
            "Train[2/3][235/240] Loss: 0.5813236236572266, BER : 0.13757421875\n",
            "Train[2/3][236/240] Loss: 0.5794357061386108, BER : 0.13819140625\n",
            "Train[2/3][237/240] Loss: 0.5813997387886047, BER : 0.14046875\n",
            "Train[2/3][238/240] Loss: 0.5792759656906128, BER : 0.13781640625\n",
            "Train[2/3][239/240] Loss: 0.5842242240905762, BER : 0.14076171875\n",
            "Test[2/3][0/40]  Loss: 1.055401086807251, BER (test): 0.2214296875\n",
            "Test[2/3][1/40]  Loss: 1.0485260486602783, BER (test): 0.21975\n",
            "Test[2/3][2/40]  Loss: 1.0546913146972656, BER (test): 0.22060546875\n",
            "Test[2/3][3/40]  Loss: 1.0576937198638916, BER (test): 0.22253515625\n",
            "Test[2/3][4/40]  Loss: 1.0500015020370483, BER (test): 0.22049609375\n",
            "Test[2/3][5/40]  Loss: 1.0551010370254517, BER (test): 0.2204140625\n",
            "Test[2/3][6/40]  Loss: 1.0527641773223877, BER (test): 0.22028125\n",
            "Test[2/3][7/40]  Loss: 1.0573465824127197, BER (test): 0.21967578125\n",
            "Test[2/3][8/40]  Loss: 1.0511146783828735, BER (test): 0.22042578125\n",
            "Test[2/3][9/40]  Loss: 1.0516713857650757, BER (test): 0.22133984375\n",
            "Test[2/3][10/40]  Loss: 1.0547754764556885, BER (test): 0.2221484375\n",
            "Test[2/3][11/40]  Loss: 1.0494353771209717, BER (test): 0.2218125\n",
            "Test[2/3][12/40]  Loss: 1.054872751235962, BER (test): 0.222609375\n",
            "Test[2/3][13/40]  Loss: 1.0569677352905273, BER (test): 0.22067578125\n",
            "Test[2/3][14/40]  Loss: 1.0528593063354492, BER (test): 0.21966015625\n",
            "Test[2/3][15/40]  Loss: 1.0577651262283325, BER (test): 0.2219296875\n",
            "Test[2/3][16/40]  Loss: 1.0543038845062256, BER (test): 0.22028515625\n",
            "Test[2/3][17/40]  Loss: 1.0416322946548462, BER (test): 0.21758203125\n",
            "Test[2/3][18/40]  Loss: 1.0565600395202637, BER (test): 0.22175\n",
            "Test[2/3][19/40]  Loss: 1.0538229942321777, BER (test): 0.2204296875\n",
            "Test[2/3][20/40]  Loss: 1.0557903051376343, BER (test): 0.221625\n",
            "Test[2/3][21/40]  Loss: 1.0546931028366089, BER (test): 0.2209921875\n",
            "Test[2/3][22/40]  Loss: 1.0547205209732056, BER (test): 0.2208671875\n",
            "Test[2/3][23/40]  Loss: 1.0452271699905396, BER (test): 0.21869140625\n",
            "Test[2/3][24/40]  Loss: 1.0558642148971558, BER (test): 0.22222265625\n",
            "Test[2/3][25/40]  Loss: 1.0512909889221191, BER (test): 0.2227890625\n",
            "Test[2/3][26/40]  Loss: 1.0554749965667725, BER (test): 0.22128515625\n",
            "Test[2/3][27/40]  Loss: 1.0363821983337402, BER (test): 0.21737890625\n",
            "Test[2/3][28/40]  Loss: 1.0542000532150269, BER (test): 0.22065234375\n",
            "Test[2/3][29/40]  Loss: 1.0547406673431396, BER (test): 0.21997265625\n",
            "Test[2/3][30/40]  Loss: 1.0456886291503906, BER (test): 0.21987109375\n",
            "Test[2/3][31/40]  Loss: 1.047318696975708, BER (test): 0.21831640625\n",
            "Test[2/3][32/40]  Loss: 1.0541179180145264, BER (test): 0.219875\n",
            "Test[2/3][33/40]  Loss: 1.0482943058013916, BER (test): 0.2188046875\n",
            "Test[2/3][34/40]  Loss: 1.0572867393493652, BER (test): 0.22211328125\n",
            "Test[2/3][35/40]  Loss: 1.049699068069458, BER (test): 0.2213515625\n",
            "Test[2/3][36/40]  Loss: 1.046293020248413, BER (test): 0.2200546875\n",
            "Test[2/3][37/40]  Loss: 1.0555880069732666, BER (test): 0.22213671875\n",
            "Test[2/3][38/40]  Loss: 1.0515638589859009, BER (test): 0.22090625\n",
            "Test[2/3][39/40]  Loss: 1.0524494647979736, BER (test): 0.22126953125\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"Saved_Trained_Checkpoints/\"\n",
        "Output_Spikes = \"Output_Spikes/\"\n",
        "Enc_syn_Spikes = \"Enc_syn_Spikes/\"\n",
        "Intermediate_Lyrs = \"Intermediate_Lyrs/\"\n",
        "epoch_activations_list = [] # Create a list to store activations for each epoch\n",
        "epoch_activations = {}\n",
        "\n",
        "\n",
        "\n",
        "# # Run training and testing\n",
        "# for e in range(epochs):\n",
        "#     train_loss = train(net, train_loader, optimizer, e)\n",
        "#     train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "#     test_loss = test(net, test_loader, optimizer, e)\n",
        "#     test_avg_loss_rec.append(sum(test_loss_rec) / (len(test_loader)))\n",
        "\n",
        "\n",
        "#     #----------Save the model every 10 epochs\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#         model_path = checkpoint_path + f\"model_epoch_{e + 1}.pt\"\n",
        "#         torch.save(net.state_dict(), model_path)\n",
        "\n",
        "#     # # ---------------------------------------------- Add hooks for specific layers\n",
        "\n",
        "#     hook_layers = [net.encoder[2], net.encoder[5], net.encoder[8], net.encoder[11], net.decoder[1], net.decoder[4], net.decoder[7], net.decoder[9]]  # You can choose the layers you want to capture activations from\n",
        "#     hook_names = [\"Enc_Lk1\",\"Enc_syn1\", \"Enc_syn2\", \"Enc_Lk2\", \"Dec_Lk1\", \"Dec_syn1\", \"Dec_syn2\", \"Dec_Lk2\"]  # Names for the captured activations\n",
        "#     hooks = []\n",
        "#     for i, layer in enumerate(hook_layers):\n",
        "#       hook_fn = get_activation(hook_names[i])\n",
        "#       hooks.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "#     epoch_activations[e] = {}\n",
        "#     for i, name in enumerate(hook_names):\n",
        "#       epoch_activations[e][name] = activation[name]\n",
        "\n",
        "#     # Check if the current epoch is a multiple of 10\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#       # Save the epoch_activations dictionary to a file\n",
        "#       activations_path = Intermediate_Lyrs + f\"Intermediate_Lyrs_epoch_{e + 1}.pkl\"\n",
        "#       with open(activations_path, 'wb') as file:\n",
        "#         pickle.dump(epoch_activations, file)\n",
        "\n",
        "\n",
        "\n",
        "# ///////////////////////////////////\n",
        "\n",
        "# Define hook_layers and hook_names\n",
        "hook_layers = [net.encoder[2], net.encoder[5], net.encoder[8], net.encoder[11], net.decoder[1], net.decoder[4], net.decoder[7], net.decoder[9]]\n",
        "hook_names = [\"Enc_Lk1\", \"Enc_syn1\", \"Enc_syn2\", \"Enc_Lk2\", \"Dec_Lk1\", \"Dec_syn1\", \"Dec_syn2\", \"Dec_Lk2\"]\n",
        "\n",
        "# Create an empty dictionary to store activations\n",
        "epoch_activations = {}\n",
        "\n",
        "# Register hooks for capturing activations\n",
        "hooks = []\n",
        "for i, layer in enumerate(hook_layers):\n",
        "    hook_fn = get_activation(hook_names[i])\n",
        "    hooks.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "# Run training and testing\n",
        "for e in range(epochs):\n",
        "    train_loss = train(net, train_loader, optimizer, e)\n",
        "    train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "    test_loss = test(net, test_loader, optimizer, e)\n",
        "    test_avg_loss_rec.append(sum(test_loss_rec) / (len(test_loader)))\n",
        "\n",
        "    # Check if the current epoch is a multiple of 10\n",
        "    if (e + 1) % 2 == 0:\n",
        "        # Save the epoch_activations dictionary to a file\n",
        "        activations_path = Intermediate_Lyrs +  f\"activations_epoch_{e + 1}.pkl\"\n",
        "        with open(activations_path, 'wb') as file:\n",
        "            pickle.dump(epoch_activations, file)\n",
        "\n",
        "    # Capture activations for the current epoch\n",
        "    epoch_activations[e] = {}\n",
        "    for i, name in enumerate(hook_names):\n",
        "        epoch_activations[e][name] = activation.get(name, None)  # Use get to avoid KeyError\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     # After running this epoch, collect the activations in a dictionary\n",
        "#     epoch_activations = {}\n",
        "#     for i, name in enumerate(hook_names):\n",
        "#         epoch_activations[name] = activation[name]\n",
        "#     # Append the epoch's activations to the list\n",
        "#     epoch_activations_list.append(epoch_activations)\n",
        "\n",
        "# # Save the list of activations for all epochs to a file\n",
        "# with open('epoch_activations_list.pkl', 'wb') as file:\n",
        "#     pickle.dump(epoch_activations_list, file)\n",
        "\n",
        "\n",
        "#     #----------Save the model every 10 epochs\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#         # Save the intermediate activations for every 10 epochs\n",
        "#         activations_path = checkpoint_path + f\"activations_epoch_{e + 1}.pth\"\n",
        "#         torch.save(epoch_activations, activations_path)\n",
        "#         # Save the activations in a pickle file if needed\n",
        "#         pickle_path = checkpoint_path + f\"activations_epoch_{e + 1}.pkl\"\n",
        "#         with open(pickle_path, 'wb') as file:\n",
        "#           pickle.dump(epoch_activations, file)\n",
        "#     #-----------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # #---------------------------------------------------------- Access the out_en tensor\n",
        "    out_en = net.out_en\n",
        "    out_en_numpy = out_en.cpu().detach().numpy()\n",
        "    # Save with a different name for each epoch\n",
        "    out_en_filename = Output_Spikes + f\"out_en_epoch_{e + 1}.npy\"\n",
        "    np.save(out_en_filename, out_en_numpy)\n",
        "\n",
        "\n",
        "    # #-----------------------------------------------------------Access the out tensor\n",
        "    out = net.out\n",
        "    out_numpy = out.cpu().detach().numpy()\n",
        "\n",
        "    # Save with a different name for each epoch\n",
        "    out_filename = Output_Spikes + f\"out_epoch_{e + 1}.npy\"\n",
        "    np.save(out_filename, out_numpy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# beta_syn=0.9\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To load the saved data"
      ],
      "metadata": {
        "id": "3r8IS1WVqDdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print available epoch numbers\n",
        "available_epochs = list(epoch_activations.keys())\n",
        "print(\"Available Epochs:\", available_epochs)\n",
        "\n",
        "# Choose a valid epoch number from the list\n",
        "valid_epoch = 2  # Replace with a valid epoch number from the list of available_epochs\n",
        "\n",
        "# Load activations for the chosen epoch\n",
        "activations_path = Intermediate_Lyrs + f\"activations_epoch_{valid_epoch}.pkl\"\n",
        "with open(activations_path, 'rb') as file:\n",
        "    epoch_activations = pickle.load(file)\n",
        "\n",
        "# Access the activations for Enc_syn1\n",
        "Enc_syn1_activations = epoch_activations[valid_epoch][\"Enc_syn1\"]\n"
      ],
      "metadata": {
        "id": "Uux-gnqAqG8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gcl3d4RtyMRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming activation[\"Enc_syn1\"] is of shape (250, 64, 8, 8)\n",
        "data = activation[\"Enc_syn1\"][:, 0, :, :].cpu().numpy()  # Transfer to CPU and convert to NumPy\n",
        "\n",
        "# Create meshgrid for x and y dimensions\n",
        "x, y = np.meshgrid(np.arange(data.shape[1]), np.arange(data.shape[0]))\n",
        "\n",
        "# Create a figure and a 3D axis\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot the 3D surface\n",
        "ax.plot_surface(x, y, data, cmap='viridis')\n",
        "\n",
        "# Set axis labels\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Activation')\n",
        "\n",
        "# Show the 3D plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "bQvnihqXys4P",
        "outputId": "3d3e4df3-c983-4086-9ea7-c356daa93eb8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## to access the saved files"
      ],
      "metadata": {
        "id": "nyjYtMx3p1r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access activations for a specific epoch and layer\n",
        "epoch_number = 4  # Replace with the epoch number you're interested in\n",
        "layer_name = \"Enc_syn1\"  # Replace with the layer name you're interested in\n",
        "\n",
        "# Access the activations for the specified epoch and layer\n",
        "activations = epoch_activations[epoch_number][layer_name]\n",
        "\n",
        "# Now, you can use the 'activations' variable for analysi"
      ],
      "metadata": {
        "id": "vdHk7pO9p0pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_path = \"Saved_Trained_Checkpoints/\"\n",
        "# Output_Spikes = \"Output_Spikes/\"\n",
        "# Enc_syn_Spikes = \"Enc_syn_Spikes/\"\n",
        "\n",
        "# # Run training and testing\n",
        "# for e in range(epochs):\n",
        "#     train_loss = train(net, train_loader, optimizer, e)\n",
        "#     train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "#     test_loss = test(net, test_loader, optimizer, e)\n",
        "#     test_avg_loss_rec.append(sum(test_loss_rec) / (len(test_loader)))\n",
        "\n",
        "#     # Save the model every 10 epochs\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#         model_path = checkpoint_path + f\"model_epoch_{e + 1}.pt\"\n",
        "#         torch.save(net.state_dict(), model_path)\n",
        "\n",
        "\n",
        "#     # Add hooks for specific layers\n",
        "#     hook_layers = [net.encoder[5], net.encoder[8]]  # You can choose the layers you want to capture activations from\n",
        "#     hook_names = [\"Enc_syn1\", \"Enc_syn2\"]  # Names for the captured activations\n",
        "#     hooks = []\n",
        "#     for i, layer in enumerate(hook_layers):\n",
        "#       hook_fn = get_activation(hook_names[i])\n",
        "#       hooks.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "\n",
        "activation[\"Enc_syn1\"]\n",
        "\n",
        "print(activation[\"Enc_Lk1\"].size())\n",
        "print(activation[\"Enc_syn1\"].size())\n",
        "print(activation[\"Enc_syn2\"].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y4DleYOlf2L",
        "outputId": "e43db961-b696-45fb-90c4-5a8cd4fbba5d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([250, 32, 16, 16])\n",
            "torch.Size([250, 64, 8, 8])\n",
            "torch.Size([250, 128, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjvlZfSt4T8L"
      },
      "outputs": [],
      "source": [
        "[[=]]\n",
        "# # Set the path for saving checkpoints\n",
        "# checkpoint_path = \"Saved_Trained_Checkpoints/\"\n",
        "\n",
        "#    #Run training and testing\n",
        "# for e in range(epochs):\n",
        "#   train_loss = train(net, train_loader, optimizer, e)\n",
        "#   train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "\n",
        "#   test_loss = test(net,test_loader, optimizer, e)\n",
        "#   test_avg_loss_rec.append(sum(test_loss_rec)/(len(test_loader)))\n",
        "\n",
        "# # /////////////////////////////////////////////////////////////////////\n",
        "#       # Save the model every 10 epochs\n",
        "#   if (e + 1) % 10 == 0:\n",
        "#     model_path = checkpoint_path + f\"model_epoch_{e + 1}.pt\"\n",
        "#     torch.save(net.state_dict(), model_path)\n",
        "\n",
        "# # After training\n",
        "# # Access the out_en tensor\n",
        "# out_en = net.out_en\n",
        "# out_en_numpy = out_en.cpu().detach().numpy()\n",
        "# np.save(\"out_en.npy\", out_en_numpy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YCKZr773q_b"
      },
      "outputs": [],
      "source": [
        "# spike_data = np.load(\"out_en.npy\")\n",
        "\n",
        "# # cmap = plt.get_cmap(\"binary\")\n",
        "\n",
        "# plt.imshow(spike_data, aspect=\"auto\")\n",
        "# # plt.xlabel(\"Time Steps\")\n",
        "# # plt.ylabel(\"Neurons\")\n",
        "# plt.title(\"Spike Activity in out_en\")\n",
        "# # plt.colorbar(label=\"Spikes\")\n",
        "\n",
        "# filename = f\"(out_en)_spike_activity_epoch_{epochs}_b_syn_{beta_syn}.pdf\"\n",
        "# plt.savefig(filename, format=\"pdf\")\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p6wqnnBH4OH"
      },
      "outputs": [],
      "source": [
        "# spike_data = np.load(\"out_en.npy\")\n",
        "\n",
        "# cmap = plt.get_cmap(\"binary\")\n",
        "# plt.imshow(spike_data, cmap=cmap, aspect=\"auto\")\n",
        "# # plt.xlabel(\"Time Steps\")\n",
        "# # plt.ylabel(\"Neurons\")\n",
        "# plt.title(\"Spike Activity in out_en\")\n",
        "# # plt.colorbar(label=\"Spikes\")\n",
        "\n",
        "# filename = f\"(out_en)_spike_activity_epoch_{epochs}_b_syn_{beta_syn}_binary.pdf\"\n",
        "# plt.savefig(filename, format=\"pdf\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the npy files"
      ],
      "metadata": {
        "id": "Q6L-Y5IU83Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "ax2_epoch = 50\n",
        "spike_data_bench = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "ax1_epoch = 1\n",
        "spike_data1_0001 = np.load(f\"out_en_epoch_{ax1_epoch}.npy\")\n",
        "ax2_epoch = 50\n",
        "spike_data_over = np.load(f\"out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mHP4qGr_82my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anaylizing the spike rates"
      ],
      "metadata": {
        "id": "IFSePS4l_IYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_rel\n",
        "spike_rate1=spike_data_bench\n",
        "spike_rate2=spike_data_over\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# Create histograms\n",
        "ax1.hist(spike_rate1, bins=20, label='Epoch 50', alpha=0.75)\n",
        "ax1.set_xlabel('Spike Rate')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('Distribution of Spike Rates (benchmark)')\n",
        "# ax1.legend(\"benchmark\")\n",
        "ax2.hist(spike_rate2, bins=20, label='Epoch 50_0001', alpha=0.75)\n",
        "ax2.set_xlabel('Spike Rate')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('Distribution of Spike Rates (over-firing)')\n",
        "# ax2.legend(\"over-firing\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "nDtmu8zx_MAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Correlation over-firing and benchmark"
      ],
      "metadata": {
        "id": "peeUdW-1DlPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ax1_epoch = 1\n",
        "# spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "# ax2_epoch = 50\n",
        "# spike_data2 = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "spike_rate1=spike_data_bench\n",
        "spike_rate2=spike_data_over\n",
        "\n",
        "\n",
        "# Calculate cross-correlation between corresponding neurons in two epochs\n",
        "cross_correlations = []\n",
        "for i in range(spike_rate1.shape[1]):  # Assuming spike_data1 and spike_data2 have the same number of neurons\n",
        "    cross_corr = correlate(spike_rate1[:, i], spike_rate2[:, i], mode='same')\n",
        "    cross_correlations.append(cross_corr)\n",
        "\n",
        "# Plot cross-correlations for a few neurons (e.g., the first 5)\n",
        "for i in range(5):\n",
        "    plt.plot(cross_correlations[i], label=f'Timestep {i + 1}')\n",
        "\n",
        "plt.xlabel('Latent dim')\n",
        "plt.ylabel('Cross-Correlation')\n",
        "plt.legend()\n",
        "plt.title('Cross-Correlation Between Timesteps (benchmark) vs (overfiring)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BaJPD68bHLAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Calculate cross-correlation between corresponding neurons in two epochs\n",
        "cross_correlations = []\n",
        "for i in range(spike_rate1.shape[1]):  # Assuming spike_rate1 and spike_rate2 have the same number of neurons\n",
        "    cross_corr = correlate(spike_rate1[:, i], spike_rate2[:, i], mode='same')\n",
        "\n",
        "    # Normalize the cross-correlation\n",
        "    cross_corr /= np.sqrt(np.sum(spike_rate1[:, i] ** 2) * np.sum(spike_rate2[:, i] ** 2))\n",
        "\n",
        "    cross_correlations.append(cross_corr)\n",
        "\n",
        "# # Plot normalized cross-correlations for a few timestep (e.g., the first 5)\n",
        "for i in range(5):\n",
        "    plt.plot(cross_correlations[i], label=f'Timestep {i + 1}')\n",
        "\n",
        "\n",
        "# plt.plot(cross_correlations[4], label=f'Timestep {4 + 1}')\n",
        "\n",
        "plt.xlabel('Latent dim')\n",
        "plt.ylabel('Normalized Cross-Correlation')\n",
        "plt.legend()\n",
        "plt.title('Normalized Cross-Correlation Between Timesteps (benchmark) vs (overfiring)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9rJSOHgmZd0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming spike_data_bench and spike_data_over are loaded as numpy arrays with shape (32, 5)\n",
        "\n",
        "# Calculate the cross-correlation\n",
        "cross_corr = correlate(spike_data_bench, spike_data_over, mode='same')\n",
        "\n",
        "# Calculate the denominators for each row separately\n",
        "denominator = np.sqrt(np.sum(spike_data_bench ** 2, axis=1, keepdims=True) * np.sum(spike_data_over ** 2, axis=1, keepdims=True))\n",
        "\n",
        "# Avoid division by zero by setting denominator to 1 where it's zero\n",
        "denominator[denominator == 0] = 1\n",
        "\n",
        "# Normalize the cross-correlation to [-1, 1]\n",
        "cross_corr /= denominator\n",
        "\n",
        "# Calculate time lags for plotting\n",
        "time_lags = np.arange(-2, 3)  # Assuming 5 time steps in your data\n",
        "\n",
        "# Plot the cross-correlation\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(len(cross_corr)):\n",
        "    plt.plot(time_lags, cross_corr[i], label=f'Latent Dim {i + 1}')\n",
        "\n",
        "plt.xlabel('Time Lag')\n",
        "plt.ylabel('Normalized Cross-Correlation')\n",
        "plt.title('Cross-Correlation Between spike_data_bench and spike_data_over')\n",
        "# plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# - The normalized cross-correlation ranges from -1 to 1.\n",
        "# - A value of 1 indicates perfect positive correlation (identical patterns).\n",
        "# - A value of -1 indicates perfect negative correlation (completely opposite patterns).\n",
        "# - A value around 0 indicates no significant correlation (random or dissimilar patterns).\n",
        "\n",
        "# You can print or analyze the 'cross_corr' array to see how correlated the spike patterns are for each latent dimension.\n"
      ],
      "metadata": {
        "id": "ks8ZcC3Zdj2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ax1_epoch = 1\n",
        "# spike_data1_0001 = np.load(f\"out_en_epoch_{ax1_epoch}.npy\")\n",
        "# ax2_epoch = 50\n",
        "# spike_data2_0001 = np.load(f\"out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "# cross_correlations = []\n",
        "# for i in range(spike_data1_0001.shape[1]):\n",
        "#     cross_corr = correlate(spike_data1_0001[:, i], spike_data2_0001[:, i], mode='same')\n",
        "#     cross_correlations.append(cross_corr)\n",
        "\n",
        "# for i in range(5):\n",
        "#     plt.plot(cross_correlations[i], label=f'Timestep {i + 1}')\n",
        "\n",
        "# plt.xlabel('Latent dim')\n",
        "# plt.ylabel('Cross-Correlation')\n",
        "# plt.legend()\n",
        "# plt.title('Cross-Correlation Between Timesteps (overfiring)')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "8l_pQFiVHtDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Theory"
      ],
      "metadata": {
        "id": "wHhK-1jAa64_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine how much information is gained or lost when transitioning from spike_data1 to spike_data2.  You should analyze how spike patterns change from from spike_data1 to spike_data2 and calculate the information gain.\n",
        "# please provide me the code based on each instruction below, wisely:\n",
        "# 1. Data Normalization:\n",
        "# Ensure that both spike_data1 and spike_data2 have the same scale or normalization. You may need to rescale or normalize the data so that they are directly comparable.\n",
        "# 2. Data Discretization:\n",
        "# Convert the spike data into discrete probability distributions. You can do this by discretizing the data into time bins and assigning probabilities to each bin based on spike counts.\n",
        "# 3. Calculate Probability Distributions:\n",
        "# Calculate probability distributions for both spike_data1 and spike_data2. These distributions represent the likelihood of observing spikes in each time bin.\n",
        "# 4. Compute KL Divergence:\n",
        "# Use the KL divergence formula to calculate the divergence between the two distributions.\n",
        "# 5. Summation:\n",
        "# Sum up the KL divergence values across all time steps and latent dimensions to obtain a single value representing the total information gain or loss when transitioning from spike_data1 to spike_data2.\n",
        "\n",
        "spike_rate1=spike_data_bench\n",
        "spike_rate2=spike_data_over\n",
        "\n",
        "# # Step 1: Data Normalization\n",
        "# spike_data1_normalized = spike_rate1 / np.max(spike_rate1)\n",
        "# spike_data2_normalized = spike_rate2 / np.max(spike_rate2)\n",
        "\n",
        "# Step 2: Data Discretization\n",
        "num_bins = 10\n",
        "spike_data1_discretized = np.histogramdd(np.where(spike_rate1 > 0), bins=num_bins)[0]\n",
        "spike_data2_discretized = np.histogramdd(np.where(spike_rate2 > 0), bins=num_bins)[0]\n",
        "\n",
        "# Step 3: Calculate Probability Distributions\n",
        "epsilon = 1e-10  # Small constant to avoid division by zero\n",
        "spike_data1_prob = (spike_data1_discretized + epsilon) / np.sum(spike_data1_discretized + epsilon)\n",
        "spike_data2_prob = (spike_data2_discretized + epsilon) / np.sum(spike_data2_discretized + epsilon)\n",
        "\n",
        "# Step 4: Compute KL Divergence\n",
        "kl_divergence = kl_div(spike_data2_prob.flatten(), spike_data1_prob.flatten())\n",
        "print(\"kl_divergence:\", kl_divergence.shape)\n",
        "\n",
        "# Step 5: Summation\n",
        "total_information_gain = np.sum(kl_divergence)\n",
        "\n",
        "print(\"Total Information Gain:\", total_information_gain)\n"
      ],
      "metadata": {
        "id": "zMPPO4Ija5V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Plot probability densities\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(spike_data1_prob, cmap='viridis', aspect='auto', origin='lower')\n",
        "plt.colorbar(label='Probability Density')\n",
        "plt.xlabel(\"Latent Dimension\")\n",
        "plt.ylabel(\"Time Step\")\n",
        "plt.title(\"Probability Density - benchmark\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(spike_data2_prob, cmap='viridis', aspect='auto', origin='lower')\n",
        "plt.colorbar(label='Probability Density')\n",
        "plt.xlabel(\"Latent Dimension\")\n",
        "plt.ylabel(\"Time Step\")\n",
        "plt.title(\"Probability Density - overfiring\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YdeVP2VBYnNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of bins for the histograms\n",
        "num_bins = 20\n",
        "\n",
        "# Create a figure with 5 subplots (one for each time step)\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "# Iterate through time steps\n",
        "for i in range(5):\n",
        "    # Create a histogram for spike_data_bench at the current time step\n",
        "    axes[i].hist(spike_data_bench[:, i], bins=num_bins, density=True, alpha=0.5, label='Benchmark')\n",
        "\n",
        "    # Create a histogram for spike_data2 at the current time step\n",
        "    axes[i].hist(spike_data_over[:, i], bins=num_bins, density=True, alpha=0.5, label='Over-firing')\n",
        "\n",
        "    # Set labels and title for the subplot\n",
        "    axes[i].set_xlabel(\"Latent Dimension\")\n",
        "    axes[i].set_ylabel(\"Probability Density\")\n",
        "    axes[i].set_title(f\"Time Step {i}\")\n",
        "    axes[i].legend()\n",
        "\n",
        "# Add a title for the entire figure\n",
        "plt.suptitle(\"Probability Density of Spike Patterns for Different Time Steps\")\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.85)  # Adjust the position of the main title\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# spike_rate1=spike_data_bench\n",
        "# spike_rate2=spike_data_over\n",
        "\n",
        "# # Define the number of bins for the histograms\n",
        "# num_bins = 10\n",
        "\n",
        "# # Create histograms for spike_data1\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for i in range(5):  # 5 time steps\n",
        "#     plt.subplot(2, 5, i + 1)\n",
        "#     plt.hist(spike_rate1[:, i], bins=num_bins, density=True, alpha=0.5)\n",
        "#     plt.title(f\"Benchmark, Time Step {i}\")\n",
        "#     plt.xlabel(\"Spikes (0 or 1)\")\n",
        "#     plt.ylabel(\"Probability Density\")\n",
        "\n",
        "# # Create histograms for spike_data2\n",
        "# for i in range(5):  # 5 time steps\n",
        "#     plt.subplot(2, 5, i + 6)\n",
        "#     plt.hist(spike_rate2[:, i], bins=num_bins, density=True, alpha=0.5)\n",
        "#     plt.title(f\"Over-firing, Time Step {i}\")\n",
        "#     plt.xlabel(\"Spikes (0 or 1)\")\n",
        "#     plt.ylabel(\"Probability Density\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "Jy48gHX2Dkhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an array to store KL Divergence values for each time step\n",
        "kl_divergence_per_step = []\n",
        "\n",
        "# Iterate through time steps\n",
        "for i in range(5):\n",
        "    # Calculate probability distributions for the two datasets at the current time step\n",
        "    prob_dist_bench, _ = np.histogram(spike_data_bench[:, i], bins=num_bins, density=True)\n",
        "    prob_dist_over, _ = np.histogram(spike_data_over[:, i], bins=num_bins, density=True)\n",
        "\n",
        "    # Ensure that the two distributions have the same length\n",
        "    min_len = min(len(prob_dist_bench), len(prob_dist_over))\n",
        "    prob_dist_bench = prob_dist_bench[:min_len]\n",
        "    prob_dist_over = prob_dist_over[:min_len]\n",
        "\n",
        "    # Calculate KL Divergence for the current time step\n",
        "    kl_divergence = entropy(prob_dist_bench, prob_dist_over)\n",
        "\n",
        "    kl_divergence_per_step.append(kl_divergence)\n",
        "\n",
        "# Plot the KL Divergence values for each time step\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(5), kl_divergence_per_step, marker='o')\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"KL Divergence\")\n",
        "plt.title(\"KL Divergence for Each Time Step\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print KL Divergence values\n",
        "for i, kl_divergence in enumerate(kl_divergence_per_step):\n",
        "    print(f\"Time Step {i}: KL Divergence = {kl_divergence}\")\n",
        "\n",
        "# ------------------------------ information gain or loss\n",
        "# Flatten the kl_divergence_per_step list to include all time steps\n",
        "kl_divergence_flat = np.array(kl_divergence_per_step)\n",
        "\n",
        "# Sum up the KL divergence values across all time steps and latent dimensions\n",
        "total_kl_divergence = np.sum(kl_divergence_flat)\n",
        "\n",
        "# Print the total KL Divergence\n",
        "print(f\"Total KL Divergence: {total_kl_divergence}\")\n"
      ],
      "metadata": {
        "id": "4YGd17N-UJi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # 2. Probability Distributions\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# for i in range(4):\n",
        "#     plt.subplot(2, 2, i + 1)\n",
        "#     sns.histplot(spike_data1[i], bins=20, label=\"Benchmark\", color='blue', kde=True)\n",
        "#     sns.histplot(spike_data2[i], bins=20, label=\"Over-firing\", color='red', kde=True)\n",
        "#     plt.xlabel(f\"Latent Dimension {i}\")\n",
        "#     plt.ylabel(\"Probability Density\")\n",
        "#     plt.legend()\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # 3. Heatmap\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.heatmap(kl_divergence, cmap=\"YlGnBu\", annot=True, fmt=\".3f\")\n",
        "# plt.xlabel(\"Latent Dimension\")\n",
        "# plt.ylabel(\"Time Step\")\n",
        "# plt.title(\"KL Divergence Heatmap\")\n",
        "# plt.show()\n",
        "\n",
        "# # 4. Time Series Plot\n",
        "# time_steps = np.arange(4)\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.plot(time_steps, kl_divergence.mean(axis=1), marker='o')\n",
        "# plt.xlabel(\"Time Step\")\n",
        "# plt.ylabel(\"Mean KL Divergence\")\n",
        "# plt.title(\"KL Divergence Over Time\")\n",
        "# plt.xticks(time_steps)\n",
        "# plt.show()\n",
        "\n",
        "# # 5. Latent Dimension Comparison\n",
        "# latent_dim_to_compare = 0  # Choose a specific latent dimension to compare\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# sns.histplot(spike_data1[:, latent_dim_to_compare], label=\"Benchmark\", color='blue', kde=True)\n",
        "# sns.histplot(spike_data2[:, latent_dim_to_compare], label=\"Over-firing\", color='red', kde=True)\n",
        "# plt.xlabel(f\"Latent Dimension {latent_dim_to_compare}\")\n",
        "# plt.ylabel(\"Probability Density\")\n",
        "# plt.legend()\n",
        "# plt.title(f\"Latent Dimension {latent_dim_to_compare} Comparison\")\n",
        "# plt.show()\n",
        "\n",
        "# # 7. Summary Statistics\n",
        "# mean_kl_divergence = kl_divergence.mean()\n",
        "# median_kl_divergence = np.median(kl_divergence)\n",
        "# std_kl_divergence = kl_divergence.std()\n",
        "# print(f\"Mean KL Divergence: {mean_kl_divergence:.3f}\")\n",
        "# print(f\"Median KL Divergence: {median_kl_divergence:.3f}\")\n",
        "# print(f\"Standard Deviation of KL Divergence: {std_kl_divergence:.3f}\")\n",
        "\n",
        "# # 8. Anomaly Detection Plot (set a threshold)\n",
        "# threshold = 0.1  # Adjust the threshold as needed\n",
        "# anomalies = np.where(kl_divergence > threshold, 1, 0)\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.heatmap(anomalies, cmap=\"Reds\", annot=True, fmt=\".0f\")\n",
        "# plt.xlabel(\"Latent Dimension\")\n",
        "# plt.ylabel(\"Time Step\")\n",
        "# plt.title(f\"Anomalies (Threshold = {threshold})\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "54vwHGfJr4qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpL7zyJXQvyE"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Encoder plotting each epochs output of out_en:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6JBtVnNX5GU"
      },
      "source": [
        "## Heatmap or Cmap plot of the Endoer spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ7gLK-kIOeu"
      },
      "outputs": [],
      "source": [
        "# ax1_epoch = 1\n",
        "# spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "# ax2_epoch = 50\n",
        "# spike_data2 = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "ax1_epoch = 50\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "cmap = plt.get_cmap(\"binary\")\n",
        "ax1.imshow(spike_data1, cmap=cmap, aspect=\"auto\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "cmap = plt.get_cmap(\"binary\")\n",
        "ax2.imshow(spike_data2, cmap=cmap, aspect=\"auto\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWyA4kYyX1CQ"
      },
      "source": [
        "## Raster plot of the Encoder spikes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax1_epoch = 50\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "# Find the coordinates of spike events\n",
        "coords = np.argwhere(spike_data1)\n",
        "ax1.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "coords = np.argwhere(spike_data2)\n",
        "ax2.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KdTScRM4EQqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqPaeRmVJvJo"
      },
      "outputs": [],
      "source": [
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "# Find the coordinates of spike events\n",
        "coords = np.argwhere(spike_data1)\n",
        "ax1.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "coords = np.argwhere(spike_data2)\n",
        "ax2.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"Time Steps\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"Latent dim\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out_en\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxp-zd_SXux4"
      },
      "source": [
        "## 3D plot of the Encoder sppikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIY7x0Z9WutH"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_en_epoch_{ax1_epoch}.npy\")\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_en_epoch_{ax2_epoch}.npy\")\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create the first subplot for spike_data1\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "x1, y1 = np.meshgrid(np.arange(spike_data1.shape[1]), np.arange(spike_data1.shape[0]))\n",
        "z1 = spike_data1.flatten()\n",
        "ax1.scatter(x1.flatten(), y1.flatten(), z1, s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlabel(\"Time Steps\")\n",
        "ax1.set_ylabel(\"Latent dim\")\n",
        "ax1.set_zlabel(\"Spike Intensity\", labelpad=2)\n",
        "ax1.set_title(f\"Spike Activity in out_en (Epoch {ax1_epoch})\")\n",
        "\n",
        "# Create the second subplot for spike_data2\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "x2, y2 = np.meshgrid(np.arange(spike_data2.shape[1]), np.arange(spike_data2.shape[0]))\n",
        "z2 = spike_data2.flatten()\n",
        "ax2.scatter(x2.flatten(), y2.flatten(), z2, s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlabel(\"Time Steps\")\n",
        "ax2.set_ylabel(\"Latent dim\")\n",
        "ax2.set_zlabel(\"Spike Intensity\", labelpad=0.1)\n",
        "ax2.set_title(f\"Spike Activity in out_en (Epoch {ax2_epoch})\")\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "# Manually adjust the position of the second subplot\n",
        "# pos = ax2.get_position()\n",
        "# pos.x0 += 0.1  # Adjust the left position\n",
        "# pos.x1 += 0.1  # Adjust the right position\n",
        "# ax2.set_position(pos)\n",
        "\n",
        "# Save and display the plot\n",
        "filename = f\"Enc_b_syn_{beta_syn}_spike_epochs_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_3D.pdf\"\n",
        "plt.savefig(filename, format=\"pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G40wCzd5tqd0"
      },
      "source": [
        "# Decoder plotting each epochs output of out:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-c7Bkw7Zp7U"
      },
      "source": [
        "## cmap plotting Decoder of (for one digit at the last time t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXZ80BPRSgoT"
      },
      "outputs": [],
      "source": [
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_epoch_{ax1_epoch}.npy\")\n",
        "spike_data1 = spike_data1[0,0,:, :]\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_epoch_{ax2_epoch}.npy\")\n",
        "spike_data2 = spike_data2[0,0,:, :]\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "cmap = plt.get_cmap(\"binary\")\n",
        "ax1.imshow(spike_data1, cmap=cmap, aspect=\"auto\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"width\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"length\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out (for one digit at the last time t)\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "cmap = plt.get_cmap(\"binary\")\n",
        "ax2.imshow(spike_data2, cmap=cmap, aspect=\"auto\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"width\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"length\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out (for one digit at the last time t)\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Dec_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBZS5QZ7Z0Kc"
      },
      "source": [
        "## Raster plot Decoder of (for one digit at the last time t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV9x0uMSTqJm"
      },
      "outputs": [],
      "source": [
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_epoch_{ax1_epoch}.npy\")\n",
        "spike_data1 = spike_data1[0,0,:, :]\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_epoch_{ax2_epoch}.npy\")\n",
        "spike_data2 = spike_data2[0,0,:, :]\n",
        "\n",
        "font_size = 15\n",
        "font = {'weight': 'bold', 'size': 15}\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 12))\n",
        "\n",
        "# -------------------------------------------------------------------------------------- ax1\n",
        "coords = np.argwhere(spike_data1)\n",
        "ax1.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlim(0, spike_data1.shape[1])\n",
        "ax1.set_ylim(0, spike_data1.shape[0])\n",
        "# Label the axes\n",
        "ax1.set_xlabel(\"width\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"length\", fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(\"Spike Activity in out (for one digit at the last time t)\" + \" [epoch: \" + str(ax1_epoch) + \"]\")\n",
        "# ax1.legend([\"epoch: \" + str(ax1_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# -------------------------------------------------------------------------------------- ax2\n",
        "coords = np.argwhere(spike_data2)\n",
        "ax2.scatter(coords[:, 1], coords[:, 0], s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlim(0, spike_data2.shape[1])\n",
        "ax2.set_ylim(0, spike_data2.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax2.set_xlabel(\"width\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"length\", fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(\"Spike Activity in out (for one digit at the last time t)\" + \" [epoch: \" + str(ax2_epoch) + \"]\")\n",
        "# ax2.legend([\"epoch: \" + str(ax2_epoch)], frameon=True, fancybox=True, shadow=True, prop={'size': 10})\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Save and display the plot\n",
        "filename1 = f\"Dec_b_syn_{beta_syn}_spike_epoch_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_raster.pdf\"\n",
        "fig.savefig(filename1, format=\"pdf\")\n",
        "\n",
        "# filename2 = f\"Enc_b_syn_{beta_syn}_spike_epoch_{ax2_epoch}_of_{epochs}_cmap.pdf\"\n",
        "# fig.savefig(filename2, format=\"pdf\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f6gFpA1Z5Nu"
      },
      "source": [
        "## 3D plotting Decoder of (for one digit at the last time t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSbkKPrvZ7my"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "ax1_epoch = 1\n",
        "spike_data1 = np.load(f\"Output_Spikes/out_epoch_{ax1_epoch}.npy\")\n",
        "spike_data1 = spike_data1[0,0,:, :]\n",
        "\n",
        "ax2_epoch = 50\n",
        "spike_data2 = np.load(f\"Output_Spikes/out_epoch_{ax2_epoch}.npy\")\n",
        "spike_data2 = spike_data2[0,0,:, :]\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create the first subplot for spike_data1\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "x1, y1 = np.meshgrid(np.arange(spike_data1.shape[1]), np.arange(spike_data1.shape[0]))\n",
        "z1 = spike_data1.flatten()\n",
        "ax1.scatter(x1.flatten(), y1.flatten(), z1, s=60, marker=\"|\", color=\"black\")\n",
        "ax1.set_xlabel(\"width\")#, fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_ylabel(\"length\")#, fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_zlabel(\"Spike Intensity\", labelpad=2)#, fontweight=\"bold\", fontdict=font)\n",
        "ax1.set_title(f\"Spike Activity in out_en (Epoch {ax1_epoch})\")\n",
        "\n",
        "# Create the second subplot for spike_data2\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "x2, y2 = np.meshgrid(np.arange(spike_data2.shape[1]), np.arange(spike_data2.shape[0]))\n",
        "z2 = spike_data2.flatten()\n",
        "ax2.scatter(x2.flatten(), y2.flatten(), z2, s=60, marker=\"|\", color=\"black\")\n",
        "ax2.set_xlabel(\"width\")#, fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_ylabel(\"length\")#, fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_zlabel(\"Spike Intensity\", labelpad=0.1)#, fontweight=\"bold\", fontdict=font)\n",
        "ax2.set_title(f\"Spike Activity in out_en (Epoch {ax2_epoch})\")\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "# Manually adjust the position of the second subplot\n",
        "# pos = ax2.get_position()\n",
        "# pos.x0 += 0.1  # Adjust the left position\n",
        "# pos.x1 += 0.1  # Adjust the right position\n",
        "# ax2.set_position(pos)\n",
        "\n",
        "# Save and display the plot\n",
        "filename = f\"Dec_b_syn_{beta_syn}_spike_epochs_{ax1_epoch}_VS_{ax2_epoch}_of_{epochs}_3D.pdf\"\n",
        "plt.savefig(filename, format=\"pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwXV7LJv5pOk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the 'out' tensor for a specific epoch\n",
        "epoch_number = 50  # Change this to the epoch you want to visualize\n",
        "out_filename = f\"out_epoch_{epoch_number}.npy\"\n",
        "out = np.load(out_filename)\n",
        "print(\"out:\",out.shape)  # Using .shape attribute\n",
        "\n",
        "# Reshape the data if needed\n",
        "# For example, if 'out' is a 4D tensor and you want to visualize a specific slice\n",
        "# along one of the dimensions, you can reshape it to a 2D matrix for plotting.\n",
        "\n",
        "# Example reshape (adjust dimensions as needed)\n",
        "out = out.reshape(-1, out.shape[-1])  # Flatten all dimensions except the last one\n",
        "print(\"out.reshape(-1, out.shape[-1]): \",out.shape)  # Using .shape attribute\n",
        "# or\n",
        "# print(out.size())  # Using .size() method\n",
        "# Create a plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(out, cmap='viridis', aspect='auto', interpolation='nearest')\n",
        "plt.xlabel(\"Dimension\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.title(\"Decoder Representations (for one digit at the last time t)\" + \" [epoch: \" + str(epoch_number) + \"]\")\n",
        "\n",
        "plt.colorbar(label='Spike Count')\n",
        "\n",
        "filename = f\"Dec_b_syn_{beta_syn}_spike_epochs_{epoch_number}_of_{epochs}_(8000x32).pdf\"\n",
        "plt.savefig(filename, format=\"pdf\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4z3jtT4MQaS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtWdKvsUAAScC75LAnjgx7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}