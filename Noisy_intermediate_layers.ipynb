{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrezakhodashenas/PhD-NN/blob/output-spikes/Noisy_intermediate_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpGlbUETWpPo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6HiiAfjcTIpU"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOq6G-QBejwM",
        "outputId": "be89915b-4710-43eb-9b61-8c7866e2e4bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8fz5OaNTdGj",
        "outputId": "f58345ab-ebd2-4508-ede6-1ec41a49374f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.40.5)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czXuC1x8el5J",
        "outputId": "07e268df-e370-43e0-ce7a-f2aa63928fee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.2.0.post0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.9.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTEDV8wqLZXr",
        "outputId": "7b51b995-2e34-4c62-da29-f14bc2352b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.25.2)\n",
            "Requirement already satisfied: nir in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.0.1)\n",
            "Requirement already satisfied: nirtorch in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install snntorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGPDVXvjdGsl",
        "outputId": "ee2e5632-cfaf-417f-8b40-78b17e2edb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EmnhljuKb4Oo"
      },
      "outputs": [],
      "source": [
        "!pip install wandb onnx -Uq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrU2LE3TX46s"
      },
      "source": [
        "## set seeds for PyTorch and Numpy to ensure reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KK1kdhFXX46s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for Python, Numpy, and Torch for reproducibility\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "\n",
        "# seed = 42\n",
        "# random.seed(seed)\n",
        "# np.random.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "# torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Additional steps if you're using GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vz5nsuENX46s"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pickle\n",
        "import matplotlib.animation as animation\n",
        "from scipy.integrate import simps\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import os, sys, time, datetime, json, random\n",
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import utils as utls\n",
        "from snntorch import utils\n",
        "from snntorch import surrogate\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import auc\n",
        "from torchsummary import summary\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn as nn\n",
        "from scipy.stats import entropy\n",
        "from scipy.special import kl_div\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score\n",
        "# from skopt import gp_minimize\n",
        "# from bayes_opt import BayesianOptimization\n",
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.ticker as mtick\n",
        "\n",
        "\n",
        "\n",
        "# import spikeflow as snn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fe7dfc869c7793392e4f225d9d64f615e2fd70ec\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "5IigYTjbTrGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6191b0f0-b114-43e9-b08b-13580dd6ca8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmreza-khodashenas\u001b[0m (\u001b[33mneuroscience_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"Noisy_StdDev0_1_Intermediate_Layers\")      # Set the project where this run will be logged\n",
        "\n",
        "config = dict(\n",
        "    std_dev = 0.1,\n",
        "    subset_fraction = 1,     # the percentage of the data used\n",
        "    epochs=50,\n",
        "\n",
        "    batch_size=250,\n",
        "    learning_rate=0.0001,\n",
        "    input_size = 32,\n",
        "    kernels=[32, 64, 128],\n",
        "    threshold_Real = 0.5,\n",
        "    threshold_Recon = 0.5,\n",
        "\n",
        "    alpha=0.9,\n",
        "    # beta_syn=0.0001,\n",
        "    beta_syn=0.9,\n",
        "    beta =0.9,\n",
        "    num_steps=5,\n",
        "    latent_dim = 32, #dimension of latent layer (how compressed we want the information)\n",
        "    thresh=1,    #spiking threshold (lower = more spikes are let through)\n",
        "\n",
        "    lr=0.0001,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=0.001,\n",
        "\n",
        "    dataset=\"MNIST\",\n",
        "    architecture=\"SAE(L_Syn_Syn_L)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "hwNnL8kRUHvW",
        "outputId": "2e484db8-9544-4612-e81d-8abd0e1dead4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240220_135238-pfoeilpw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/neuroscience_ai/Noisy_StdDev0_1_Intermediate_Layers/runs/pfoeilpw' target=\"_blank\">prosperous-peony-4</a></strong> to <a href='https://wandb.ai/neuroscience_ai/Noisy_StdDev0_1_Intermediate_Layers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/neuroscience_ai/Noisy_StdDev0_1_Intermediate_Layers' target=\"_blank\">https://wandb.ai/neuroscience_ai/Noisy_StdDev0_1_Intermediate_Layers</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/neuroscience_ai/Noisy_StdDev0_1_Intermediate_Layers/runs/pfoeilpw' target=\"_blank\">https://wandb.ai/neuroscience_ai/Noisy_StdDev0_1_Intermediate_Layers/runs/pfoeilpw</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KehrXuVuPiAt"
      },
      "source": [
        "## set seeds for PyTorch and Numpy to ensure reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nOyTY0dE50vG"
      },
      "outputs": [],
      "source": [
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# /////////////////////# Building the Autoencoder\n",
        "#-------------------DataLoaders.  using the MNIST dataset\n",
        "\n",
        "# dataloader arguments\n",
        "batch_size = 250\n",
        "data_path='/data/mnist'\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# /////////////////////////////////# Define a transform\n",
        "input_size = 32 # resizing the original MNIST from 28 to 32\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((input_size, input_size)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "#------------------------------------------- Load MNIST\n",
        "# Training data\n",
        "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Testing data\n",
        "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ha5yn2VjO39Q"
      },
      "outputs": [],
      "source": [
        "# creating directories where we can save the original and reconstructed images for training and testing:\n",
        "# create training/ and testing/ folders in the chosen path\n",
        "if not os.path.isdir('figures/training'):\n",
        "    os.makedirs('figures/training')\n",
        "if not os.path.isdir('figures/binarytraining'):\n",
        "    os.makedirs('figures/binarytraining')\n",
        "\n",
        "if not os.path.isdir('figures/testing'):\n",
        "    os.makedirs('figures/testing')\n",
        "if not os.path.isdir('figures/binarytesting'):\n",
        "    os.makedirs('figures/binarytesting')\n",
        "\n",
        "\n",
        "if not os.path.isdir('Saved_Trained_Checkpoints/'):\n",
        "    os.makedirs('Saved_Trained_Checkpoints/')\n",
        "\n",
        "if not os.path.isdir('Output_Spikes/'):\n",
        "    os.makedirs('Output_Spikes/')\n",
        "\n",
        "if not os.path.isdir('Enc_syn_Spikes/'):\n",
        "    os.makedirs('Enc_syn_Spikes/')\n",
        "\n",
        "\n",
        "if not os.path.isdir('Intermediate_Lyrs/'):\n",
        "    os.makedirs('Intermediate_Lyrs/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcZMZrCoKFaq",
        "outputId": "20caadbe-703c-462f-b65b-f7e427053ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Tesla T4 (cuda)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using {torch.cuda.get_device_name()} ({device})\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        # encoder_layers = [\n",
        "        #     ('conv1', nn.Conv2d(1, 32, 3, padding=1, stride=2)),  # Conv Layer 1\n",
        "        #     ('batchnorm1', nn.BatchNorm2d(32)),\n",
        "        #     ('leaky1', snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),\n",
        "        #     ('conv2', nn.Conv2d(32, 64, 3, padding=1, stride=2)),  # Conv Layer 2\n",
        "        #     ('batchnorm2', nn.BatchNorm2d(64)),\n",
        "        #     ('synaptic1', snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),  # SNN TORCH LIF NEURON\n",
        "        #     ('conv3', nn.Conv2d(64, 128, 3, padding=1, stride=2)),  # Conv Layer 3\n",
        "        #     ('batchnorm3', nn.BatchNorm2d(128)),\n",
        "        #     ('synaptic2', snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh)),  # SNN TORCH LIF NEURON\n",
        "        #     ('flatten', nn.Flatten(start_dim=1, end_dim=3)),  # Flatten convolutional output\n",
        "        #     ('linear', nn.Linear(128 * 4 * 4, latent_dim)),  # Fully connected linear layer\n",
        "        #     ('leaky2', snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True, threshold=thresh))\n",
        "        # ]\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "                            nn.Conv2d(1, 32, 3,padding = 1,stride=2), # Conv Layer 1\n",
        "                            nn.BatchNorm2d(32),\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.Conv2d(32, 64, 3,padding = 1,stride=2), # Conv Layer 2\n",
        "                            nn.BatchNorm2d(64),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True, threshold=thresh), #SNN TORCH LIF NEURON\n",
        "                            nn.Conv2d(64, 128, 3,padding = 1,stride=2), # Conv Layer 3\n",
        "                            nn.BatchNorm2d(128),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh), #SNN TORCH LIF NEURON\n",
        "                            nn.Flatten(start_dim = 1, end_dim = 3), #Flatten convolutional output\n",
        "                            nn.Linear(128*4*4, latent_dim), # Fully connected linear layer\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh)\n",
        "                            )\n",
        "\n",
        "\n",
        "        self.latent_dim = latent_dim #dimensions of the encoded z-space data\n",
        "        self.linearNet= nn.Sequential(\n",
        "                                      nn.Linear(latent_dim,128*4*4),\n",
        "                                      snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True,threshold=thresh))\n",
        "\n",
        "        # Decoder:\n",
        "        self.decoder = nn.Sequential(\n",
        "                            nn.Unflatten(1,(128,4,4)), #Unflatten data from 1 dim to tensor of 128 x 4 x 4\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(128, 64, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            nn.BatchNorm2d(64),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(64, 32, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            nn.BatchNorm2d(32),\n",
        "                            snn.Synaptic(alpha=alpha, beta=beta_syn, spike_grad=spike_grad, init_hidden=True,threshold=thresh),\n",
        "                            nn.ConvTranspose2d(32, 1, 3,padding = 1,stride=(2,2),output_padding=1),\n",
        "                            snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True,output=True,threshold=20000) #so membrane can be trained\n",
        "                            )\n",
        "    def forward(self, x):\n",
        "        utils.reset(self.encoder) #need to reset the hidden states of LIF\n",
        "        utils.reset(self.decoder)\n",
        "        utils.reset(self.linearNet)\n",
        "\n",
        "    #-----------------------------encode\n",
        "        spk_mem=[];\n",
        "        spk_rec=[];\n",
        "        spk_rec_syn=[];\n",
        "        encoder_mem=[];\n",
        "        spk_rec_dec=[];\n",
        "        spk_mem_dec=[];\n",
        "        enc5_rec = [];\n",
        "\n",
        "\n",
        "     #------------------------------ intermediate layers\n",
        "\n",
        "        # for step in range(num_steps):\n",
        "        #     enc5 = self.encoder[5](x)             #  enc5 shape: torch.Size([250, 1, 32, 32])\n",
        "        #     enc5_rec.append(enc5)\n",
        "        # enc5_rec = torch.stack(enc5_rec, dim=2)            #   enc5_rec size: torch.Size([250, 1, 5, 32, 32])\n",
        "        # Enc_syn_1 = enc5_rec[:, :, -1]                      # #   torch.Size([250, 1, 32, 32])\n",
        "\n",
        "     #------------------------------ encode\n",
        "        for step in range(num_steps):\n",
        "            spk_x, mem_x = self.encoder(x)              # spk_x size: ([250, 32])  ,   mem_x size: ([250, 32])  , x.shape : torch.Size([250, 1, 32, 32])\n",
        "            spk_rec.append(spk_x)\n",
        "            spk_mem.append(mem_x)\n",
        "\n",
        "        spk_rec=torch.stack(spk_rec,dim=2) # stack spikes in second tensor dimension # ----------------spk_rec in torch.stack(spk_rec,dim=2):  torch.Size([250, 32, 5])\n",
        "        spk_mem=torch.stack(spk_mem,dim=2) # stack membranes in second tensor dimension # ----------------spk_mem in torch.stack(spk_mem,dim=2):  torch.Size([250, 32, 5])\n",
        "        out_en = spk_rec[...,step]\n",
        "\n",
        "        # print(\"out_en= spk_rec[...,step]:-----------\" , spk_rec[...,step].size()) # spk_rec[...,step]:----------- torch.Size([250, 32])       input of the latent and then decoder\n",
        "\n",
        "     #------------------------------decode\n",
        "        spk_mem2=[];\n",
        "        spk_rec2=[];\n",
        "        decoded_x=[];\n",
        "        spk_x_dec=[];\n",
        "        mem_x_dec=[];\n",
        "        for step in range(num_steps): #for t in time                           #        from decoder: ([250, 1, 32, 32])\n",
        "            x_recon, x_mem_recon = self.decode(spk_rec[...,step])\n",
        "            spk_rec2.append(x_recon)\n",
        "            spk_mem2.append(x_mem_recon)\n",
        "\n",
        "        spk_rec2=torch.stack(spk_rec2,dim=4)\n",
        "        spk_mem2=torch.stack(spk_mem2,dim=4)\n",
        "\n",
        "        out = spk_mem2[:,:,:,:,-1]\n",
        "\n",
        "        self.out_en = out_en\n",
        "        self.out = out\n",
        "\n",
        "        return out, out_en\n",
        "        # return out, Enc_syn_1\n",
        "\n",
        "    def encode(self,x):\n",
        "      spk_latent_x, mem_latent_x = self.encoder(x)\n",
        "      return spk_latent_x, mem_latent_x\n",
        "\n",
        "\n",
        "    def decode(self,x):\n",
        "        spk_x, mem_x = self.linearNet(x) #convert latent dimension back to total size of features in encoder final layer\n",
        "        spk_x2, mem_x2 = self.decoder(spk_x)\n",
        "        return spk_x2, mem_x2\n",
        "\n"
      ],
      "metadata": {
        "id": "U_D8NuZnxpXG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_pixel_accuracy(real_img, x_recon):\n",
        "    # Flatten the tensors and convert them to NumPy arrays\n",
        "    real_img_np = real_img.flatten().cpu().numpy()\n",
        "    x_recon_np = x_recon.flatten().detach().cpu().numpy()\n",
        "\n",
        "    # Calculate pixel-wise similarity\n",
        "    pixel_accuracy = 1 - np.mean(np.abs(x_recon_np - real_img_np))\n",
        "\n",
        "    return pixel_accuracy\n"
      ],
      "metadata": {
        "id": "CR6iBkrRYTaM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5HbcSNNp6vK2"
      },
      "outputs": [],
      "source": [
        "# Training and Testing\n",
        "# using MSE loss to compare the reconstructed image (x_recon) with the original image (real_img)\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "spike_recordings = []\n",
        "train_ber_rec = []\n",
        "test_ber_rec = []\n",
        "threshold_Real = 0.5\n",
        "threshold_Recon = 0.5\n",
        "\n",
        "def train(network, trainloader, opti, epoch):\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(network, log=\"all\", log_freq=10)\n",
        "    network=network.train()\n",
        "    train_loss_hist=torch.zeros((1), dtype=dtype, device=device)\n",
        "    train_avg_loss_rec=[]\n",
        "\n",
        "    for batch_idx, (real_img, labels) in enumerate(trainloader):\n",
        "        opti.zero_grad()\n",
        "        real_img = real_img.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "      # -------------Add Gaussian noise to the input images\n",
        "        real_img = real_img + torch.randn_like(real_img) * std_dev\n",
        "\n",
        "\n",
        "        out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "        x_recon, out = network(real_img)  # Pass data into network and return reconstructed image and spk_rec.  #        x_recon size torch.Size([250, 1, 32, 32]) ,  #        out size torch.Size([250, 32])\n",
        "        #Calculate loss\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "        for step in range(num_steps):\n",
        "          loss_val += F.mse_loss(x_recon, real_img)                  #.view(1, -1)\n",
        "        train_loss_hist += (loss_val.item())/num_steps\n",
        "        avg_loss=train_loss_hist.mean()\n",
        "\n",
        "        # # ---------------------------- Calculate Bit Error Rate (BER)\n",
        "        real_img_binary = (real_img > threshold_Real).float()\n",
        "        x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "        bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "        total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "        bit_error_rate = bit_errors.item() / total_pixels\n",
        "        train_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "        # ---------------\n",
        "        loss_val.backward()\n",
        "        opti.step()\n",
        "        train_loss_rec.append(loss_val.item())\n",
        "        # ----------------\n",
        "\n",
        "\n",
        "\n",
        "        # Save binary images\n",
        "        Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "        if batch_idx == len(trainloader)-1:\n",
        "          if epoch in [0, 25, 49]:\n",
        "            utls.save_image(real_img_binary, f'figures/binarytraining/ep{epoch}_inputs_binary.png')\n",
        "            utls.save_image(x_recon_binary, f'figures/binarytraining/ep{epoch}_recon_binary.png')\n",
        "            utls.save_image(Error_bin, f'figures/binarytraining/ep{epoch}_Error_bin.png')\n",
        "\n",
        "        # Logging for wandb\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "          wandb.log({\"Epoch\": epoch, \"Train_Batch\": batch_idx, \"Train_Loss\": loss_val.item(), \"Train_BER\": bit_error_rate})\n",
        "\n",
        "\n",
        "        print(f'Train[{epoch}/{max_epoch}][{batch_idx}/{len(trainloader)}] Loss: {loss_val.item()}, ' f'BER : {bit_error_rate}')\n",
        "\n",
        "\n",
        "        #Save reconstructed images every at the end of the epoch\n",
        "        if batch_idx == len(trainloader)-1:\n",
        "          if epoch in [0, 10, 25, 49]:\n",
        "            utls.save_image((real_img+1)/2, f'figures/training/epoch{epoch}_finalbatch_inputs.png')\n",
        "            utls.save_image((x_recon+1)/2, f'figures/training/epoch{epoch}_finalbatch_recon.png')\n",
        "            train_auc = auc(np.arange(len(train_loss_rec)), train_loss_rec)\n",
        "\n",
        "    epoch_loss = sum(train_loss_rec) / len(train_loss_rec)\n",
        "    print(f'Epoch [{epoch}/{max_epoch}] Train_Avg_loss(epoch): {epoch_loss:.5f}')\n",
        "    print('-------------------------------------')\n",
        "    wandb.log({\"Epoch\": epoch, \"Train_AvgLoss(Epoch)\": epoch_loss})\n",
        "\n",
        "    # return loss_val, train_loss_rec, train_auc , out, out_en  #, spk_rec_batches#, train_avg_loss_rec, #avg_loss #, train_loss_hist\n",
        "    return loss_val, train_loss_rec, epoch_loss , out, out_en      # train_accuracy, epoch_accuracy, noisy_out_en -----------------added\n",
        "\n",
        "    # return loss_val, train_loss_rec, train_auc   #\n",
        "\n",
        "\n",
        "# For Testing, not doing backpropagate, therefore no gradients are required and we use torch.no_grad():\n",
        "#Testing Loop\n",
        "def test(network, testloader, opti, epoch):\n",
        "    network=network.eval()\n",
        "    test_loss_hist=[]\n",
        "    test_avg_loss_rec=[]\n",
        "    test_avg_loss_hist = []\n",
        "\n",
        "    spk_rec_test = []\n",
        "    with torch.no_grad(): #no gradient this time\n",
        "        for batch_idx, (real_img, labels) in enumerate(testloader):\n",
        "            real_img = real_img.to(device)#\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            out, out_en = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "            x_recon , out = network(real_img)  # Pass data into network and return reconstructed image and spk_rec\n",
        "            # average Loss:\n",
        "            loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "            for step in range(num_steps):\n",
        "              loss_val += F.mse_loss(x_recon, real_img)\n",
        "            avg_loss=loss_val/num_steps\n",
        "            test_loss_hist.append(loss_val.item())\n",
        "\n",
        "            real_img_binary = (real_img > threshold_Real).float()\n",
        "            x_recon_binary = (x_recon > threshold_Recon).float()\n",
        "            bit_errors = torch.sum(torch.abs(real_img_binary - x_recon_binary))\n",
        "            total_pixels = real_img_binary.numel()  # Total number of pixels in the images\n",
        "            bit_error_rate = bit_errors.item() / total_pixels\n",
        "            test_ber_rec.append(bit_error_rate)  # Append BER to the list\n",
        "\n",
        "            # Save binary images\n",
        "            Error_bin = (torch.abs(x_recon_binary - real_img_binary))\n",
        "\n",
        "            if batch_idx == len(testloader)-1:\n",
        "              if epoch in [0, 25, 49]:\n",
        "                save_image(real_img_binary, f'figures/binarytesting/ep{epoch}_inputs_binary.png')\n",
        "                save_image(x_recon_binary, f'figures/binarytesting/ep{epoch}_recon_binary.png')\n",
        "                save_image(Error_bin, f'figures/binarytesting/ep{epoch}_Error_bin.png')\n",
        "\n",
        "\n",
        "        # Logging for wandb\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "              wandb.log({\"Epoch\": epoch, \"Test_Batch\": batch_idx, \"Test_Loss\": loss_val.item(), \"Test_BER\": bit_error_rate})\n",
        "\n",
        "            # -------------------------------------------------------------------------------------------------\n",
        "\n",
        "            print(f'Test[{epoch}/{max_epoch}][{batch_idx}/{len(testloader)}]  Loss: {loss_val.item()}, '  f'BER (test): {bit_error_rate}')\n",
        "\n",
        "            test_loss_rec.append(loss_val.item())\n",
        "\n",
        "            if batch_idx == len(testloader)-1:\n",
        "              if epoch in [0, 10, 25, 49]:\n",
        "                utls.save_image((real_img+1)/2, f'figures/testing/epoch{epoch}_finalbatch_inputs.png')\n",
        "                utls.save_image((real_img+(torch.randn_like(real_img) * std_dev)+1)/2, f'figures/testing/(real_img+noise)_epoch{epoch}_finalbatch_inputs.png')\n",
        "                utls.save_image((x_recon+1)/2, f'figures/testing/epoch{epoch}_finalbatch_recons.png')\n",
        "                test_auc = auc(np.arange(len(test_loss_rec)), test_loss_rec)\n",
        "\n",
        "    epoch_loss_test = sum(test_loss_hist) / len(test_loss_hist)\n",
        "    print(f'Test_Epoch [{epoch}/{max_epoch}] Test_Avg_loss(epoch): {epoch_loss_test:.5f}')\n",
        "\n",
        "    wandb.log({\"Test_Epoch\": epoch, \"Test_AvgLoss(Epoch)\": epoch_loss_test})\n",
        "\n",
        "    # return loss_val, test_loss_rec, test_auc, out, out_en                     # -------------------------------------------------ADDED\n",
        "    return loss_val, test_loss_rec, epoch_loss_test, out, out_en     # ------------------------------------- noisy_out_en  , test_accuracy ------------------ADDED\n",
        "\n",
        "    # return loss_val, test_loss_rec, test_auc  #\n",
        "\n",
        "for batch_spikes in spike_recordings:\n",
        "    print(batch_spikes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lF2cwM20PKAL"
      },
      "outputs": [],
      "source": [
        "input_size = 32 #resize of mnist data (optional)\n",
        "\n",
        "#setup GPU\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# neuron and simulation parameters\n",
        "spike_grad = surrogate.atan(alpha=2.0)  # alternate surrogate gradient fast_sigmoid(slope=25)\n",
        "\n",
        "train_loss_rec = []\n",
        "test_loss_rec = []\n",
        "train_loss_record = []\n",
        "test_loss_record = []\n",
        "train_avg_loss_rec=[]\n",
        "test_avg_loss_rec=[]\n",
        "\n",
        "  # Synaptic current and membrane potential decay exponentially with rates of alpha and beta\n",
        "alpha=0.9\n",
        "# beta_syn=0.0001\n",
        "beta_syn=0.9\n",
        "beta =0.9\n",
        "std_dev=0.1\n",
        "\n",
        "\n",
        "num_steps=5\n",
        "latent_dim = 32 #dimension of latent layer (how compressed we want the information)\n",
        "thresh=1    #spiking threshold (lower = more spikes are let through)\n",
        "epochs=50\n",
        "# epochs=5\n",
        "max_epoch=epochs\n",
        "\n",
        "  #Define Network and optimizer\n",
        "net=SAE()\n",
        "net = net.to(device)\n",
        "optimizer = torch.optim.AdamW(net.parameters(),\n",
        "                            lr=0.0001,\n",
        "                            betas=(0.9, 0.999),\n",
        "                            weight_decay=0.001)\n",
        "\n",
        "\n",
        "\n",
        "activation = {}\n",
        "# def get_activation(name):\n",
        "#     def hook(model, input, output):\n",
        "#         activation[name] = output.detach()\n",
        "#     return hook\n",
        "\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        if isinstance(output, tuple):\n",
        "            activation[name] = [out.detach() for out in output]\n",
        "        else:\n",
        "            activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "\n",
        "# net.encoder[5].register_forward_hook(get_activation('encoder[5]'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2hQ0wiOxUr"
      },
      "source": [
        "## for saving after each epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUOjPTrBOjmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e9a299-db16-4db3-c1f4-9abd194b5428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train[30/50][16/240] Loss: 0.11156463623046875, BER : 0.04204296875\n",
            "Train[30/50][17/240] Loss: 0.11268308758735657, BER : 0.042125\n",
            "Train[30/50][18/240] Loss: 0.11138640344142914, BER : 0.04152734375\n",
            "Train[30/50][19/240] Loss: 0.11234129965305328, BER : 0.04269140625\n",
            "Train[30/50][20/240] Loss: 0.11419010162353516, BER : 0.0435703125\n",
            "Train[30/50][21/240] Loss: 0.11199674755334854, BER : 0.04142578125\n",
            "Train[30/50][22/240] Loss: 0.1090904250741005, BER : 0.04060546875\n",
            "Train[30/50][23/240] Loss: 0.11163678020238876, BER : 0.04158203125\n",
            "Train[30/50][24/240] Loss: 0.11199823766946793, BER : 0.04175390625\n",
            "Train[30/50][25/240] Loss: 0.11562618613243103, BER : 0.04432421875\n",
            "Train[30/50][26/240] Loss: 0.11235829442739487, BER : 0.04210546875\n",
            "Train[30/50][27/240] Loss: 0.1141076534986496, BER : 0.0424921875\n",
            "Train[30/50][28/240] Loss: 0.11394181102514267, BER : 0.04315625\n",
            "Train[30/50][29/240] Loss: 0.11319425702095032, BER : 0.0430625\n",
            "Train[30/50][30/240] Loss: 0.11467508226633072, BER : 0.04348046875\n",
            "Train[30/50][31/240] Loss: 0.114474818110466, BER : 0.0431953125\n",
            "Train[30/50][32/240] Loss: 0.11075987666845322, BER : 0.04128125\n",
            "Train[30/50][33/240] Loss: 0.11067354679107666, BER : 0.041015625\n",
            "Train[30/50][34/240] Loss: 0.1167919784784317, BER : 0.04490234375\n",
            "Train[30/50][35/240] Loss: 0.11344419419765472, BER : 0.0427890625\n",
            "Train[30/50][36/240] Loss: 0.11384381353855133, BER : 0.04337890625\n",
            "Train[30/50][37/240] Loss: 0.11229754239320755, BER : 0.042234375\n",
            "Train[30/50][38/240] Loss: 0.1128019243478775, BER : 0.04300390625\n",
            "Train[30/50][39/240] Loss: 0.11053565889596939, BER : 0.04124609375\n",
            "Train[30/50][40/240] Loss: 0.11202006042003632, BER : 0.04194140625\n",
            "Train[30/50][41/240] Loss: 0.11246038973331451, BER : 0.0419453125\n",
            "Train[30/50][42/240] Loss: 0.11095602810382843, BER : 0.0411640625\n",
            "Train[30/50][43/240] Loss: 0.1128038689494133, BER : 0.04180078125\n",
            "Train[30/50][44/240] Loss: 0.1115822046995163, BER : 0.04208203125\n",
            "Train[30/50][45/240] Loss: 0.10938750207424164, BER : 0.04058984375\n",
            "Train[30/50][46/240] Loss: 0.11245745420455933, BER : 0.04223828125\n",
            "Train[30/50][47/240] Loss: 0.11427304148674011, BER : 0.0429921875\n",
            "Train[30/50][48/240] Loss: 0.1140289381146431, BER : 0.0427265625\n",
            "Train[30/50][49/240] Loss: 0.11539103090763092, BER : 0.04342578125\n",
            "Train[30/50][50/240] Loss: 0.11283598840236664, BER : 0.04276953125\n",
            "Train[30/50][51/240] Loss: 0.1143965795636177, BER : 0.04288671875\n",
            "Train[30/50][52/240] Loss: 0.11425299942493439, BER : 0.0432734375\n",
            "Train[30/50][53/240] Loss: 0.11346718668937683, BER : 0.04266015625\n",
            "Train[30/50][54/240] Loss: 0.11560723185539246, BER : 0.04415234375\n",
            "Train[30/50][55/240] Loss: 0.11540375649929047, BER : 0.04353515625\n",
            "Train[30/50][56/240] Loss: 0.11513634771108627, BER : 0.044015625\n",
            "Train[30/50][57/240] Loss: 0.11320903897285461, BER : 0.04386328125\n",
            "Train[30/50][58/240] Loss: 0.11046604067087173, BER : 0.04114453125\n",
            "Train[30/50][59/240] Loss: 0.11408240348100662, BER : 0.04303125\n",
            "Train[30/50][60/240] Loss: 0.11283814162015915, BER : 0.04271484375\n",
            "Train[30/50][61/240] Loss: 0.11275786906480789, BER : 0.04194140625\n",
            "Train[30/50][62/240] Loss: 0.11601532995700836, BER : 0.0446171875\n",
            "Train[30/50][63/240] Loss: 0.11588513106107712, BER : 0.04416796875\n",
            "Train[30/50][64/240] Loss: 0.11189787089824677, BER : 0.04231640625\n",
            "Train[30/50][65/240] Loss: 0.1122489720582962, BER : 0.04185546875\n",
            "Train[30/50][66/240] Loss: 0.11461922526359558, BER : 0.04332421875\n",
            "Train[30/50][67/240] Loss: 0.10868415981531143, BER : 0.03993359375\n",
            "Train[30/50][68/240] Loss: 0.11431732028722763, BER : 0.04258984375\n",
            "Train[30/50][69/240] Loss: 0.11328175663948059, BER : 0.04294140625\n",
            "Train[30/50][70/240] Loss: 0.11114635318517685, BER : 0.04225390625\n",
            "Train[30/50][71/240] Loss: 0.11053230613470078, BER : 0.04080859375\n",
            "Train[30/50][72/240] Loss: 0.11086063086986542, BER : 0.04211328125\n",
            "Train[30/50][73/240] Loss: 0.11008819937705994, BER : 0.04112890625\n",
            "Train[30/50][74/240] Loss: 0.1090250238776207, BER : 0.0400859375\n",
            "Train[30/50][75/240] Loss: 0.11290417611598969, BER : 0.04282421875\n",
            "Train[30/50][76/240] Loss: 0.11212922632694244, BER : 0.04208203125\n",
            "Train[30/50][77/240] Loss: 0.11345566064119339, BER : 0.04315625\n",
            "Train[30/50][78/240] Loss: 0.11053543537855148, BER : 0.0411328125\n",
            "Train[30/50][79/240] Loss: 0.11456463485956192, BER : 0.04353125\n",
            "Train[30/50][80/240] Loss: 0.11091484874486923, BER : 0.04120703125\n",
            "Train[30/50][81/240] Loss: 0.11240199208259583, BER : 0.04190234375\n",
            "Train[30/50][82/240] Loss: 0.1137087494134903, BER : 0.04258984375\n",
            "Train[30/50][83/240] Loss: 0.11358489096164703, BER : 0.04316796875\n",
            "Train[30/50][84/240] Loss: 0.11373578757047653, BER : 0.0427890625\n",
            "Train[30/50][85/240] Loss: 0.11040843278169632, BER : 0.0411640625\n",
            "Train[30/50][86/240] Loss: 0.10985039174556732, BER : 0.04097265625\n",
            "Train[30/50][87/240] Loss: 0.10911423712968826, BER : 0.0403984375\n",
            "Train[30/50][88/240] Loss: 0.11175213754177094, BER : 0.04188671875\n",
            "Train[30/50][89/240] Loss: 0.10901209712028503, BER : 0.04027734375\n",
            "Train[30/50][90/240] Loss: 0.11348139494657516, BER : 0.04324609375\n",
            "Train[30/50][91/240] Loss: 0.11355984210968018, BER : 0.04317578125\n",
            "Train[30/50][92/240] Loss: 0.11083866655826569, BER : 0.04211328125\n",
            "Train[30/50][93/240] Loss: 0.11185184866189957, BER : 0.04200390625\n",
            "Train[30/50][94/240] Loss: 0.11352609843015671, BER : 0.04268359375\n",
            "Train[30/50][95/240] Loss: 0.11287577450275421, BER : 0.04226953125\n",
            "Train[30/50][96/240] Loss: 0.11708196997642517, BER : 0.04508203125\n",
            "Train[30/50][97/240] Loss: 0.10921492427587509, BER : 0.04059765625\n",
            "Train[30/50][98/240] Loss: 0.10997145622968674, BER : 0.041296875\n",
            "Train[30/50][99/240] Loss: 0.11309068650007248, BER : 0.0432265625\n",
            "Train[30/50][100/240] Loss: 0.11434740573167801, BER : 0.04216015625\n",
            "Train[30/50][101/240] Loss: 0.11352955549955368, BER : 0.0434921875\n",
            "Train[30/50][102/240] Loss: 0.11237970739603043, BER : 0.04223046875\n",
            "Train[30/50][103/240] Loss: 0.11508148908615112, BER : 0.04363671875\n",
            "Train[30/50][104/240] Loss: 0.11175568401813507, BER : 0.0409296875\n",
            "Train[30/50][105/240] Loss: 0.11406822502613068, BER : 0.04322265625\n",
            "Train[30/50][106/240] Loss: 0.1092461347579956, BER : 0.0402109375\n",
            "Train[30/50][107/240] Loss: 0.11132818460464478, BER : 0.041625\n",
            "Train[30/50][108/240] Loss: 0.11280257999897003, BER : 0.04149609375\n",
            "Train[30/50][109/240] Loss: 0.11102522909641266, BER : 0.0415234375\n",
            "Train[30/50][110/240] Loss: 0.10951918363571167, BER : 0.04051171875\n",
            "Train[30/50][111/240] Loss: 0.10776211321353912, BER : 0.0394921875\n",
            "Train[30/50][112/240] Loss: 0.10967075824737549, BER : 0.04133984375\n",
            "Train[30/50][113/240] Loss: 0.11222727596759796, BER : 0.04205078125\n",
            "Train[30/50][114/240] Loss: 0.11337604373693466, BER : 0.04269921875\n",
            "Train[30/50][115/240] Loss: 0.11214548349380493, BER : 0.04208203125\n",
            "Train[30/50][116/240] Loss: 0.11227171123027802, BER : 0.042\n",
            "Train[30/50][117/240] Loss: 0.11324978619813919, BER : 0.04324609375\n",
            "Train[30/50][118/240] Loss: 0.11340871453285217, BER : 0.04305859375\n",
            "Train[30/50][119/240] Loss: 0.11022388935089111, BER : 0.04133203125\n",
            "Train[30/50][120/240] Loss: 0.11240843683481216, BER : 0.04262109375\n",
            "Train[30/50][121/240] Loss: 0.11345311999320984, BER : 0.04283984375\n",
            "Train[30/50][122/240] Loss: 0.11145002394914627, BER : 0.0424765625\n",
            "Train[30/50][123/240] Loss: 0.11653746664524078, BER : 0.04508203125\n",
            "Train[30/50][124/240] Loss: 0.10983706265687943, BER : 0.0404609375\n",
            "Train[30/50][125/240] Loss: 0.1125459223985672, BER : 0.042265625\n",
            "Train[30/50][126/240] Loss: 0.11278918385505676, BER : 0.0419765625\n",
            "Train[30/50][127/240] Loss: 0.10946165770292282, BER : 0.04055078125\n",
            "Train[30/50][128/240] Loss: 0.10877260565757751, BER : 0.03968359375\n",
            "Train[30/50][129/240] Loss: 0.11052811145782471, BER : 0.04112109375\n",
            "Train[30/50][130/240] Loss: 0.11159972846508026, BER : 0.041734375\n",
            "Train[30/50][131/240] Loss: 0.1109478622674942, BER : 0.04130859375\n",
            "Train[30/50][132/240] Loss: 0.10849837213754654, BER : 0.04101953125\n",
            "Train[30/50][133/240] Loss: 0.10976137220859528, BER : 0.04088671875\n",
            "Train[30/50][134/240] Loss: 0.11244672536849976, BER : 0.042296875\n",
            "Train[30/50][135/240] Loss: 0.11019493639469147, BER : 0.0414609375\n",
            "Train[30/50][136/240] Loss: 0.11396391689777374, BER : 0.0437265625\n",
            "Train[30/50][137/240] Loss: 0.10793320089578629, BER : 0.0397109375\n",
            "Train[30/50][138/240] Loss: 0.11137430369853973, BER : 0.04197265625\n",
            "Train[30/50][139/240] Loss: 0.10863795131444931, BER : 0.04037890625\n",
            "Train[30/50][140/240] Loss: 0.11246156692504883, BER : 0.04181640625\n",
            "Train[30/50][141/240] Loss: 0.11452872306108475, BER : 0.04294921875\n",
            "Train[30/50][142/240] Loss: 0.11062255501747131, BER : 0.041140625\n",
            "Train[30/50][143/240] Loss: 0.11233442276716232, BER : 0.0423046875\n",
            "Train[30/50][144/240] Loss: 0.11040668189525604, BER : 0.04078515625\n",
            "Train[30/50][145/240] Loss: 0.11879999935626984, BER : 0.04515234375\n",
            "Train[30/50][146/240] Loss: 0.11588547378778458, BER : 0.0438984375\n",
            "Train[30/50][147/240] Loss: 0.11277153342962265, BER : 0.04263671875\n",
            "Train[30/50][148/240] Loss: 0.11041165143251419, BER : 0.04120703125\n",
            "Train[30/50][149/240] Loss: 0.11197418719530106, BER : 0.04169921875\n",
            "Train[30/50][150/240] Loss: 0.11225830018520355, BER : 0.04251171875\n",
            "Train[30/50][151/240] Loss: 0.10943018645048141, BER : 0.04017578125\n",
            "Train[30/50][152/240] Loss: 0.11164502799510956, BER : 0.042\n",
            "Train[30/50][153/240] Loss: 0.11346378922462463, BER : 0.04309375\n",
            "Train[30/50][154/240] Loss: 0.11313166469335556, BER : 0.0425234375\n",
            "Train[30/50][155/240] Loss: 0.11437226831912994, BER : 0.04341015625\n",
            "Train[30/50][156/240] Loss: 0.11160160601139069, BER : 0.0417109375\n",
            "Train[30/50][157/240] Loss: 0.11217404156923294, BER : 0.04218359375\n",
            "Train[30/50][158/240] Loss: 0.11106221377849579, BER : 0.04119140625\n",
            "Train[30/50][159/240] Loss: 0.11269357055425644, BER : 0.04211328125\n",
            "Train[30/50][160/240] Loss: 0.11239222437143326, BER : 0.04265234375\n",
            "Train[30/50][161/240] Loss: 0.11125405132770538, BER : 0.04142578125\n",
            "Train[30/50][162/240] Loss: 0.11223314702510834, BER : 0.042546875\n",
            "Train[30/50][163/240] Loss: 0.10906568169593811, BER : 0.04111328125\n",
            "Train[30/50][164/240] Loss: 0.11193405091762543, BER : 0.0420234375\n",
            "Train[30/50][165/240] Loss: 0.11202928423881531, BER : 0.04228515625\n",
            "Train[30/50][166/240] Loss: 0.11127001792192459, BER : 0.04210546875\n",
            "Train[30/50][167/240] Loss: 0.11238879710435867, BER : 0.04275\n",
            "Train[30/50][168/240] Loss: 0.11061355471611023, BER : 0.04071484375\n",
            "Train[30/50][169/240] Loss: 0.11257488280534744, BER : 0.0421640625\n",
            "Train[30/50][170/240] Loss: 0.11510644853115082, BER : 0.04385546875\n",
            "Train[30/50][171/240] Loss: 0.1097649335861206, BER : 0.04155859375\n",
            "Train[30/50][172/240] Loss: 0.11168093979358673, BER : 0.042703125\n",
            "Train[30/50][173/240] Loss: 0.11118569225072861, BER : 0.04109765625\n",
            "Train[30/50][174/240] Loss: 0.1107855886220932, BER : 0.04187890625\n",
            "Train[30/50][175/240] Loss: 0.11094629019498825, BER : 0.0418046875\n",
            "Train[30/50][176/240] Loss: 0.11271883547306061, BER : 0.0423359375\n",
            "Train[30/50][177/240] Loss: 0.11022619903087616, BER : 0.041125\n",
            "Train[30/50][178/240] Loss: 0.1129922941327095, BER : 0.0424765625\n",
            "Train[30/50][179/240] Loss: 0.11287134885787964, BER : 0.042625\n",
            "Train[30/50][180/240] Loss: 0.11186253279447556, BER : 0.0418828125\n",
            "Train[30/50][181/240] Loss: 0.11114053428173065, BER : 0.040515625\n",
            "Train[30/50][182/240] Loss: 0.1106540784239769, BER : 0.04196484375\n",
            "Train[30/50][183/240] Loss: 0.11203434318304062, BER : 0.0421640625\n",
            "Train[30/50][184/240] Loss: 0.10893095284700394, BER : 0.0406640625\n",
            "Train[30/50][185/240] Loss: 0.11420436948537827, BER : 0.04291796875\n",
            "Train[30/50][186/240] Loss: 0.11475526541471481, BER : 0.042796875\n",
            "Train[30/50][187/240] Loss: 0.11332051455974579, BER : 0.0430234375\n",
            "Train[30/50][188/240] Loss: 0.1158236563205719, BER : 0.04428515625\n",
            "Train[30/50][189/240] Loss: 0.11158160120248795, BER : 0.04253125\n",
            "Train[30/50][190/240] Loss: 0.10980111360549927, BER : 0.0404765625\n",
            "Train[30/50][191/240] Loss: 0.11187168955802917, BER : 0.041578125\n",
            "Train[30/50][192/240] Loss: 0.11115558445453644, BER : 0.04108984375\n",
            "Train[30/50][193/240] Loss: 0.11010067164897919, BER : 0.0405703125\n",
            "Train[30/50][194/240] Loss: 0.11253401637077332, BER : 0.0421875\n",
            "Train[30/50][195/240] Loss: 0.1105392724275589, BER : 0.04215625\n",
            "Train[30/50][196/240] Loss: 0.11336489766836166, BER : 0.04306640625\n",
            "Train[30/50][197/240] Loss: 0.11295177042484283, BER : 0.04313671875\n",
            "Train[30/50][198/240] Loss: 0.11194265633821487, BER : 0.0422578125\n",
            "Train[30/50][199/240] Loss: 0.11211337149143219, BER : 0.04228125\n",
            "Train[30/50][200/240] Loss: 0.11028267443180084, BER : 0.041046875\n",
            "Train[30/50][201/240] Loss: 0.1124090626835823, BER : 0.042359375\n",
            "Train[30/50][202/240] Loss: 0.11491337418556213, BER : 0.044640625\n",
            "Train[30/50][203/240] Loss: 0.10958437621593475, BER : 0.04126953125\n",
            "Train[30/50][204/240] Loss: 0.1108136773109436, BER : 0.04101171875\n",
            "Train[30/50][205/240] Loss: 0.1084824651479721, BER : 0.039625\n",
            "Train[30/50][206/240] Loss: 0.11233401298522949, BER : 0.0424296875\n",
            "Train[30/50][207/240] Loss: 0.11559319496154785, BER : 0.0440859375\n",
            "Train[30/50][208/240] Loss: 0.11072629690170288, BER : 0.04086328125\n",
            "Train[30/50][209/240] Loss: 0.11201557517051697, BER : 0.04252734375\n",
            "Train[30/50][210/240] Loss: 0.11079023778438568, BER : 0.0416640625\n",
            "Train[30/50][211/240] Loss: 0.11059816926717758, BER : 0.04142578125\n",
            "Train[30/50][212/240] Loss: 0.1129794791340828, BER : 0.04294921875\n",
            "Train[30/50][213/240] Loss: 0.11245457828044891, BER : 0.04233203125\n",
            "Train[30/50][214/240] Loss: 0.11319524049758911, BER : 0.042890625\n",
            "Train[30/50][215/240] Loss: 0.11264663934707642, BER : 0.04295703125\n",
            "Train[30/50][216/240] Loss: 0.11272444576025009, BER : 0.04324609375\n",
            "Train[30/50][217/240] Loss: 0.11073023080825806, BER : 0.04104296875\n",
            "Train[30/50][218/240] Loss: 0.11185947805643082, BER : 0.0418828125\n",
            "Train[30/50][219/240] Loss: 0.11344746500253677, BER : 0.04303515625\n",
            "Train[30/50][220/240] Loss: 0.11142752319574356, BER : 0.0413984375\n",
            "Train[30/50][221/240] Loss: 0.11178313195705414, BER : 0.0427109375\n",
            "Train[30/50][222/240] Loss: 0.1141190454363823, BER : 0.04356640625\n",
            "Train[30/50][223/240] Loss: 0.11354583501815796, BER : 0.0433359375\n",
            "Train[30/50][224/240] Loss: 0.11397556215524673, BER : 0.04378125\n",
            "Train[30/50][225/240] Loss: 0.11113995313644409, BER : 0.041421875\n",
            "Train[30/50][226/240] Loss: 0.11221688985824585, BER : 0.04149609375\n",
            "Train[30/50][227/240] Loss: 0.11300750821828842, BER : 0.04261328125\n",
            "Train[30/50][228/240] Loss: 0.11041851341724396, BER : 0.04126953125\n",
            "Train[30/50][229/240] Loss: 0.1137818768620491, BER : 0.0430390625\n",
            "Train[30/50][230/240] Loss: 0.11150460690259933, BER : 0.04196484375\n",
            "Train[30/50][231/240] Loss: 0.11229725182056427, BER : 0.0423828125\n",
            "Train[30/50][232/240] Loss: 0.11189641058444977, BER : 0.0418125\n",
            "Train[30/50][233/240] Loss: 0.11152400076389313, BER : 0.0420234375\n",
            "Train[30/50][234/240] Loss: 0.11293205618858337, BER : 0.0426171875\n",
            "Train[30/50][235/240] Loss: 0.10903091728687286, BER : 0.04015625\n",
            "Train[30/50][236/240] Loss: 0.11140820384025574, BER : 0.0413515625\n",
            "Train[30/50][237/240] Loss: 0.11114764213562012, BER : 0.04171875\n",
            "Train[30/50][238/240] Loss: 0.11218980699777603, BER : 0.04205859375\n",
            "Train[30/50][239/240] Loss: 0.11218325793743134, BER : 0.04259375\n",
            "Epoch [30/50] Train_Avg_loss(epoch): 0.31897\n",
            "-------------------------------------\n",
            "Test[30/50][0/40]  Loss: 0.0775269865989685, BER (test): 0.04156640625\n",
            "Test[30/50][1/40]  Loss: 0.08097828179597855, BER (test): 0.0438515625\n",
            "Test[30/50][2/40]  Loss: 0.08246812969446182, BER (test): 0.0445390625\n",
            "Test[30/50][3/40]  Loss: 0.07901180535554886, BER (test): 0.04198046875\n",
            "Test[30/50][4/40]  Loss: 0.0809868723154068, BER (test): 0.043515625\n",
            "Test[30/50][5/40]  Loss: 0.08084140717983246, BER (test): 0.043453125\n",
            "Test[30/50][6/40]  Loss: 0.08123825490474701, BER (test): 0.044359375\n",
            "Test[30/50][7/40]  Loss: 0.07773049920797348, BER (test): 0.042140625\n",
            "Test[30/50][8/40]  Loss: 0.07993359863758087, BER (test): 0.042859375\n",
            "Test[30/50][9/40]  Loss: 0.0788123831152916, BER (test): 0.0427109375\n",
            "Test[30/50][10/40]  Loss: 0.0801452249288559, BER (test): 0.0432109375\n",
            "Test[30/50][11/40]  Loss: 0.08014655858278275, BER (test): 0.0430859375\n",
            "Test[30/50][12/40]  Loss: 0.07908804714679718, BER (test): 0.0420859375\n",
            "Test[30/50][13/40]  Loss: 0.08003413677215576, BER (test): 0.04283984375\n",
            "Test[30/50][14/40]  Loss: 0.07904751598834991, BER (test): 0.04271484375\n",
            "Test[30/50][15/40]  Loss: 0.07937201112508774, BER (test): 0.04197265625\n",
            "Test[30/50][16/40]  Loss: 0.0785663053393364, BER (test): 0.04215625\n",
            "Test[30/50][17/40]  Loss: 0.07747479528188705, BER (test): 0.0412265625\n",
            "Test[30/50][18/40]  Loss: 0.08059612661600113, BER (test): 0.043265625\n",
            "Test[30/50][19/40]  Loss: 0.082964688539505, BER (test): 0.04450390625\n",
            "Test[30/50][20/40]  Loss: 0.07845772802829742, BER (test): 0.04242578125\n",
            "Test[30/50][21/40]  Loss: 0.07932782918214798, BER (test): 0.04222265625\n",
            "Test[30/50][22/40]  Loss: 0.08090519160032272, BER (test): 0.04387890625\n",
            "Test[30/50][23/40]  Loss: 0.07656200230121613, BER (test): 0.0411328125\n",
            "Test[30/50][24/40]  Loss: 0.0792786180973053, BER (test): 0.04237109375\n",
            "Test[30/50][25/40]  Loss: 0.0795050710439682, BER (test): 0.04312109375\n",
            "Test[30/50][26/40]  Loss: 0.07951588928699493, BER (test): 0.04244921875\n",
            "Test[30/50][27/40]  Loss: 0.08132670074701309, BER (test): 0.04343359375\n",
            "Test[30/50][28/40]  Loss: 0.07848381996154785, BER (test): 0.04173046875\n",
            "Test[30/50][29/40]  Loss: 0.08178062736988068, BER (test): 0.0439140625\n",
            "Test[30/50][30/40]  Loss: 0.07769971340894699, BER (test): 0.0415234375\n",
            "Test[30/50][31/40]  Loss: 0.08037744462490082, BER (test): 0.0429921875\n",
            "Test[30/50][32/40]  Loss: 0.07760848104953766, BER (test): 0.0413359375\n",
            "Test[30/50][33/40]  Loss: 0.07647469639778137, BER (test): 0.040921875\n",
            "Test[30/50][34/40]  Loss: 0.08010344207286835, BER (test): 0.04301953125\n",
            "Test[30/50][35/40]  Loss: 0.07822482287883759, BER (test): 0.04183203125\n",
            "Test[30/50][36/40]  Loss: 0.0811530351638794, BER (test): 0.043234375\n",
            "Test[30/50][37/40]  Loss: 0.0823260024189949, BER (test): 0.04425\n",
            "Test[30/50][38/40]  Loss: 0.080575130879879, BER (test): 0.043671875\n",
            "Test[30/50][39/40]  Loss: 0.0804261714220047, BER (test): 0.042859375\n",
            "Test_Epoch [30/50] Test_Avg_loss(epoch): 0.07968\n",
            "Train[31/50][0/240] Loss: 0.11342589557170868, BER : 0.04279296875\n",
            "Train[31/50][1/240] Loss: 0.11305782943964005, BER : 0.04283984375\n",
            "Train[31/50][2/240] Loss: 0.11304926127195358, BER : 0.0424921875\n",
            "Train[31/50][3/240] Loss: 0.1093379408121109, BER : 0.04035546875\n",
            "Train[31/50][4/240] Loss: 0.1115228533744812, BER : 0.0423828125\n",
            "Train[31/50][5/240] Loss: 0.1111697107553482, BER : 0.04196484375\n",
            "Train[31/50][6/240] Loss: 0.10923302173614502, BER : 0.0409375\n",
            "Train[31/50][7/240] Loss: 0.11236532032489777, BER : 0.041828125\n",
            "Train[31/50][8/240] Loss: 0.11178132891654968, BER : 0.04173046875\n",
            "Train[31/50][9/240] Loss: 0.11065258830785751, BER : 0.04100390625\n",
            "Train[31/50][10/240] Loss: 0.11156473308801651, BER : 0.0412578125\n",
            "Train[31/50][11/240] Loss: 0.1131034716963768, BER : 0.04262890625\n",
            "Train[31/50][12/240] Loss: 0.11230357736349106, BER : 0.042203125\n",
            "Train[31/50][13/240] Loss: 0.11065685749053955, BER : 0.04069140625\n",
            "Train[31/50][14/240] Loss: 0.1086154580116272, BER : 0.0401796875\n",
            "Train[31/50][15/240] Loss: 0.1122090145945549, BER : 0.0429140625\n",
            "Train[31/50][16/240] Loss: 0.11128120869398117, BER : 0.04195703125\n",
            "Train[31/50][17/240] Loss: 0.11323709040880203, BER : 0.0427578125\n",
            "Train[31/50][18/240] Loss: 0.11170534789562225, BER : 0.0431015625\n",
            "Train[31/50][19/240] Loss: 0.11104544997215271, BER : 0.041546875\n",
            "Train[31/50][20/240] Loss: 0.11028032749891281, BER : 0.041625\n",
            "Train[31/50][21/240] Loss: 0.1117539033293724, BER : 0.04238671875\n",
            "Train[31/50][22/240] Loss: 0.11264409124851227, BER : 0.04180078125\n",
            "Train[31/50][23/240] Loss: 0.11084768921136856, BER : 0.0410546875\n",
            "Train[31/50][24/240] Loss: 0.11361994594335556, BER : 0.04261328125\n",
            "Train[31/50][25/240] Loss: 0.11446527391672134, BER : 0.0438515625\n",
            "Train[31/50][26/240] Loss: 0.11498958617448807, BER : 0.044015625\n",
            "Train[31/50][27/240] Loss: 0.11317387223243713, BER : 0.04347265625\n",
            "Train[31/50][28/240] Loss: 0.11154337972402573, BER : 0.04203125\n",
            "Train[31/50][29/240] Loss: 0.1108195036649704, BER : 0.041484375\n",
            "Train[31/50][30/240] Loss: 0.1116572767496109, BER : 0.04285546875\n",
            "Train[31/50][31/240] Loss: 0.11316576600074768, BER : 0.04254296875\n",
            "Train[31/50][32/240] Loss: 0.11321915686130524, BER : 0.0430859375\n",
            "Train[31/50][33/240] Loss: 0.11066515743732452, BER : 0.04112109375\n",
            "Train[31/50][34/240] Loss: 0.11136141419410706, BER : 0.0426953125\n",
            "Train[31/50][35/240] Loss: 0.11195632815361023, BER : 0.04258203125\n",
            "Train[31/50][36/240] Loss: 0.11571108549833298, BER : 0.04365625\n",
            "Train[31/50][37/240] Loss: 0.11029098927974701, BER : 0.0417109375\n",
            "Train[31/50][38/240] Loss: 0.1161273643374443, BER : 0.04416015625\n",
            "Train[31/50][39/240] Loss: 0.11032740771770477, BER : 0.0409296875\n",
            "Train[31/50][40/240] Loss: 0.10924980789422989, BER : 0.0399453125\n",
            "Train[31/50][41/240] Loss: 0.1101524755358696, BER : 0.0408046875\n",
            "Train[31/50][42/240] Loss: 0.11397694051265717, BER : 0.04314453125\n",
            "Train[31/50][43/240] Loss: 0.11362866312265396, BER : 0.042796875\n",
            "Train[31/50][44/240] Loss: 0.10884896665811539, BER : 0.040625\n",
            "Train[31/50][45/240] Loss: 0.11315414309501648, BER : 0.04259765625\n",
            "Train[31/50][46/240] Loss: 0.1137716993689537, BER : 0.04279296875\n",
            "Train[31/50][47/240] Loss: 0.11132606118917465, BER : 0.042203125\n",
            "Train[31/50][48/240] Loss: 0.11399514228105545, BER : 0.04379296875\n",
            "Train[31/50][49/240] Loss: 0.10947268456220627, BER : 0.04083984375\n",
            "Train[31/50][50/240] Loss: 0.11005502194166183, BER : 0.04103515625\n",
            "Train[31/50][51/240] Loss: 0.1084350049495697, BER : 0.04087109375\n",
            "Train[31/50][52/240] Loss: 0.11198122799396515, BER : 0.04258203125\n",
            "Train[31/50][53/240] Loss: 0.11131016910076141, BER : 0.041203125\n",
            "Train[31/50][54/240] Loss: 0.11387617886066437, BER : 0.0430546875\n",
            "Train[31/50][55/240] Loss: 0.11119407415390015, BER : 0.0414375\n",
            "Train[31/50][56/240] Loss: 0.11350449919700623, BER : 0.04350390625\n",
            "Train[31/50][57/240] Loss: 0.11171181499958038, BER : 0.04192578125\n",
            "Train[31/50][58/240] Loss: 0.10830574482679367, BER : 0.0405\n",
            "Train[31/50][59/240] Loss: 0.1135706827044487, BER : 0.04266015625\n",
            "Train[31/50][60/240] Loss: 0.11448896676301956, BER : 0.0433984375\n",
            "Train[31/50][61/240] Loss: 0.1115608960390091, BER : 0.04182421875\n",
            "Train[31/50][62/240] Loss: 0.11201109737157822, BER : 0.0419453125\n",
            "Train[31/50][63/240] Loss: 0.11336719989776611, BER : 0.04284375\n",
            "Train[31/50][64/240] Loss: 0.1140839233994484, BER : 0.04344140625\n",
            "Train[31/50][65/240] Loss: 0.11391491442918777, BER : 0.04290625\n",
            "Train[31/50][66/240] Loss: 0.11186511814594269, BER : 0.042453125\n",
            "Train[31/50][67/240] Loss: 0.11034135520458221, BER : 0.04090234375\n",
            "Train[31/50][68/240] Loss: 0.11119567602872849, BER : 0.04133203125\n",
            "Train[31/50][69/240] Loss: 0.11077152192592621, BER : 0.0416796875\n",
            "Train[31/50][70/240] Loss: 0.11148235946893692, BER : 0.0416796875\n",
            "Train[31/50][71/240] Loss: 0.10938525199890137, BER : 0.04087109375\n",
            "Train[31/50][72/240] Loss: 0.111729197204113, BER : 0.04187890625\n",
            "Train[31/50][73/240] Loss: 0.1100161075592041, BER : 0.0410546875\n",
            "Train[31/50][74/240] Loss: 0.1121785119175911, BER : 0.04273828125\n",
            "Train[31/50][75/240] Loss: 0.10856766253709793, BER : 0.04082421875\n",
            "Train[31/50][76/240] Loss: 0.11087149381637573, BER : 0.04141796875\n",
            "Train[31/50][77/240] Loss: 0.11384886503219604, BER : 0.043609375\n",
            "Train[31/50][78/240] Loss: 0.11375033110380173, BER : 0.0436328125\n",
            "Train[31/50][79/240] Loss: 0.11202803999185562, BER : 0.04202734375\n",
            "Train[31/50][80/240] Loss: 0.11058376729488373, BER : 0.0412421875\n",
            "Train[31/50][81/240] Loss: 0.11454981565475464, BER : 0.04363671875\n",
            "Train[31/50][82/240] Loss: 0.1087128072977066, BER : 0.04022265625\n",
            "Train[31/50][83/240] Loss: 0.11298098415136337, BER : 0.04184375\n",
            "Train[31/50][84/240] Loss: 0.10985761880874634, BER : 0.04137109375\n",
            "Train[31/50][85/240] Loss: 0.10911014676094055, BER : 0.0405\n",
            "Train[31/50][86/240] Loss: 0.10795155167579651, BER : 0.040234375\n",
            "Train[31/50][87/240] Loss: 0.11258044838905334, BER : 0.04274609375\n",
            "Train[31/50][88/240] Loss: 0.11315242946147919, BER : 0.04330078125\n",
            "Train[31/50][89/240] Loss: 0.10924748331308365, BER : 0.04007421875\n",
            "Train[31/50][90/240] Loss: 0.10807148367166519, BER : 0.04036328125\n",
            "Train[31/50][91/240] Loss: 0.11372142285108566, BER : 0.04323828125\n",
            "Train[31/50][92/240] Loss: 0.11086971312761307, BER : 0.04182421875\n",
            "Train[31/50][93/240] Loss: 0.11097148060798645, BER : 0.041984375\n",
            "Train[31/50][94/240] Loss: 0.1090121939778328, BER : 0.04034765625\n",
            "Train[31/50][95/240] Loss: 0.11282328516244888, BER : 0.04315234375\n",
            "Train[31/50][96/240] Loss: 0.11219080537557602, BER : 0.04153125\n",
            "Train[31/50][97/240] Loss: 0.11223503202199936, BER : 0.0422578125\n",
            "Train[31/50][98/240] Loss: 0.11053948104381561, BER : 0.04107421875\n",
            "Train[31/50][99/240] Loss: 0.11171022057533264, BER : 0.0421171875\n",
            "Train[31/50][100/240] Loss: 0.11005944013595581, BER : 0.040859375\n",
            "Train[31/50][101/240] Loss: 0.10927435010671616, BER : 0.04076171875\n",
            "Train[31/50][102/240] Loss: 0.11080873012542725, BER : 0.04118359375\n",
            "Train[31/50][103/240] Loss: 0.1129942387342453, BER : 0.04329296875\n",
            "Train[31/50][104/240] Loss: 0.11043912917375565, BER : 0.0414140625\n",
            "Train[31/50][105/240] Loss: 0.11665838956832886, BER : 0.0445625\n",
            "Train[31/50][106/240] Loss: 0.11170946061611176, BER : 0.0417109375\n",
            "Train[31/50][107/240] Loss: 0.11236207187175751, BER : 0.04220703125\n",
            "Train[31/50][108/240] Loss: 0.107720285654068, BER : 0.0401953125\n",
            "Train[31/50][109/240] Loss: 0.10987389087677002, BER : 0.04066015625\n",
            "Train[31/50][110/240] Loss: 0.11214707046747208, BER : 0.04240625\n",
            "Train[31/50][111/240] Loss: 0.11395855993032455, BER : 0.04355078125\n",
            "Train[31/50][112/240] Loss: 0.11100601404905319, BER : 0.04158984375\n",
            "Train[31/50][113/240] Loss: 0.1142709031701088, BER : 0.04338671875\n",
            "Train[31/50][114/240] Loss: 0.11180323362350464, BER : 0.04221484375\n",
            "Train[31/50][115/240] Loss: 0.10955850780010223, BER : 0.040984375\n",
            "Train[31/50][116/240] Loss: 0.11302996426820755, BER : 0.04288671875\n",
            "Train[31/50][117/240] Loss: 0.10951687395572662, BER : 0.04141015625\n",
            "Train[31/50][118/240] Loss: 0.1107388287782669, BER : 0.04222265625\n",
            "Train[31/50][119/240] Loss: 0.1121193915605545, BER : 0.042078125\n",
            "Train[31/50][120/240] Loss: 0.11097556352615356, BER : 0.0417734375\n",
            "Train[31/50][121/240] Loss: 0.11122389882802963, BER : 0.0414140625\n",
            "Train[31/50][122/240] Loss: 0.11143350601196289, BER : 0.04259375\n",
            "Train[31/50][123/240] Loss: 0.10934524983167648, BER : 0.04037890625\n",
            "Train[31/50][124/240] Loss: 0.11212270706892014, BER : 0.0416953125\n",
            "Train[31/50][125/240] Loss: 0.11318303644657135, BER : 0.04312890625\n",
            "Train[31/50][126/240] Loss: 0.10864409804344177, BER : 0.04043359375\n",
            "Train[31/50][127/240] Loss: 0.11087386310100555, BER : 0.04126953125\n",
            "Train[31/50][128/240] Loss: 0.10974712669849396, BER : 0.04062109375\n",
            "Train[31/50][129/240] Loss: 0.11348667740821838, BER : 0.04270703125\n",
            "Train[31/50][130/240] Loss: 0.1096118688583374, BER : 0.04112890625\n",
            "Train[31/50][131/240] Loss: 0.11256550252437592, BER : 0.04218359375\n",
            "Train[31/50][132/240] Loss: 0.11159508675336838, BER : 0.04169921875\n",
            "Train[31/50][133/240] Loss: 0.11361853778362274, BER : 0.0432578125\n",
            "Train[31/50][134/240] Loss: 0.10820455104112625, BER : 0.03965234375\n",
            "Train[31/50][135/240] Loss: 0.10671766102313995, BER : 0.03948828125\n",
            "Train[31/50][136/240] Loss: 0.10988378524780273, BER : 0.041625\n",
            "Train[31/50][137/240] Loss: 0.11347859352827072, BER : 0.04272265625\n",
            "Train[31/50][138/240] Loss: 0.11181198805570602, BER : 0.04176953125\n",
            "Train[31/50][139/240] Loss: 0.11086554080247879, BER : 0.0417578125\n",
            "Train[31/50][140/240] Loss: 0.11011648923158646, BER : 0.04221484375\n",
            "Train[31/50][141/240] Loss: 0.11392296105623245, BER : 0.04298828125\n",
            "Train[31/50][142/240] Loss: 0.11367876827716827, BER : 0.043765625\n",
            "Train[31/50][143/240] Loss: 0.1099553108215332, BER : 0.0410546875\n",
            "Train[31/50][144/240] Loss: 0.10869544744491577, BER : 0.04084765625\n",
            "Train[31/50][145/240] Loss: 0.11438389122486115, BER : 0.0441171875\n",
            "Train[31/50][146/240] Loss: 0.11559931933879852, BER : 0.0443984375\n",
            "Train[31/50][147/240] Loss: 0.11063387989997864, BER : 0.04150390625\n",
            "Train[31/50][148/240] Loss: 0.11104512959718704, BER : 0.0424375\n",
            "Train[31/50][149/240] Loss: 0.11331231892108917, BER : 0.04259375\n",
            "Train[31/50][150/240] Loss: 0.1115528792142868, BER : 0.0419140625\n",
            "Train[31/50][151/240] Loss: 0.11183366924524307, BER : 0.042125\n",
            "Train[31/50][152/240] Loss: 0.1089918240904808, BER : 0.04069140625\n",
            "Train[31/50][153/240] Loss: 0.10968199372291565, BER : 0.04073828125\n",
            "Train[31/50][154/240] Loss: 0.11054977774620056, BER : 0.0414921875\n",
            "Train[31/50][155/240] Loss: 0.11273830384016037, BER : 0.04268359375\n",
            "Train[31/50][156/240] Loss: 0.10931414365768433, BER : 0.0408203125\n",
            "Train[31/50][157/240] Loss: 0.11347074806690216, BER : 0.04310546875\n",
            "Train[31/50][158/240] Loss: 0.110123410820961, BER : 0.0413828125\n",
            "Train[31/50][159/240] Loss: 0.1122676208615303, BER : 0.04225390625\n",
            "Train[31/50][160/240] Loss: 0.11210952699184418, BER : 0.0429375\n",
            "Train[31/50][161/240] Loss: 0.1103338897228241, BER : 0.041109375\n",
            "Train[31/50][162/240] Loss: 0.11332480609416962, BER : 0.04287890625\n",
            "Train[31/50][163/240] Loss: 0.11245585232973099, BER : 0.04241796875\n",
            "Train[31/50][164/240] Loss: 0.11146461963653564, BER : 0.04138671875\n",
            "Train[31/50][165/240] Loss: 0.10945665836334229, BER : 0.0401015625\n",
            "Train[31/50][166/240] Loss: 0.1093502938747406, BER : 0.040046875\n",
            "Train[31/50][167/240] Loss: 0.11342625319957733, BER : 0.04275\n",
            "Train[31/50][168/240] Loss: 0.11466896533966064, BER : 0.0438203125\n",
            "Train[31/50][169/240] Loss: 0.11193189769983292, BER : 0.04223046875\n",
            "Train[31/50][170/240] Loss: 0.10901707410812378, BER : 0.040859375\n",
            "Train[31/50][171/240] Loss: 0.11098162829875946, BER : 0.04253515625\n",
            "Train[31/50][172/240] Loss: 0.11234714090824127, BER : 0.04309375\n",
            "Train[31/50][173/240] Loss: 0.11176376789808273, BER : 0.0420859375\n",
            "Train[31/50][174/240] Loss: 0.11159154772758484, BER : 0.0427578125\n",
            "Train[31/50][175/240] Loss: 0.1115177646279335, BER : 0.04237890625\n",
            "Train[31/50][176/240] Loss: 0.1095467060804367, BER : 0.0409921875\n",
            "Train[31/50][177/240] Loss: 0.11130967736244202, BER : 0.0420859375\n",
            "Train[31/50][178/240] Loss: 0.10971296578645706, BER : 0.04110546875\n",
            "Train[31/50][179/240] Loss: 0.1078033521771431, BER : 0.0399453125\n",
            "Train[31/50][180/240] Loss: 0.11008386313915253, BER : 0.0412890625\n",
            "Train[31/50][181/240] Loss: 0.11014768481254578, BER : 0.04103125\n",
            "Train[31/50][182/240] Loss: 0.11285584419965744, BER : 0.042453125\n",
            "Train[31/50][183/240] Loss: 0.10944390296936035, BER : 0.041171875\n",
            "Train[31/50][184/240] Loss: 0.11328139901161194, BER : 0.0435859375\n",
            "Train[31/50][185/240] Loss: 0.11040149629116058, BER : 0.041765625\n",
            "Train[31/50][186/240] Loss: 0.10902300477027893, BER : 0.040953125\n",
            "Train[31/50][187/240] Loss: 0.1102292537689209, BER : 0.04169140625\n",
            "Train[31/50][188/240] Loss: 0.10934078693389893, BER : 0.0412421875\n",
            "Train[31/50][189/240] Loss: 0.11489403247833252, BER : 0.04380078125\n",
            "Train[31/50][190/240] Loss: 0.1139192134141922, BER : 0.0431484375\n",
            "Train[31/50][191/240] Loss: 0.11101198196411133, BER : 0.04153515625\n",
            "Train[31/50][192/240] Loss: 0.11198607087135315, BER : 0.04233203125\n",
            "Train[31/50][193/240] Loss: 0.1104460284113884, BER : 0.0422578125\n",
            "Train[31/50][194/240] Loss: 0.11362919211387634, BER : 0.04252734375\n",
            "Train[31/50][195/240] Loss: 0.10831177234649658, BER : 0.0401484375\n",
            "Train[31/50][196/240] Loss: 0.10755472630262375, BER : 0.03988671875\n",
            "Train[31/50][197/240] Loss: 0.11402328312397003, BER : 0.04375\n",
            "Train[31/50][198/240] Loss: 0.10860861092805862, BER : 0.04152734375\n",
            "Train[31/50][199/240] Loss: 0.11068053543567657, BER : 0.0422890625\n",
            "Train[31/50][200/240] Loss: 0.10919959843158722, BER : 0.03998046875\n",
            "Train[31/50][201/240] Loss: 0.11300749331712723, BER : 0.043296875\n",
            "Train[31/50][202/240] Loss: 0.11516328901052475, BER : 0.0442578125\n",
            "Train[31/50][203/240] Loss: 0.10826140642166138, BER : 0.0396484375\n",
            "Train[31/50][204/240] Loss: 0.11141778528690338, BER : 0.042671875\n",
            "Train[31/50][205/240] Loss: 0.1122390553355217, BER : 0.0425234375\n",
            "Train[31/50][206/240] Loss: 0.10911846160888672, BER : 0.0406640625\n",
            "Train[31/50][207/240] Loss: 0.11149731278419495, BER : 0.04159375\n",
            "Train[31/50][208/240] Loss: 0.11017464101314545, BER : 0.04128515625\n",
            "Train[31/50][209/240] Loss: 0.10910630226135254, BER : 0.04057421875\n",
            "Train[31/50][210/240] Loss: 0.11190126836299896, BER : 0.0420625\n",
            "Train[31/50][211/240] Loss: 0.11196669191122055, BER : 0.041890625\n",
            "Train[31/50][212/240] Loss: 0.11695155501365662, BER : 0.044265625\n",
            "Train[31/50][213/240] Loss: 0.11082909256219864, BER : 0.04137890625\n",
            "Train[31/50][214/240] Loss: 0.11101540178060532, BER : 0.04109375\n",
            "Train[31/50][215/240] Loss: 0.11282023042440414, BER : 0.04275\n",
            "Train[31/50][216/240] Loss: 0.11111893504858017, BER : 0.0417265625\n",
            "Train[31/50][217/240] Loss: 0.11041708290576935, BER : 0.04163671875\n",
            "Train[31/50][218/240] Loss: 0.11319492012262344, BER : 0.0433203125\n",
            "Train[31/50][219/240] Loss: 0.11291678249835968, BER : 0.04255859375\n",
            "Train[31/50][220/240] Loss: 0.1092824786901474, BER : 0.04046875\n",
            "Train[31/50][221/240] Loss: 0.10962387919425964, BER : 0.0412265625\n",
            "Train[31/50][222/240] Loss: 0.11006838083267212, BER : 0.04129296875\n",
            "Train[31/50][223/240] Loss: 0.11073778569698334, BER : 0.04200390625\n",
            "Train[31/50][224/240] Loss: 0.11172979325056076, BER : 0.04186328125\n",
            "Train[31/50][225/240] Loss: 0.10957426577806473, BER : 0.04128125\n",
            "Train[31/50][226/240] Loss: 0.11018719524145126, BER : 0.04121484375\n",
            "Train[31/50][227/240] Loss: 0.11216410249471664, BER : 0.04250390625\n",
            "Train[31/50][228/240] Loss: 0.1106455847620964, BER : 0.0413984375\n",
            "Train[31/50][229/240] Loss: 0.11082980036735535, BER : 0.04272265625\n",
            "Train[31/50][230/240] Loss: 0.10998430848121643, BER : 0.04134765625\n",
            "Train[31/50][231/240] Loss: 0.1101485937833786, BER : 0.04057421875\n",
            "Train[31/50][232/240] Loss: 0.1112791895866394, BER : 0.0424921875\n",
            "Train[31/50][233/240] Loss: 0.11270712316036224, BER : 0.04276171875\n",
            "Train[31/50][234/240] Loss: 0.10952477157115936, BER : 0.04037109375\n",
            "Train[31/50][235/240] Loss: 0.10945822298526764, BER : 0.04111328125\n",
            "Train[31/50][236/240] Loss: 0.11028233170509338, BER : 0.04054296875\n",
            "Train[31/50][237/240] Loss: 0.10999459773302078, BER : 0.0417890625\n",
            "Train[31/50][238/240] Loss: 0.11143598705530167, BER : 0.042828125\n",
            "Train[31/50][239/240] Loss: 0.11051367968320847, BER : 0.04157421875\n",
            "Epoch [31/50] Train_Avg_loss(epoch): 0.31248\n",
            "-------------------------------------\n",
            "Test[31/50][0/40]  Loss: 0.0797564908862114, BER (test): 0.04174609375\n",
            "Test[31/50][1/40]  Loss: 0.07966884970664978, BER (test): 0.0423828125\n",
            "Test[31/50][2/40]  Loss: 0.07786037027835846, BER (test): 0.04127734375\n",
            "Test[31/50][3/40]  Loss: 0.0800696313381195, BER (test): 0.0428046875\n",
            "Test[31/50][4/40]  Loss: 0.08100955188274384, BER (test): 0.043625\n",
            "Test[31/50][5/40]  Loss: 0.07881820201873779, BER (test): 0.042375\n",
            "Test[31/50][6/40]  Loss: 0.08243989944458008, BER (test): 0.04509765625\n",
            "Test[31/50][7/40]  Loss: 0.08095112442970276, BER (test): 0.0433359375\n",
            "Test[31/50][8/40]  Loss: 0.07824471592903137, BER (test): 0.041328125\n",
            "Test[31/50][9/40]  Loss: 0.08057951927185059, BER (test): 0.04314453125\n",
            "Test[31/50][10/40]  Loss: 0.08207610249519348, BER (test): 0.04382421875\n",
            "Test[31/50][11/40]  Loss: 0.0798846036195755, BER (test): 0.042640625\n",
            "Test[31/50][12/40]  Loss: 0.07880748063325882, BER (test): 0.04132421875\n",
            "Test[31/50][13/40]  Loss: 0.08055748045444489, BER (test): 0.04328125\n",
            "Test[31/50][14/40]  Loss: 0.07969382405281067, BER (test): 0.04197265625\n",
            "Test[31/50][15/40]  Loss: 0.08217968046665192, BER (test): 0.0438515625\n",
            "Test[31/50][16/40]  Loss: 0.07852846384048462, BER (test): 0.04204296875\n",
            "Test[31/50][17/40]  Loss: 0.0778413638472557, BER (test): 0.04184375\n",
            "Test[31/50][18/40]  Loss: 0.08214231580495834, BER (test): 0.04444921875\n",
            "Test[31/50][19/40]  Loss: 0.079414501786232, BER (test): 0.0418125\n",
            "Test[31/50][20/40]  Loss: 0.08081838488578796, BER (test): 0.0433984375\n",
            "Test[31/50][21/40]  Loss: 0.08038352429866791, BER (test): 0.0425859375\n",
            "Test[31/50][22/40]  Loss: 0.08115082234144211, BER (test): 0.04348828125\n",
            "Test[31/50][23/40]  Loss: 0.08037426322698593, BER (test): 0.042890625\n",
            "Test[31/50][24/40]  Loss: 0.078591488301754, BER (test): 0.04215625\n",
            "Test[31/50][25/40]  Loss: 0.07893431186676025, BER (test): 0.04187890625\n",
            "Test[31/50][26/40]  Loss: 0.08166325092315674, BER (test): 0.04353125\n",
            "Test[31/50][27/40]  Loss: 0.0795796811580658, BER (test): 0.0422578125\n",
            "Test[31/50][28/40]  Loss: 0.07967741787433624, BER (test): 0.04262890625\n",
            "Test[31/50][29/40]  Loss: 0.08036409318447113, BER (test): 0.04365234375\n",
            "Test[31/50][30/40]  Loss: 0.08116436004638672, BER (test): 0.04337109375\n",
            "Test[31/50][31/40]  Loss: 0.08152899146080017, BER (test): 0.043609375\n",
            "Test[31/50][32/40]  Loss: 0.08105257153511047, BER (test): 0.04338671875\n",
            "Test[31/50][33/40]  Loss: 0.08141762018203735, BER (test): 0.043234375\n",
            "Test[31/50][34/40]  Loss: 0.07796081900596619, BER (test): 0.0414375\n",
            "Test[31/50][35/40]  Loss: 0.08059412240982056, BER (test): 0.04397265625\n",
            "Test[31/50][36/40]  Loss: 0.08223698288202286, BER (test): 0.044625\n",
            "Test[31/50][37/40]  Loss: 0.07804452627897263, BER (test): 0.041375\n",
            "Test[31/50][38/40]  Loss: 0.0809442326426506, BER (test): 0.04369921875\n",
            "Test[31/50][39/40]  Loss: 0.08191884309053421, BER (test): 0.04421875\n",
            "Test_Epoch [31/50] Test_Avg_loss(epoch): 0.08022\n",
            "Train[32/50][0/240] Loss: 0.11167027056217194, BER : 0.0426640625\n",
            "Train[32/50][1/240] Loss: 0.11040298640727997, BER : 0.04146875\n",
            "Train[32/50][2/240] Loss: 0.10728278756141663, BER : 0.0394375\n",
            "Train[32/50][3/240] Loss: 0.1090627908706665, BER : 0.04144140625\n",
            "Train[32/50][4/240] Loss: 0.1106177344918251, BER : 0.04176953125\n",
            "Train[32/50][5/240] Loss: 0.11035460978746414, BER : 0.04171484375\n",
            "Train[32/50][6/240] Loss: 0.11018716543912888, BER : 0.04092578125\n",
            "Train[32/50][7/240] Loss: 0.11024875938892365, BER : 0.04169921875\n",
            "Train[32/50][8/240] Loss: 0.11284996569156647, BER : 0.0423828125\n",
            "Train[32/50][9/240] Loss: 0.111596018075943, BER : 0.04140625\n",
            "Train[32/50][10/240] Loss: 0.10902733355760574, BER : 0.04116015625\n",
            "Train[32/50][11/240] Loss: 0.11194050312042236, BER : 0.0419921875\n",
            "Train[32/50][12/240] Loss: 0.11177361011505127, BER : 0.04183203125\n",
            "Train[32/50][13/240] Loss: 0.10874401032924652, BER : 0.04058984375\n",
            "Train[32/50][14/240] Loss: 0.10720442235469818, BER : 0.03954296875\n",
            "Train[32/50][15/240] Loss: 0.11160531640052795, BER : 0.04248046875\n",
            "Train[32/50][16/240] Loss: 0.11073734611272812, BER : 0.04145703125\n",
            "Train[32/50][17/240] Loss: 0.1095718964934349, BER : 0.0414375\n",
            "Train[32/50][18/240] Loss: 0.11127457767724991, BER : 0.04166796875\n",
            "Train[32/50][19/240] Loss: 0.11119590699672699, BER : 0.0426171875\n",
            "Train[32/50][20/240] Loss: 0.11041298508644104, BER : 0.04225390625\n",
            "Train[32/50][21/240] Loss: 0.1130150854587555, BER : 0.04272265625\n",
            "Train[32/50][22/240] Loss: 0.11128602921962738, BER : 0.04148046875\n",
            "Train[32/50][23/240] Loss: 0.1074652373790741, BER : 0.040140625\n",
            "Train[32/50][24/240] Loss: 0.11159071326255798, BER : 0.04232421875\n",
            "Train[32/50][25/240] Loss: 0.11070245504379272, BER : 0.04182421875\n",
            "Train[32/50][26/240] Loss: 0.11360706388950348, BER : 0.04282421875\n",
            "Train[32/50][27/240] Loss: 0.11289335042238235, BER : 0.04265234375\n",
            "Train[32/50][28/240] Loss: 0.10920636355876923, BER : 0.0406328125\n",
            "Train[32/50][29/240] Loss: 0.10718709975481033, BER : 0.03987890625\n",
            "Train[32/50][30/240] Loss: 0.11097096651792526, BER : 0.0421484375\n",
            "Train[32/50][31/240] Loss: 0.11472509056329727, BER : 0.04428125\n",
            "Train[32/50][32/240] Loss: 0.11202631890773773, BER : 0.04202734375\n",
            "Train[32/50][33/240] Loss: 0.11496063321828842, BER : 0.043765625\n",
            "Train[32/50][34/240] Loss: 0.11046048253774643, BER : 0.0414609375\n",
            "Train[32/50][35/240] Loss: 0.11046503484249115, BER : 0.0408515625\n",
            "Train[32/50][36/240] Loss: 0.11089962720870972, BER : 0.04189453125\n",
            "Train[32/50][37/240] Loss: 0.11008671671152115, BER : 0.04046484375\n",
            "Train[32/50][38/240] Loss: 0.10974639654159546, BER : 0.04100390625\n",
            "Train[32/50][39/240] Loss: 0.11131555587053299, BER : 0.041625\n",
            "Train[32/50][40/240] Loss: 0.11098355054855347, BER : 0.0416171875\n",
            "Train[32/50][41/240] Loss: 0.10929211229085922, BER : 0.04041015625\n",
            "Train[32/50][42/240] Loss: 0.11220573633909225, BER : 0.04198046875\n",
            "Train[32/50][43/240] Loss: 0.10995560139417648, BER : 0.0412109375\n",
            "Train[32/50][44/240] Loss: 0.11101938784122467, BER : 0.04173046875\n",
            "Train[32/50][45/240] Loss: 0.11180433630943298, BER : 0.04231640625\n",
            "Train[32/50][46/240] Loss: 0.10896187275648117, BER : 0.040578125\n",
            "Train[32/50][47/240] Loss: 0.10951351374387741, BER : 0.04133203125\n",
            "Train[32/50][48/240] Loss: 0.11105215549468994, BER : 0.0422421875\n",
            "Train[32/50][49/240] Loss: 0.11206839233636856, BER : 0.04289453125\n",
            "Train[32/50][50/240] Loss: 0.10955056548118591, BER : 0.04074609375\n",
            "Train[32/50][51/240] Loss: 0.10886825621128082, BER : 0.0410078125\n",
            "Train[32/50][52/240] Loss: 0.1107863038778305, BER : 0.04170703125\n",
            "Train[32/50][53/240] Loss: 0.11078329384326935, BER : 0.0418828125\n",
            "Train[32/50][54/240] Loss: 0.11147581785917282, BER : 0.042078125\n",
            "Train[32/50][55/240] Loss: 0.11061762273311615, BER : 0.04188671875\n",
            "Train[32/50][56/240] Loss: 0.10909076035022736, BER : 0.04119921875\n",
            "Train[32/50][57/240] Loss: 0.10771602392196655, BER : 0.04031640625\n",
            "Train[32/50][58/240] Loss: 0.1107383668422699, BER : 0.04173828125\n",
            "Train[32/50][59/240] Loss: 0.10966488718986511, BER : 0.04244921875\n",
            "Train[32/50][60/240] Loss: 0.11014613509178162, BER : 0.042515625\n",
            "Train[32/50][61/240] Loss: 0.1103975921869278, BER : 0.0410390625\n",
            "Train[32/50][62/240] Loss: 0.11290203779935837, BER : 0.04241015625\n",
            "Train[32/50][63/240] Loss: 0.10837367922067642, BER : 0.0404375\n",
            "Train[32/50][64/240] Loss: 0.10666317492723465, BER : 0.03984765625\n",
            "Train[32/50][65/240] Loss: 0.10937400907278061, BER : 0.04075\n",
            "Train[32/50][66/240] Loss: 0.1117338240146637, BER : 0.0424453125\n",
            "Train[32/50][67/240] Loss: 0.10886932909488678, BER : 0.040546875\n",
            "Train[32/50][68/240] Loss: 0.11449384689331055, BER : 0.04373828125\n",
            "Train[32/50][69/240] Loss: 0.11089479923248291, BER : 0.04212890625\n",
            "Train[32/50][70/240] Loss: 0.10925028473138809, BER : 0.04060546875\n",
            "Train[32/50][71/240] Loss: 0.10917533934116364, BER : 0.04075390625\n",
            "Train[32/50][72/240] Loss: 0.11088208854198456, BER : 0.041984375\n",
            "Train[32/50][73/240] Loss: 0.11209218949079514, BER : 0.0419921875\n",
            "Train[32/50][74/240] Loss: 0.11317534744739532, BER : 0.043140625\n",
            "Train[32/50][75/240] Loss: 0.10597747564315796, BER : 0.03815234375\n",
            "Train[32/50][76/240] Loss: 0.11021088063716888, BER : 0.0415078125\n",
            "Train[32/50][77/240] Loss: 0.10916057974100113, BER : 0.04113671875\n",
            "Train[32/50][78/240] Loss: 0.11056498438119888, BER : 0.04200390625\n",
            "Train[32/50][79/240] Loss: 0.10889820754528046, BER : 0.04117578125\n",
            "Train[32/50][80/240] Loss: 0.10947111994028091, BER : 0.04053125\n",
            "Train[32/50][81/240] Loss: 0.10889534652233124, BER : 0.04084375\n",
            "Train[32/50][82/240] Loss: 0.11098930239677429, BER : 0.042484375\n",
            "Train[32/50][83/240] Loss: 0.11019998788833618, BER : 0.04133203125\n",
            "Train[32/50][84/240] Loss: 0.10995466262102127, BER : 0.04089453125\n",
            "Train[32/50][85/240] Loss: 0.1082131415605545, BER : 0.0404375\n",
            "Train[32/50][86/240] Loss: 0.1130213588476181, BER : 0.0427265625\n",
            "Train[32/50][87/240] Loss: 0.10802517831325531, BER : 0.0399140625\n",
            "Train[32/50][88/240] Loss: 0.10917918384075165, BER : 0.04080859375\n",
            "Train[32/50][89/240] Loss: 0.10923467576503754, BER : 0.04058984375\n",
            "Train[32/50][90/240] Loss: 0.11010254919528961, BER : 0.04160546875\n",
            "Train[32/50][91/240] Loss: 0.10970346629619598, BER : 0.0409296875\n",
            "Train[32/50][92/240] Loss: 0.1117674708366394, BER : 0.0423671875\n",
            "Train[32/50][93/240] Loss: 0.10895787179470062, BER : 0.04082421875\n",
            "Train[32/50][94/240] Loss: 0.11154332011938095, BER : 0.041765625\n",
            "Train[32/50][95/240] Loss: 0.11191323399543762, BER : 0.0422734375\n",
            "Train[32/50][96/240] Loss: 0.11350572854280472, BER : 0.043609375\n",
            "Train[32/50][97/240] Loss: 0.11411619186401367, BER : 0.043453125\n",
            "Train[32/50][98/240] Loss: 0.11158911883831024, BER : 0.04309765625\n",
            "Train[32/50][99/240] Loss: 0.11196139454841614, BER : 0.04260546875\n",
            "Train[32/50][100/240] Loss: 0.10907604545354843, BER : 0.04034375\n",
            "Train[32/50][101/240] Loss: 0.11197562515735626, BER : 0.04157421875\n",
            "Train[32/50][102/240] Loss: 0.11416944861412048, BER : 0.04334375\n",
            "Train[32/50][103/240] Loss: 0.11175338923931122, BER : 0.0420703125\n",
            "Train[32/50][104/240] Loss: 0.11137739568948746, BER : 0.0422109375\n",
            "Train[32/50][105/240] Loss: 0.10900299996137619, BER : 0.040765625\n",
            "Train[32/50][106/240] Loss: 0.10950185358524323, BER : 0.04140625\n",
            "Train[32/50][107/240] Loss: 0.10853971540927887, BER : 0.0404921875\n",
            "Train[32/50][108/240] Loss: 0.11116951704025269, BER : 0.042\n",
            "Train[32/50][109/240] Loss: 0.11519694328308105, BER : 0.044\n",
            "Train[32/50][110/240] Loss: 0.11350344121456146, BER : 0.0429609375\n",
            "Train[32/50][111/240] Loss: 0.10872255265712738, BER : 0.04045703125\n",
            "Train[32/50][112/240] Loss: 0.1117771565914154, BER : 0.04161328125\n",
            "Train[32/50][113/240] Loss: 0.11167490482330322, BER : 0.0422265625\n",
            "Train[32/50][114/240] Loss: 0.11338220536708832, BER : 0.04293359375\n",
            "Train[32/50][115/240] Loss: 0.10798947513103485, BER : 0.03962890625\n",
            "Train[32/50][116/240] Loss: 0.11159250140190125, BER : 0.04195703125\n",
            "Train[32/50][117/240] Loss: 0.10833922773599625, BER : 0.04049609375\n",
            "Train[32/50][118/240] Loss: 0.10996995866298676, BER : 0.04077734375\n",
            "Train[32/50][119/240] Loss: 0.11236976087093353, BER : 0.04349609375\n",
            "Train[32/50][120/240] Loss: 0.1127467006444931, BER : 0.04334765625\n",
            "Train[32/50][121/240] Loss: 0.11423218995332718, BER : 0.04335546875\n",
            "Train[32/50][122/240] Loss: 0.11062965542078018, BER : 0.04230078125\n",
            "Train[32/50][123/240] Loss: 0.10712557286024094, BER : 0.03940625\n",
            "Train[32/50][124/240] Loss: 0.10776665061712265, BER : 0.03958984375\n",
            "Train[32/50][125/240] Loss: 0.11221921443939209, BER : 0.0423828125\n",
            "Train[32/50][126/240] Loss: 0.11006908118724823, BER : 0.0407265625\n",
            "Train[32/50][127/240] Loss: 0.11199130117893219, BER : 0.04221484375\n",
            "Train[32/50][128/240] Loss: 0.11070245504379272, BER : 0.0418984375\n",
            "Train[32/50][129/240] Loss: 0.1136651262640953, BER : 0.04332421875\n",
            "Train[32/50][130/240] Loss: 0.11307041347026825, BER : 0.0421484375\n",
            "Train[32/50][131/240] Loss: 0.10876312106847763, BER : 0.04055078125\n",
            "Train[32/50][132/240] Loss: 0.1099044680595398, BER : 0.0408671875\n",
            "Train[32/50][133/240] Loss: 0.11228878796100616, BER : 0.04196875\n",
            "Train[32/50][134/240] Loss: 0.11288027465343475, BER : 0.0436484375\n",
            "Train[32/50][135/240] Loss: 0.10933826863765717, BER : 0.0403359375\n",
            "Train[32/50][136/240] Loss: 0.11023000627756119, BER : 0.04146484375\n",
            "Train[32/50][137/240] Loss: 0.11042345315217972, BER : 0.0413984375\n",
            "Train[32/50][138/240] Loss: 0.10735355317592621, BER : 0.04026171875\n",
            "Train[32/50][139/240] Loss: 0.11238527297973633, BER : 0.0429296875\n",
            "Train[32/50][140/240] Loss: 0.10913980007171631, BER : 0.04036328125\n",
            "Train[32/50][141/240] Loss: 0.10491339862346649, BER : 0.039125\n",
            "Train[32/50][142/240] Loss: 0.1082972064614296, BER : 0.04067578125\n",
            "Train[32/50][143/240] Loss: 0.11077921837568283, BER : 0.04198046875\n",
            "Train[32/50][144/240] Loss: 0.1098862886428833, BER : 0.04171484375\n",
            "Train[32/50][145/240] Loss: 0.1095428466796875, BER : 0.041921875\n",
            "Train[32/50][146/240] Loss: 0.11213663220405579, BER : 0.042140625\n",
            "Train[32/50][147/240] Loss: 0.11098626255989075, BER : 0.04247265625\n",
            "Train[32/50][148/240] Loss: 0.11401788890361786, BER : 0.04350390625\n",
            "Train[32/50][149/240] Loss: 0.1113460585474968, BER : 0.0413046875\n",
            "Train[32/50][150/240] Loss: 0.11000195145606995, BER : 0.0414765625\n",
            "Train[32/50][151/240] Loss: 0.10998107492923737, BER : 0.04172265625\n",
            "Train[32/50][152/240] Loss: 0.11291376501321793, BER : 0.04253515625\n",
            "Train[32/50][153/240] Loss: 0.11000453680753708, BER : 0.04080078125\n",
            "Train[32/50][154/240] Loss: 0.11384011059999466, BER : 0.043421875\n",
            "Train[32/50][155/240] Loss: 0.11041195690631866, BER : 0.0416796875\n",
            "Train[32/50][156/240] Loss: 0.11022041738033295, BER : 0.04089453125\n",
            "Train[32/50][157/240] Loss: 0.10943752527236938, BER : 0.04130078125\n",
            "Train[32/50][158/240] Loss: 0.11159662902355194, BER : 0.0423671875\n",
            "Train[32/50][159/240] Loss: 0.11066645383834839, BER : 0.04170703125\n",
            "Train[32/50][160/240] Loss: 0.11159717291593552, BER : 0.042484375\n",
            "Train[32/50][161/240] Loss: 0.11105068773031235, BER : 0.04224609375\n",
            "Train[32/50][162/240] Loss: 0.11074136942625046, BER : 0.04164453125\n",
            "Train[32/50][163/240] Loss: 0.10853308439254761, BER : 0.041171875\n",
            "Train[32/50][164/240] Loss: 0.11269982159137726, BER : 0.04330859375\n",
            "Train[32/50][165/240] Loss: 0.1100342646241188, BER : 0.0414296875\n",
            "Train[32/50][166/240] Loss: 0.10951900482177734, BER : 0.0414453125\n",
            "Train[32/50][167/240] Loss: 0.11189720779657364, BER : 0.0426015625\n",
            "Train[32/50][168/240] Loss: 0.11205627024173737, BER : 0.0423125\n",
            "Train[32/50][169/240] Loss: 0.11195757240056992, BER : 0.042\n",
            "Train[32/50][170/240] Loss: 0.11044986546039581, BER : 0.04162109375\n",
            "Train[32/50][171/240] Loss: 0.11038459837436676, BER : 0.04125390625\n",
            "Train[32/50][172/240] Loss: 0.11308673024177551, BER : 0.0431484375\n",
            "Train[32/50][173/240] Loss: 0.11189482361078262, BER : 0.041921875\n",
            "Train[32/50][174/240] Loss: 0.1106361448764801, BER : 0.04201953125\n",
            "Train[32/50][175/240] Loss: 0.11038217693567276, BER : 0.04119140625\n",
            "Train[32/50][176/240] Loss: 0.11082813888788223, BER : 0.04231640625\n",
            "Train[32/50][177/240] Loss: 0.1072838082909584, BER : 0.04018359375\n",
            "Train[32/50][178/240] Loss: 0.10960254818201065, BER : 0.0412109375\n",
            "Train[32/50][179/240] Loss: 0.10813840478658676, BER : 0.0403046875\n",
            "Train[32/50][180/240] Loss: 0.10975100845098495, BER : 0.0417265625\n",
            "Train[32/50][181/240] Loss: 0.10914716869592667, BER : 0.04133203125\n",
            "Train[32/50][182/240] Loss: 0.10987173020839691, BER : 0.0406484375\n",
            "Train[32/50][183/240] Loss: 0.11389467865228653, BER : 0.04346484375\n",
            "Train[32/50][184/240] Loss: 0.10986210405826569, BER : 0.04087890625\n",
            "Train[32/50][185/240] Loss: 0.11348522454500198, BER : 0.043\n",
            "Train[32/50][186/240] Loss: 0.10904349386692047, BER : 0.039984375\n",
            "Train[32/50][187/240] Loss: 0.11053595691919327, BER : 0.04153125\n",
            "Train[32/50][188/240] Loss: 0.11086840182542801, BER : 0.04212890625\n",
            "Train[32/50][189/240] Loss: 0.11106917262077332, BER : 0.041671875\n",
            "Train[32/50][190/240] Loss: 0.11060279607772827, BER : 0.04158203125\n",
            "Train[32/50][191/240] Loss: 0.11220414191484451, BER : 0.0423359375\n",
            "Train[32/50][192/240] Loss: 0.10926637798547745, BER : 0.04003125\n",
            "Train[32/50][193/240] Loss: 0.10947397351264954, BER : 0.04113671875\n",
            "Train[32/50][194/240] Loss: 0.11166095733642578, BER : 0.04162109375\n",
            "Train[32/50][195/240] Loss: 0.11350005120038986, BER : 0.04388671875\n",
            "Train[32/50][196/240] Loss: 0.11161987483501434, BER : 0.04222265625\n",
            "Train[32/50][197/240] Loss: 0.10857295989990234, BER : 0.0401875\n",
            "Train[32/50][198/240] Loss: 0.11015687137842178, BER : 0.04128125\n",
            "Train[32/50][199/240] Loss: 0.10676052421331406, BER : 0.03977734375\n",
            "Train[32/50][200/240] Loss: 0.10908868908882141, BER : 0.0402578125\n",
            "Train[32/50][201/240] Loss: 0.1100921779870987, BER : 0.04140625\n",
            "Train[32/50][202/240] Loss: 0.1090606227517128, BER : 0.04101953125\n",
            "Train[32/50][203/240] Loss: 0.11155357956886292, BER : 0.042453125\n",
            "Train[32/50][204/240] Loss: 0.10980114340782166, BER : 0.04187109375\n",
            "Train[32/50][205/240] Loss: 0.11251748353242874, BER : 0.043484375\n",
            "Train[32/50][206/240] Loss: 0.11116351187229156, BER : 0.0411875\n",
            "Train[32/50][207/240] Loss: 0.11064290255308151, BER : 0.04224609375\n",
            "Train[32/50][208/240] Loss: 0.11134132742881775, BER : 0.04210546875\n",
            "Train[32/50][209/240] Loss: 0.11207084357738495, BER : 0.0428046875\n",
            "Train[32/50][210/240] Loss: 0.10906800627708435, BER : 0.0407109375\n",
            "Train[32/50][211/240] Loss: 0.10802260041236877, BER : 0.04059765625\n",
            "Train[32/50][212/240] Loss: 0.10806703567504883, BER : 0.0406328125\n",
            "Train[32/50][213/240] Loss: 0.10933884233236313, BER : 0.0412265625\n",
            "Train[32/50][214/240] Loss: 0.11032677441835403, BER : 0.04208203125\n",
            "Train[32/50][215/240] Loss: 0.11016245931386948, BER : 0.041734375\n",
            "Train[32/50][216/240] Loss: 0.1082044392824173, BER : 0.04010546875\n",
            "Train[32/50][217/240] Loss: 0.11289314925670624, BER : 0.0429453125\n",
            "Train[32/50][218/240] Loss: 0.11233270913362503, BER : 0.04247265625\n",
            "Train[32/50][219/240] Loss: 0.11005308479070663, BER : 0.04168359375\n",
            "Train[32/50][220/240] Loss: 0.11067274957895279, BER : 0.04242578125\n",
            "Train[32/50][221/240] Loss: 0.10825372487306595, BER : 0.04024609375\n",
            "Train[32/50][222/240] Loss: 0.10928301513195038, BER : 0.0412578125\n",
            "Train[32/50][223/240] Loss: 0.10733485221862793, BER : 0.03994140625\n",
            "Train[32/50][224/240] Loss: 0.10735861212015152, BER : 0.040078125\n",
            "Train[32/50][225/240] Loss: 0.10713555663824081, BER : 0.04075390625\n",
            "Train[32/50][226/240] Loss: 0.10764130204916, BER : 0.04058203125\n",
            "Train[32/50][227/240] Loss: 0.1080714538693428, BER : 0.03994140625\n",
            "Train[32/50][228/240] Loss: 0.11077370494604111, BER : 0.04155859375\n",
            "Train[32/50][229/240] Loss: 0.10705345124006271, BER : 0.040140625\n",
            "Train[32/50][230/240] Loss: 0.1097831204533577, BER : 0.0413671875\n",
            "Train[32/50][231/240] Loss: 0.1100003719329834, BER : 0.041359375\n",
            "Train[32/50][232/240] Loss: 0.109205462038517, BER : 0.04016015625\n",
            "Train[32/50][233/240] Loss: 0.11211520433425903, BER : 0.043046875\n",
            "Train[32/50][234/240] Loss: 0.11234703660011292, BER : 0.04305859375\n",
            "Train[32/50][235/240] Loss: 0.1108076348900795, BER : 0.041921875\n",
            "Train[32/50][236/240] Loss: 0.1105480045080185, BER : 0.0412578125\n",
            "Train[32/50][237/240] Loss: 0.10992304235696793, BER : 0.04188671875\n",
            "Train[32/50][238/240] Loss: 0.11137297749519348, BER : 0.04210546875\n",
            "Train[32/50][239/240] Loss: 0.10863737016916275, BER : 0.04119921875\n",
            "Epoch [32/50] Train_Avg_loss(epoch): 0.30636\n",
            "-------------------------------------\n",
            "Test[32/50][0/40]  Loss: 0.07937978953123093, BER (test): 0.04193359375\n",
            "Test[32/50][1/40]  Loss: 0.07814770936965942, BER (test): 0.04205078125\n",
            "Test[32/50][2/40]  Loss: 0.07585317641496658, BER (test): 0.04075\n",
            "Test[32/50][3/40]  Loss: 0.07754624634981155, BER (test): 0.04131640625\n",
            "Test[32/50][4/40]  Loss: 0.080388143658638, BER (test): 0.04305859375\n",
            "Test[32/50][5/40]  Loss: 0.08022098988294601, BER (test): 0.0428515625\n",
            "Test[32/50][6/40]  Loss: 0.07943210005760193, BER (test): 0.0426796875\n",
            "Test[32/50][7/40]  Loss: 0.07962912321090698, BER (test): 0.042453125\n",
            "Test[32/50][8/40]  Loss: 0.0812210738658905, BER (test): 0.0429375\n",
            "Test[32/50][9/40]  Loss: 0.07905012369155884, BER (test): 0.042203125\n",
            "Test[32/50][10/40]  Loss: 0.08139315247535706, BER (test): 0.0438515625\n",
            "Test[32/50][11/40]  Loss: 0.08103754371404648, BER (test): 0.0430703125\n",
            "Test[32/50][12/40]  Loss: 0.07972516864538193, BER (test): 0.04233203125\n",
            "Test[32/50][13/40]  Loss: 0.07986314594745636, BER (test): 0.04287890625\n",
            "Test[32/50][14/40]  Loss: 0.08077143877744675, BER (test): 0.04371875\n",
            "Test[32/50][15/40]  Loss: 0.08009713888168335, BER (test): 0.04301171875\n",
            "Test[32/50][16/40]  Loss: 0.07803122699260712, BER (test): 0.0421953125\n",
            "Test[32/50][17/40]  Loss: 0.08037523925304413, BER (test): 0.04304296875\n",
            "Test[32/50][18/40]  Loss: 0.08129875361919403, BER (test): 0.04446484375\n",
            "Test[32/50][19/40]  Loss: 0.07965835928916931, BER (test): 0.0419296875\n",
            "Test[32/50][20/40]  Loss: 0.07945261150598526, BER (test): 0.04315625\n",
            "Test[32/50][21/40]  Loss: 0.07880464196205139, BER (test): 0.0425703125\n",
            "Test[32/50][22/40]  Loss: 0.0796404555439949, BER (test): 0.0424765625\n",
            "Test[32/50][23/40]  Loss: 0.07838834822177887, BER (test): 0.0423125\n",
            "Test[32/50][24/40]  Loss: 0.07934875786304474, BER (test): 0.042875\n",
            "Test[32/50][25/40]  Loss: 0.08141859620809555, BER (test): 0.0435703125\n",
            "Test[32/50][26/40]  Loss: 0.08223825693130493, BER (test): 0.04461328125\n",
            "Test[32/50][27/40]  Loss: 0.07920928299427032, BER (test): 0.0420234375\n",
            "Test[32/50][28/40]  Loss: 0.07975231111049652, BER (test): 0.04255078125\n",
            "Test[32/50][29/40]  Loss: 0.07980480790138245, BER (test): 0.04319140625\n",
            "Test[32/50][30/40]  Loss: 0.07795072346925735, BER (test): 0.042109375\n",
            "Test[32/50][31/40]  Loss: 0.0795026496052742, BER (test): 0.042828125\n",
            "Test[32/50][32/40]  Loss: 0.07818758487701416, BER (test): 0.04271875\n",
            "Test[32/50][33/40]  Loss: 0.08234763145446777, BER (test): 0.04475\n",
            "Test[32/50][34/40]  Loss: 0.08060051500797272, BER (test): 0.04368359375\n",
            "Test[32/50][35/40]  Loss: 0.08041373640298843, BER (test): 0.043140625\n",
            "Test[32/50][36/40]  Loss: 0.08074234426021576, BER (test): 0.04409765625\n",
            "Test[32/50][37/40]  Loss: 0.07882124930620193, BER (test): 0.04167578125\n",
            "Test[32/50][38/40]  Loss: 0.0803750604391098, BER (test): 0.04294140625\n",
            "Test[32/50][39/40]  Loss: 0.0796617865562439, BER (test): 0.0426875\n",
            "Test_Epoch [32/50] Test_Avg_loss(epoch): 0.07974\n",
            "Train[33/50][0/240] Loss: 0.1107301115989685, BER : 0.04165625\n",
            "Train[33/50][1/240] Loss: 0.11100631207227707, BER : 0.04178125\n",
            "Train[33/50][2/240] Loss: 0.10894010961055756, BER : 0.04053515625\n",
            "Train[33/50][3/240] Loss: 0.10812491178512573, BER : 0.040984375\n",
            "Train[33/50][4/240] Loss: 0.11161357164382935, BER : 0.04231640625\n",
            "Train[33/50][5/240] Loss: 0.11090391874313354, BER : 0.04146875\n",
            "Train[33/50][6/240] Loss: 0.11286728829145432, BER : 0.043203125\n",
            "Train[33/50][7/240] Loss: 0.11042711138725281, BER : 0.04192578125\n",
            "Train[33/50][8/240] Loss: 0.10728605091571808, BER : 0.03925\n",
            "Train[33/50][9/240] Loss: 0.11170714348554611, BER : 0.04223046875\n",
            "Train[33/50][10/240] Loss: 0.10912226140499115, BER : 0.04066796875\n",
            "Train[33/50][11/240] Loss: 0.11182403564453125, BER : 0.042296875\n",
            "Train[33/50][12/240] Loss: 0.11395297944545746, BER : 0.04373828125\n",
            "Train[33/50][13/240] Loss: 0.11117266118526459, BER : 0.04158984375\n",
            "Train[33/50][14/240] Loss: 0.11023961007595062, BER : 0.0414296875\n",
            "Train[33/50][15/240] Loss: 0.11056695133447647, BER : 0.042015625\n",
            "Train[33/50][16/240] Loss: 0.1108366921544075, BER : 0.04120703125\n",
            "Train[33/50][17/240] Loss: 0.11050795018672943, BER : 0.04176953125\n",
            "Train[33/50][18/240] Loss: 0.11044865846633911, BER : 0.04209765625\n",
            "Train[33/50][19/240] Loss: 0.1109524518251419, BER : 0.0423515625\n",
            "Train[33/50][20/240] Loss: 0.10895723104476929, BER : 0.0414140625\n",
            "Train[33/50][21/240] Loss: 0.1088518351316452, BER : 0.04062109375\n",
            "Train[33/50][22/240] Loss: 0.10927259176969528, BER : 0.04068359375\n",
            "Train[33/50][23/240] Loss: 0.11111176013946533, BER : 0.04133984375\n",
            "Train[33/50][24/240] Loss: 0.1081494614481926, BER : 0.0402734375\n",
            "Train[33/50][25/240] Loss: 0.11124987155199051, BER : 0.04189453125\n",
            "Train[33/50][26/240] Loss: 0.11132441461086273, BER : 0.04224609375\n",
            "Train[33/50][27/240] Loss: 0.10714922100305557, BER : 0.0393671875\n",
            "Train[33/50][28/240] Loss: 0.11022419482469559, BER : 0.0412734375\n",
            "Train[33/50][29/240] Loss: 0.11066920310258865, BER : 0.04161328125\n",
            "Train[33/50][30/240] Loss: 0.11383925378322601, BER : 0.04361328125\n",
            "Train[33/50][31/240] Loss: 0.10758952796459198, BER : 0.039671875\n",
            "Train[33/50][32/240] Loss: 0.11235211789608002, BER : 0.04229296875\n",
            "Train[33/50][33/240] Loss: 0.11254361271858215, BER : 0.0420859375\n",
            "Train[33/50][34/240] Loss: 0.10809328407049179, BER : 0.03974609375\n",
            "Train[33/50][35/240] Loss: 0.11196815967559814, BER : 0.04228515625\n",
            "Train[33/50][36/240] Loss: 0.10971665382385254, BER : 0.0414140625\n",
            "Train[33/50][37/240] Loss: 0.11097902059555054, BER : 0.04211328125\n",
            "Train[33/50][38/240] Loss: 0.10787282884120941, BER : 0.04018359375\n",
            "Train[33/50][39/240] Loss: 0.11478087306022644, BER : 0.04372265625\n",
            "Train[33/50][40/240] Loss: 0.11157375574111938, BER : 0.0421015625\n",
            "Train[33/50][41/240] Loss: 0.1116238385438919, BER : 0.04215234375\n",
            "Train[33/50][42/240] Loss: 0.10694079101085663, BER : 0.0391171875\n",
            "Train[33/50][43/240] Loss: 0.11089291423559189, BER : 0.04078125\n",
            "Train[33/50][44/240] Loss: 0.10965709388256073, BER : 0.041390625\n",
            "Train[33/50][45/240] Loss: 0.10761664062738419, BER : 0.03975\n",
            "Train[33/50][46/240] Loss: 0.11080670356750488, BER : 0.043046875\n",
            "Train[33/50][47/240] Loss: 0.10800332576036453, BER : 0.0398203125\n",
            "Train[33/50][48/240] Loss: 0.11000130325555801, BER : 0.041203125\n",
            "Train[33/50][49/240] Loss: 0.11304027587175369, BER : 0.042921875\n",
            "Train[33/50][50/240] Loss: 0.11090876162052155, BER : 0.04083203125\n",
            "Train[33/50][51/240] Loss: 0.11179731786251068, BER : 0.04248046875\n",
            "Train[33/50][52/240] Loss: 0.11211094260215759, BER : 0.04271875\n",
            "Train[33/50][53/240] Loss: 0.11025634407997131, BER : 0.04148828125\n",
            "Train[33/50][54/240] Loss: 0.11105858534574509, BER : 0.04221484375\n",
            "Train[33/50][55/240] Loss: 0.10960333049297333, BER : 0.041\n",
            "Train[33/50][56/240] Loss: 0.10941535979509354, BER : 0.04121875\n",
            "Train[33/50][57/240] Loss: 0.1124916523694992, BER : 0.04301953125\n",
            "Train[33/50][58/240] Loss: 0.11066380143165588, BER : 0.04205078125\n",
            "Train[33/50][59/240] Loss: 0.11075642704963684, BER : 0.042421875\n",
            "Train[33/50][60/240] Loss: 0.11036494374275208, BER : 0.042359375\n",
            "Train[33/50][61/240] Loss: 0.11307649314403534, BER : 0.0429296875\n",
            "Train[33/50][62/240] Loss: 0.11110905557870865, BER : 0.0426171875\n",
            "Train[33/50][63/240] Loss: 0.11347952485084534, BER : 0.0433359375\n",
            "Train[33/50][64/240] Loss: 0.10867952555418015, BER : 0.04050390625\n",
            "Train[33/50][65/240] Loss: 0.10812666267156601, BER : 0.0403125\n",
            "Train[33/50][66/240] Loss: 0.10874893516302109, BER : 0.0414140625\n",
            "Train[33/50][67/240] Loss: 0.1076337993144989, BER : 0.03948046875\n",
            "Train[33/50][68/240] Loss: 0.10889185965061188, BER : 0.0403125\n",
            "Train[33/50][69/240] Loss: 0.10839416086673737, BER : 0.0404921875\n",
            "Train[33/50][70/240] Loss: 0.11027804017066956, BER : 0.041828125\n",
            "Train[33/50][71/240] Loss: 0.10943810641765594, BER : 0.04020703125\n",
            "Train[33/50][72/240] Loss: 0.11537441611289978, BER : 0.04478125\n",
            "Train[33/50][73/240] Loss: 0.10814027488231659, BER : 0.04\n",
            "Train[33/50][74/240] Loss: 0.10770532488822937, BER : 0.04027734375\n",
            "Train[33/50][75/240] Loss: 0.11123912036418915, BER : 0.04265625\n",
            "Train[33/50][76/240] Loss: 0.1092718094587326, BER : 0.04072265625\n",
            "Train[33/50][77/240] Loss: 0.10624148696660995, BER : 0.03924609375\n",
            "Train[33/50][78/240] Loss: 0.10675244778394699, BER : 0.03934375\n",
            "Train[33/50][79/240] Loss: 0.11292650550603867, BER : 0.0430859375\n",
            "Train[33/50][80/240] Loss: 0.10884816944599152, BER : 0.04083203125\n",
            "Train[33/50][81/240] Loss: 0.11378639191389084, BER : 0.0436640625\n",
            "Train[33/50][82/240] Loss: 0.10822738707065582, BER : 0.04069140625\n",
            "Train[33/50][83/240] Loss: 0.10788887739181519, BER : 0.040234375\n",
            "Train[33/50][84/240] Loss: 0.1065882220864296, BER : 0.03996484375\n",
            "Train[33/50][85/240] Loss: 0.1086435541510582, BER : 0.040953125\n",
            "Train[33/50][86/240] Loss: 0.11277298629283905, BER : 0.043453125\n",
            "Train[33/50][87/240] Loss: 0.11061592400074005, BER : 0.0413671875\n",
            "Train[33/50][88/240] Loss: 0.10651271790266037, BER : 0.03955078125\n",
            "Train[33/50][89/240] Loss: 0.10967814922332764, BER : 0.04063671875\n",
            "Train[33/50][90/240] Loss: 0.10656402260065079, BER : 0.039609375\n",
            "Train[33/50][91/240] Loss: 0.1102118045091629, BER : 0.04194140625\n",
            "Train[33/50][92/240] Loss: 0.1108328327536583, BER : 0.04240234375\n",
            "Train[33/50][93/240] Loss: 0.1069905236363411, BER : 0.0397421875\n",
            "Train[33/50][94/240] Loss: 0.10899168252944946, BER : 0.0409453125\n",
            "Train[33/50][95/240] Loss: 0.10844726860523224, BER : 0.04069921875\n",
            "Train[33/50][96/240] Loss: 0.10802605003118515, BER : 0.04075390625\n",
            "Train[33/50][97/240] Loss: 0.10947614163160324, BER : 0.04098046875\n",
            "Train[33/50][98/240] Loss: 0.10950686037540436, BER : 0.0414765625\n",
            "Train[33/50][99/240] Loss: 0.10670299828052521, BER : 0.03966796875\n",
            "Train[33/50][100/240] Loss: 0.10843376815319061, BER : 0.04118359375\n",
            "Train[33/50][101/240] Loss: 0.10931017994880676, BER : 0.04195703125\n",
            "Train[33/50][102/240] Loss: 0.1122218519449234, BER : 0.04292578125\n",
            "Train[33/50][103/240] Loss: 0.1090371310710907, BER : 0.040734375\n",
            "Train[33/50][104/240] Loss: 0.11008284986019135, BER : 0.0421484375\n",
            "Train[33/50][105/240] Loss: 0.11051236093044281, BER : 0.0421953125\n",
            "Train[33/50][106/240] Loss: 0.11168085783720016, BER : 0.04223046875\n",
            "Train[33/50][107/240] Loss: 0.10914421826601028, BER : 0.04094921875\n",
            "Train[33/50][108/240] Loss: 0.10681167244911194, BER : 0.03984375\n",
            "Train[33/50][109/240] Loss: 0.10954386740922928, BER : 0.0417109375\n",
            "Train[33/50][110/240] Loss: 0.10825271904468536, BER : 0.04101953125\n",
            "Train[33/50][111/240] Loss: 0.10653737187385559, BER : 0.0395\n",
            "Train[33/50][112/240] Loss: 0.10933411121368408, BER : 0.0411796875\n",
            "Train[33/50][113/240] Loss: 0.11280626803636551, BER : 0.04269921875\n",
            "Train[33/50][114/240] Loss: 0.10819828510284424, BER : 0.03984375\n",
            "Train[33/50][115/240] Loss: 0.11070418357849121, BER : 0.0419609375\n",
            "Train[33/50][116/240] Loss: 0.10817621648311615, BER : 0.0397734375\n",
            "Train[33/50][117/240] Loss: 0.11084333807229996, BER : 0.042109375\n",
            "Train[33/50][118/240] Loss: 0.10963865369558334, BER : 0.04188671875\n",
            "Train[33/50][119/240] Loss: 0.10776215046644211, BER : 0.0407109375\n",
            "Train[33/50][120/240] Loss: 0.10897324234247208, BER : 0.0414453125\n",
            "Train[33/50][121/240] Loss: 0.10956312716007233, BER : 0.04130078125\n",
            "Train[33/50][122/240] Loss: 0.11339198797941208, BER : 0.04326953125\n",
            "Train[33/50][123/240] Loss: 0.10990400612354279, BER : 0.0417265625\n",
            "Train[33/50][124/240] Loss: 0.10900881141424179, BER : 0.04140625\n",
            "Train[33/50][125/240] Loss: 0.10693729668855667, BER : 0.0397734375\n",
            "Train[33/50][126/240] Loss: 0.11020474135875702, BER : 0.04226953125\n",
            "Train[33/50][127/240] Loss: 0.10814093053340912, BER : 0.04029296875\n",
            "Train[33/50][128/240] Loss: 0.1066804751753807, BER : 0.03976171875\n",
            "Train[33/50][129/240] Loss: 0.10927778482437134, BER : 0.04096875\n",
            "Train[33/50][130/240] Loss: 0.10722678154706955, BER : 0.040140625\n",
            "Train[33/50][131/240] Loss: 0.11033014208078384, BER : 0.04148828125\n",
            "Train[33/50][132/240] Loss: 0.10909076780080795, BER : 0.040171875\n",
            "Train[33/50][133/240] Loss: 0.11175365000963211, BER : 0.042296875\n",
            "Train[33/50][134/240] Loss: 0.10933693498373032, BER : 0.0409765625\n",
            "Train[33/50][135/240] Loss: 0.10957115143537521, BER : 0.04132421875\n",
            "Train[33/50][136/240] Loss: 0.1095026284456253, BER : 0.0404296875\n",
            "Train[33/50][137/240] Loss: 0.11182263493537903, BER : 0.04251953125\n",
            "Train[33/50][138/240] Loss: 0.11045113950967789, BER : 0.0426875\n",
            "Train[33/50][139/240] Loss: 0.11010950803756714, BER : 0.04103515625\n",
            "Train[33/50][140/240] Loss: 0.1100120022892952, BER : 0.04146875\n",
            "Train[33/50][141/240] Loss: 0.11266704648733139, BER : 0.0431484375\n",
            "Train[33/50][142/240] Loss: 0.109918974339962, BER : 0.04112890625\n",
            "Train[33/50][143/240] Loss: 0.1099955141544342, BER : 0.04162109375\n",
            "Train[33/50][144/240] Loss: 0.11141251027584076, BER : 0.04194921875\n",
            "Train[33/50][145/240] Loss: 0.1096193939447403, BER : 0.041171875\n",
            "Train[33/50][146/240] Loss: 0.10937517881393433, BER : 0.04126953125\n",
            "Train[33/50][147/240] Loss: 0.10965241491794586, BER : 0.041453125\n",
            "Train[33/50][148/240] Loss: 0.11345095187425613, BER : 0.043609375\n",
            "Train[33/50][149/240] Loss: 0.10774030536413193, BER : 0.0406484375\n",
            "Train[33/50][150/240] Loss: 0.1130269318819046, BER : 0.0434765625\n",
            "Train[33/50][151/240] Loss: 0.10706870257854462, BER : 0.0398984375\n",
            "Train[33/50][152/240] Loss: 0.10991410911083221, BER : 0.04105859375\n",
            "Train[33/50][153/240] Loss: 0.11140688508749008, BER : 0.042\n",
            "Train[33/50][154/240] Loss: 0.10824796557426453, BER : 0.04085546875\n",
            "Train[33/50][155/240] Loss: 0.10808687657117844, BER : 0.04105859375\n",
            "Train[33/50][156/240] Loss: 0.10992035269737244, BER : 0.04111328125\n",
            "Train[33/50][157/240] Loss: 0.10801133513450623, BER : 0.04033984375\n",
            "Train[33/50][158/240] Loss: 0.1103534922003746, BER : 0.04139453125\n",
            "Train[33/50][159/240] Loss: 0.10917367041110992, BER : 0.0411328125\n",
            "Train[33/50][160/240] Loss: 0.11368133872747421, BER : 0.0432109375\n",
            "Train[33/50][161/240] Loss: 0.10903126746416092, BER : 0.04164453125\n",
            "Train[33/50][162/240] Loss: 0.11125469207763672, BER : 0.04273046875\n",
            "Train[33/50][163/240] Loss: 0.10960842669010162, BER : 0.04102734375\n",
            "Train[33/50][164/240] Loss: 0.10863357782363892, BER : 0.0404453125\n",
            "Train[33/50][165/240] Loss: 0.10881908982992172, BER : 0.04062109375\n",
            "Train[33/50][166/240] Loss: 0.10746664553880692, BER : 0.0398125\n",
            "Train[33/50][167/240] Loss: 0.10893413424491882, BER : 0.04129296875\n",
            "Train[33/50][168/240] Loss: 0.11085125803947449, BER : 0.04182421875\n",
            "Train[33/50][169/240] Loss: 0.10711480677127838, BER : 0.03941796875\n",
            "Train[33/50][170/240] Loss: 0.10982825607061386, BER : 0.04183203125\n",
            "Train[33/50][171/240] Loss: 0.10935857892036438, BER : 0.040796875\n",
            "Train[33/50][172/240] Loss: 0.10954594612121582, BER : 0.041046875\n",
            "Train[33/50][173/240] Loss: 0.11430806666612625, BER : 0.0434921875\n",
            "Train[33/50][174/240] Loss: 0.11041946709156036, BER : 0.04163671875\n",
            "Train[33/50][175/240] Loss: 0.1099093109369278, BER : 0.04155859375\n",
            "Train[33/50][176/240] Loss: 0.10790421068668365, BER : 0.0407265625\n",
            "Train[33/50][177/240] Loss: 0.11014126241207123, BER : 0.0410546875\n",
            "Train[33/50][178/240] Loss: 0.11280643939971924, BER : 0.04303515625\n",
            "Train[33/50][179/240] Loss: 0.10971777141094208, BER : 0.0412265625\n",
            "Train[33/50][180/240] Loss: 0.10894443094730377, BER : 0.04075390625\n",
            "Train[33/50][181/240] Loss: 0.10997026413679123, BER : 0.04140625\n",
            "Train[33/50][182/240] Loss: 0.11004732549190521, BER : 0.04177734375\n",
            "Train[33/50][183/240] Loss: 0.10948117077350616, BER : 0.04096875\n",
            "Train[33/50][184/240] Loss: 0.1091974675655365, BER : 0.0414140625\n",
            "Train[33/50][185/240] Loss: 0.11059188097715378, BER : 0.04190625\n",
            "Train[33/50][186/240] Loss: 0.11222393065690994, BER : 0.04270703125\n",
            "Train[33/50][187/240] Loss: 0.1103222519159317, BER : 0.0422265625\n",
            "Train[33/50][188/240] Loss: 0.10736328363418579, BER : 0.040453125\n",
            "Train[33/50][189/240] Loss: 0.11068601906299591, BER : 0.04201171875\n",
            "Train[33/50][190/240] Loss: 0.10875527560710907, BER : 0.04133203125\n",
            "Train[33/50][191/240] Loss: 0.11157877743244171, BER : 0.042\n",
            "Train[33/50][192/240] Loss: 0.11126221716403961, BER : 0.04302734375\n",
            "Train[33/50][193/240] Loss: 0.10879597812891006, BER : 0.04068359375\n",
            "Train[33/50][194/240] Loss: 0.1099826842546463, BER : 0.040890625\n",
            "Train[33/50][195/240] Loss: 0.10873858630657196, BER : 0.0409921875\n",
            "Train[33/50][196/240] Loss: 0.11112407594919205, BER : 0.0419765625\n",
            "Train[33/50][197/240] Loss: 0.1091461256146431, BER : 0.040859375\n",
            "Train[33/50][198/240] Loss: 0.10811439156532288, BER : 0.03996875\n",
            "Train[33/50][199/240] Loss: 0.1099163293838501, BER : 0.04219921875\n",
            "Train[33/50][200/240] Loss: 0.10808752477169037, BER : 0.04070703125\n",
            "Train[33/50][201/240] Loss: 0.1095789223909378, BER : 0.0413046875\n",
            "Train[33/50][202/240] Loss: 0.10876080393791199, BER : 0.04126171875\n",
            "Train[33/50][203/240] Loss: 0.10964863747358322, BER : 0.04171484375\n",
            "Train[33/50][204/240] Loss: 0.10750684142112732, BER : 0.04042578125\n",
            "Train[33/50][205/240] Loss: 0.10790207982063293, BER : 0.04046484375\n",
            "Train[33/50][206/240] Loss: 0.1072588711977005, BER : 0.039234375\n",
            "Train[33/50][207/240] Loss: 0.10997915267944336, BER : 0.04144140625\n",
            "Train[33/50][208/240] Loss: 0.10820946842432022, BER : 0.0398125\n",
            "Train[33/50][209/240] Loss: 0.10753223299980164, BER : 0.0404765625\n",
            "Train[33/50][210/240] Loss: 0.10952036082744598, BER : 0.0412421875\n",
            "Train[33/50][211/240] Loss: 0.11054877191781998, BER : 0.0410859375\n",
            "Train[33/50][212/240] Loss: 0.10916264355182648, BER : 0.041765625\n",
            "Train[33/50][213/240] Loss: 0.10955973714590073, BER : 0.0410546875\n",
            "Train[33/50][214/240] Loss: 0.11007347702980042, BER : 0.04176953125\n",
            "Train[33/50][215/240] Loss: 0.10977711528539658, BER : 0.04106640625\n",
            "Train[33/50][216/240] Loss: 0.10821114480495453, BER : 0.0411953125\n",
            "Train[33/50][217/240] Loss: 0.10973453521728516, BER : 0.04149609375\n",
            "Train[33/50][218/240] Loss: 0.11113235354423523, BER : 0.042171875\n",
            "Train[33/50][219/240] Loss: 0.11004141718149185, BER : 0.04188671875\n",
            "Train[33/50][220/240] Loss: 0.10586246848106384, BER : 0.038921875\n",
            "Train[33/50][221/240] Loss: 0.11026260256767273, BER : 0.04257421875\n",
            "Train[33/50][222/240] Loss: 0.1083226427435875, BER : 0.0402265625\n",
            "Train[33/50][223/240] Loss: 0.10854044556617737, BER : 0.04048046875\n",
            "Train[33/50][224/240] Loss: 0.11338981986045837, BER : 0.0426640625\n",
            "Train[33/50][225/240] Loss: 0.10780300945043564, BER : 0.04051953125\n",
            "Train[33/50][226/240] Loss: 0.11027552932500839, BER : 0.04206640625\n",
            "Train[33/50][227/240] Loss: 0.10851483047008514, BER : 0.04046875\n",
            "Train[33/50][228/240] Loss: 0.10977447032928467, BER : 0.04194140625\n",
            "Train[33/50][229/240] Loss: 0.10961424559354782, BER : 0.04212890625\n",
            "Train[33/50][230/240] Loss: 0.10918714851140976, BER : 0.04080859375\n",
            "Train[33/50][231/240] Loss: 0.1089627742767334, BER : 0.04159375\n",
            "Train[33/50][232/240] Loss: 0.10812342166900635, BER : 0.04026171875\n",
            "Train[33/50][233/240] Loss: 0.11095714569091797, BER : 0.04092578125\n",
            "Train[33/50][234/240] Loss: 0.10904364287853241, BER : 0.04045703125\n",
            "Train[33/50][235/240] Loss: 0.11231198161840439, BER : 0.043109375\n",
            "Train[33/50][236/240] Loss: 0.10807892680168152, BER : 0.04059765625\n",
            "Train[33/50][237/240] Loss: 0.11109523475170135, BER : 0.04223828125\n",
            "Train[33/50][238/240] Loss: 0.11324656009674072, BER : 0.042796875\n",
            "Train[33/50][239/240] Loss: 0.10891862958669662, BER : 0.04114453125\n",
            "Epoch [33/50] Train_Avg_loss(epoch): 0.30058\n",
            "-------------------------------------\n",
            "Test[33/50][0/40]  Loss: 0.07405294477939606, BER (test): 0.03963671875\n",
            "Test[33/50][1/40]  Loss: 0.07765787094831467, BER (test): 0.04175390625\n",
            "Test[33/50][2/40]  Loss: 0.07834789156913757, BER (test): 0.0418125\n",
            "Test[33/50][3/40]  Loss: 0.0762571468949318, BER (test): 0.0411796875\n",
            "Test[33/50][4/40]  Loss: 0.07497765123844147, BER (test): 0.040625\n",
            "Test[33/50][5/40]  Loss: 0.07582511007785797, BER (test): 0.04084765625\n",
            "Test[33/50][6/40]  Loss: 0.07724138349294662, BER (test): 0.04175390625\n",
            "Test[33/50][7/40]  Loss: 0.0808693915605545, BER (test): 0.0435390625\n",
            "Test[33/50][8/40]  Loss: 0.07892265915870667, BER (test): 0.04285546875\n",
            "Test[33/50][9/40]  Loss: 0.07699581235647202, BER (test): 0.04171484375\n",
            "Test[33/50][10/40]  Loss: 0.07679706811904907, BER (test): 0.0411796875\n",
            "Test[33/50][11/40]  Loss: 0.07857966423034668, BER (test): 0.0423203125\n",
            "Test[33/50][12/40]  Loss: 0.07754788547754288, BER (test): 0.0422109375\n",
            "Test[33/50][13/40]  Loss: 0.07768656313419342, BER (test): 0.04215234375\n",
            "Test[33/50][14/40]  Loss: 0.07708346098661423, BER (test): 0.0417265625\n",
            "Test[33/50][15/40]  Loss: 0.07959869503974915, BER (test): 0.04298828125\n",
            "Test[33/50][16/40]  Loss: 0.07821279764175415, BER (test): 0.04252734375\n",
            "Test[33/50][17/40]  Loss: 0.07596012204885483, BER (test): 0.0408828125\n",
            "Test[33/50][18/40]  Loss: 0.07787961512804031, BER (test): 0.0421953125\n",
            "Test[33/50][19/40]  Loss: 0.0780499204993248, BER (test): 0.04198046875\n",
            "Test[33/50][20/40]  Loss: 0.07733289897441864, BER (test): 0.04239453125\n",
            "Test[33/50][21/40]  Loss: 0.07792039960622787, BER (test): 0.042046875\n",
            "Test[33/50][22/40]  Loss: 0.07856924831867218, BER (test): 0.04207421875\n",
            "Test[33/50][23/40]  Loss: 0.07680860161781311, BER (test): 0.04091015625\n",
            "Test[33/50][24/40]  Loss: 0.07684462517499924, BER (test): 0.04163671875\n",
            "Test[33/50][25/40]  Loss: 0.075223907828331, BER (test): 0.0398984375\n",
            "Test[33/50][26/40]  Loss: 0.07796507328748703, BER (test): 0.04237890625\n",
            "Test[33/50][27/40]  Loss: 0.07728050649166107, BER (test): 0.04126953125\n",
            "Test[33/50][28/40]  Loss: 0.07835642248392105, BER (test): 0.04226953125\n",
            "Test[33/50][29/40]  Loss: 0.07474732398986816, BER (test): 0.04108984375\n",
            "Test[33/50][30/40]  Loss: 0.07635969668626785, BER (test): 0.04155078125\n",
            "Test[33/50][31/40]  Loss: 0.08103647828102112, BER (test): 0.04380859375\n",
            "Test[33/50][32/40]  Loss: 0.07688332349061966, BER (test): 0.04141796875\n",
            "Test[33/50][33/40]  Loss: 0.07806843519210815, BER (test): 0.0428515625\n",
            "Test[33/50][34/40]  Loss: 0.07864832133054733, BER (test): 0.04220703125\n",
            "Test[33/50][35/40]  Loss: 0.0761818215250969, BER (test): 0.0410078125\n",
            "Test[33/50][36/40]  Loss: 0.07806708663702011, BER (test): 0.042328125\n",
            "Test[33/50][37/40]  Loss: 0.07645232230424881, BER (test): 0.040875\n",
            "Test[33/50][38/40]  Loss: 0.07774379849433899, BER (test): 0.04159375\n",
            "Test[33/50][39/40]  Loss: 0.07914751768112183, BER (test): 0.0433671875\n",
            "Test_Epoch [33/50] Test_Avg_loss(epoch): 0.07745\n",
            "Train[34/50][0/240] Loss: 0.11038576066493988, BER : 0.0422578125\n",
            "Train[34/50][1/240] Loss: 0.11163867264986038, BER : 0.042578125\n",
            "Train[34/50][2/240] Loss: 0.10818248987197876, BER : 0.040765625\n",
            "Train[34/50][3/240] Loss: 0.11055000871419907, BER : 0.041625\n",
            "Train[34/50][4/240] Loss: 0.11001481115818024, BER : 0.04221484375\n",
            "Train[34/50][5/240] Loss: 0.10583853721618652, BER : 0.0392890625\n",
            "Train[34/50][6/240] Loss: 0.11082712560892105, BER : 0.04133984375\n",
            "Train[34/50][7/240] Loss: 0.10900695621967316, BER : 0.04068359375\n",
            "Train[34/50][8/240] Loss: 0.10947370529174805, BER : 0.04123828125\n",
            "Train[34/50][9/240] Loss: 0.10918152332305908, BER : 0.04090234375\n",
            "Train[34/50][10/240] Loss: 0.10829717665910721, BER : 0.0404375\n",
            "Train[34/50][11/240] Loss: 0.11112511903047562, BER : 0.04249609375\n",
            "Train[34/50][12/240] Loss: 0.11303868144750595, BER : 0.04324609375\n",
            "Train[34/50][13/240] Loss: 0.10954059660434723, BER : 0.04035546875\n",
            "Train[34/50][14/240] Loss: 0.11097311973571777, BER : 0.0424765625\n",
            "Train[34/50][15/240] Loss: 0.10820687562227249, BER : 0.04044921875\n",
            "Train[34/50][16/240] Loss: 0.10944442451000214, BER : 0.04120703125\n",
            "Train[34/50][17/240] Loss: 0.10797405242919922, BER : 0.040984375\n",
            "Train[34/50][18/240] Loss: 0.10977859795093536, BER : 0.0416875\n",
            "Train[34/50][19/240] Loss: 0.10699576139450073, BER : 0.04055078125\n",
            "Train[34/50][20/240] Loss: 0.11030897498130798, BER : 0.04065234375\n",
            "Train[34/50][21/240] Loss: 0.10715524852275848, BER : 0.039578125\n",
            "Train[34/50][22/240] Loss: 0.11003211885690689, BER : 0.0415\n",
            "Train[34/50][23/240] Loss: 0.10888763517141342, BER : 0.04121484375\n",
            "Train[34/50][24/240] Loss: 0.10780255496501923, BER : 0.04021484375\n",
            "Train[34/50][25/240] Loss: 0.10802567005157471, BER : 0.04065234375\n",
            "Train[34/50][26/240] Loss: 0.11255933344364166, BER : 0.04248828125\n",
            "Train[34/50][27/240] Loss: 0.108645498752594, BER : 0.04080859375\n",
            "Train[34/50][28/240] Loss: 0.11032307147979736, BER : 0.041515625\n",
            "Train[34/50][29/240] Loss: 0.11251592636108398, BER : 0.04249609375\n",
            "Train[34/50][30/240] Loss: 0.10988643765449524, BER : 0.04109765625\n",
            "Train[34/50][31/240] Loss: 0.11147674918174744, BER : 0.042859375\n",
            "Train[34/50][32/240] Loss: 0.10839243233203888, BER : 0.04064453125\n",
            "Train[34/50][33/240] Loss: 0.10965532809495926, BER : 0.04120703125\n",
            "Train[34/50][34/240] Loss: 0.1109880656003952, BER : 0.0408671875\n",
            "Train[34/50][35/240] Loss: 0.10918465256690979, BER : 0.04057421875\n",
            "Train[34/50][36/240] Loss: 0.10681181401014328, BER : 0.03959375\n",
            "Train[34/50][37/240] Loss: 0.11230485141277313, BER : 0.04288671875\n",
            "Train[34/50][38/240] Loss: 0.10675546526908875, BER : 0.03949609375\n",
            "Train[34/50][39/240] Loss: 0.11186686158180237, BER : 0.04255859375\n",
            "Train[34/50][40/240] Loss: 0.1104351058602333, BER : 0.04156640625\n",
            "Train[34/50][41/240] Loss: 0.1084517315030098, BER : 0.040265625\n",
            "Train[34/50][42/240] Loss: 0.10601131618022919, BER : 0.0390078125\n",
            "Train[34/50][43/240] Loss: 0.11189982295036316, BER : 0.04215234375\n",
            "Train[34/50][44/240] Loss: 0.11003271490335464, BER : 0.0420234375\n",
            "Train[34/50][45/240] Loss: 0.10797753185033798, BER : 0.03947265625\n",
            "Train[34/50][46/240] Loss: 0.10843288898468018, BER : 0.0409921875\n",
            "Train[34/50][47/240] Loss: 0.10819873213768005, BER : 0.04080078125\n",
            "Train[34/50][48/240] Loss: 0.10904402285814285, BER : 0.04066015625\n",
            "Train[34/50][49/240] Loss: 0.10797452181577682, BER : 0.04057421875\n",
            "Train[34/50][50/240] Loss: 0.10866183042526245, BER : 0.04057421875\n",
            "Train[34/50][51/240] Loss: 0.10565409064292908, BER : 0.03925390625\n",
            "Train[34/50][52/240] Loss: 0.10906599462032318, BER : 0.04053125\n",
            "Train[34/50][53/240] Loss: 0.10686766356229782, BER : 0.040625\n",
            "Train[34/50][54/240] Loss: 0.1100555956363678, BER : 0.041359375\n",
            "Train[34/50][55/240] Loss: 0.10866224765777588, BER : 0.04053515625\n",
            "Train[34/50][56/240] Loss: 0.11042936891317368, BER : 0.0418828125\n",
            "Train[34/50][57/240] Loss: 0.10668810456991196, BER : 0.04000390625\n",
            "Train[34/50][58/240] Loss: 0.10910338908433914, BER : 0.041625\n",
            "Train[34/50][59/240] Loss: 0.10613179951906204, BER : 0.03933984375\n",
            "Train[34/50][60/240] Loss: 0.10806246101856232, BER : 0.040640625\n",
            "Train[34/50][61/240] Loss: 0.10703463107347488, BER : 0.04008203125\n",
            "Train[34/50][62/240] Loss: 0.1064232587814331, BER : 0.03918359375\n",
            "Train[34/50][63/240] Loss: 0.1086568534374237, BER : 0.04136328125\n",
            "Train[34/50][64/240] Loss: 0.10876470059156418, BER : 0.04137109375\n",
            "Train[34/50][65/240] Loss: 0.10950246453285217, BER : 0.0412734375\n",
            "Train[34/50][66/240] Loss: 0.10764050483703613, BER : 0.04059765625\n",
            "Train[34/50][67/240] Loss: 0.10881738364696503, BER : 0.04053125\n",
            "Train[34/50][68/240] Loss: 0.10980083793401718, BER : 0.04198046875\n",
            "Train[34/50][69/240] Loss: 0.10886205732822418, BER : 0.0411640625\n",
            "Train[34/50][70/240] Loss: 0.10823513567447662, BER : 0.04033984375\n",
            "Train[34/50][71/240] Loss: 0.11026746034622192, BER : 0.0409296875\n",
            "Train[34/50][72/240] Loss: 0.1084861233830452, BER : 0.04071484375\n",
            "Train[34/50][73/240] Loss: 0.10655861347913742, BER : 0.04001953125\n",
            "Train[34/50][74/240] Loss: 0.10888305306434631, BER : 0.04142578125\n",
            "Train[34/50][75/240] Loss: 0.10895320028066635, BER : 0.04015625\n",
            "Train[34/50][76/240] Loss: 0.1089617982506752, BER : 0.04141015625\n",
            "Train[34/50][77/240] Loss: 0.11028623580932617, BER : 0.0424609375\n",
            "Train[34/50][78/240] Loss: 0.11049950122833252, BER : 0.04252734375\n",
            "Train[34/50][79/240] Loss: 0.10821786522865295, BER : 0.04087109375\n",
            "Train[34/50][80/240] Loss: 0.10995504260063171, BER : 0.0418828125\n",
            "Train[34/50][81/240] Loss: 0.11203651130199432, BER : 0.04244921875\n",
            "Train[34/50][82/240] Loss: 0.1102137565612793, BER : 0.0426015625\n",
            "Train[34/50][83/240] Loss: 0.10792358964681625, BER : 0.04003125\n",
            "Train[34/50][84/240] Loss: 0.10978929698467255, BER : 0.04167578125\n",
            "Train[34/50][85/240] Loss: 0.10790133476257324, BER : 0.04051171875\n",
            "Train[34/50][86/240] Loss: 0.10950252413749695, BER : 0.0411640625\n",
            "Train[34/50][87/240] Loss: 0.10876219719648361, BER : 0.04060546875\n",
            "Train[34/50][88/240] Loss: 0.10713225603103638, BER : 0.03928515625\n",
            "Train[34/50][89/240] Loss: 0.1080898866057396, BER : 0.04117578125\n",
            "Train[34/50][90/240] Loss: 0.10746437311172485, BER : 0.04006640625\n",
            "Train[34/50][91/240] Loss: 0.11090480536222458, BER : 0.04203125\n",
            "Train[34/50][92/240] Loss: 0.10756714642047882, BER : 0.04070703125\n",
            "Train[34/50][93/240] Loss: 0.10715465247631073, BER : 0.03998046875\n",
            "Train[34/50][94/240] Loss: 0.1092943474650383, BER : 0.0417265625\n",
            "Train[34/50][95/240] Loss: 0.11152394115924835, BER : 0.04191796875\n",
            "Train[34/50][96/240] Loss: 0.10999727249145508, BER : 0.04116796875\n",
            "Train[34/50][97/240] Loss: 0.11182025820016861, BER : 0.0424921875\n",
            "Train[34/50][98/240] Loss: 0.10862068086862564, BER : 0.0403203125\n",
            "Train[34/50][99/240] Loss: 0.11072628200054169, BER : 0.04172265625\n",
            "Train[34/50][100/240] Loss: 0.10906025022268295, BER : 0.04046484375\n",
            "Train[34/50][101/240] Loss: 0.10746021568775177, BER : 0.040171875\n",
            "Train[34/50][102/240] Loss: 0.10943706333637238, BER : 0.04056640625\n",
            "Train[34/50][103/240] Loss: 0.1076069101691246, BER : 0.039953125\n",
            "Train[34/50][104/240] Loss: 0.10708584636449814, BER : 0.04000390625\n",
            "Train[34/50][105/240] Loss: 0.10875077545642853, BER : 0.04122265625\n",
            "Train[34/50][106/240] Loss: 0.11188499629497528, BER : 0.042640625\n",
            "Train[34/50][107/240] Loss: 0.11067923158407211, BER : 0.04165234375\n",
            "Train[34/50][108/240] Loss: 0.10831248760223389, BER : 0.04046875\n",
            "Train[34/50][109/240] Loss: 0.1099809929728508, BER : 0.04158203125\n",
            "Train[34/50][110/240] Loss: 0.10608921945095062, BER : 0.0394296875\n",
            "Train[34/50][111/240] Loss: 0.10887634754180908, BER : 0.0410078125\n",
            "Train[34/50][112/240] Loss: 0.10908331722021103, BER : 0.0410546875\n",
            "Train[34/50][113/240] Loss: 0.10799455642700195, BER : 0.04062109375\n",
            "Train[34/50][114/240] Loss: 0.10770189762115479, BER : 0.04019921875\n",
            "Train[34/50][115/240] Loss: 0.11004463583230972, BER : 0.04188671875\n",
            "Train[34/50][116/240] Loss: 0.11035267263650894, BER : 0.0415859375\n",
            "Train[34/50][117/240] Loss: 0.10745899379253387, BER : 0.0411796875\n",
            "Train[34/50][118/240] Loss: 0.10666897892951965, BER : 0.03976171875\n",
            "Train[34/50][119/240] Loss: 0.1068730503320694, BER : 0.03984375\n",
            "Train[34/50][120/240] Loss: 0.1065589189529419, BER : 0.04002734375\n",
            "Train[34/50][121/240] Loss: 0.11013931035995483, BER : 0.04200390625\n",
            "Train[34/50][122/240] Loss: 0.11222102493047714, BER : 0.0428046875\n",
            "Train[34/50][123/240] Loss: 0.10777904093265533, BER : 0.04004296875\n",
            "Train[34/50][124/240] Loss: 0.10849401354789734, BER : 0.0406328125\n",
            "Train[34/50][125/240] Loss: 0.10964293032884598, BER : 0.041421875\n",
            "Train[34/50][126/240] Loss: 0.11128399521112442, BER : 0.04265625\n",
            "Train[34/50][127/240] Loss: 0.10699570924043655, BER : 0.03935546875\n",
            "Train[34/50][128/240] Loss: 0.10880899429321289, BER : 0.0409296875\n",
            "Train[34/50][129/240] Loss: 0.1077398955821991, BER : 0.04019140625\n",
            "Train[34/50][130/240] Loss: 0.11338181048631668, BER : 0.04308984375\n",
            "Train[34/50][131/240] Loss: 0.10741785168647766, BER : 0.039625\n",
            "Train[34/50][132/240] Loss: 0.11019476503133774, BER : 0.04218359375\n",
            "Train[34/50][133/240] Loss: 0.11057604104280472, BER : 0.04159375\n",
            "Train[34/50][134/240] Loss: 0.10740232467651367, BER : 0.03968359375\n",
            "Train[34/50][135/240] Loss: 0.11099335551261902, BER : 0.0420625\n",
            "Train[34/50][136/240] Loss: 0.1067081093788147, BER : 0.03978125\n",
            "Train[34/50][137/240] Loss: 0.10605047643184662, BER : 0.0394375\n",
            "Train[34/50][138/240] Loss: 0.10993427783250809, BER : 0.0408671875\n",
            "Train[34/50][139/240] Loss: 0.10603243112564087, BER : 0.0391171875\n",
            "Train[34/50][140/240] Loss: 0.10587649047374725, BER : 0.0387265625\n",
            "Train[34/50][141/240] Loss: 0.10706513375043869, BER : 0.03953515625\n",
            "Train[34/50][142/240] Loss: 0.10930407792329788, BER : 0.04071875\n",
            "Train[34/50][143/240] Loss: 0.1109408587217331, BER : 0.0421015625\n",
            "Train[34/50][144/240] Loss: 0.10618412494659424, BER : 0.03966015625\n",
            "Train[34/50][145/240] Loss: 0.10831935703754425, BER : 0.0408984375\n",
            "Train[34/50][146/240] Loss: 0.10557196289300919, BER : 0.03940625\n",
            "Train[34/50][147/240] Loss: 0.10825765132904053, BER : 0.0407578125\n",
            "Train[34/50][148/240] Loss: 0.10851084440946579, BER : 0.04118359375\n",
            "Train[34/50][149/240] Loss: 0.1102701872587204, BER : 0.04216796875\n",
            "Train[34/50][150/240] Loss: 0.10724414139986038, BER : 0.03999609375\n",
            "Train[34/50][151/240] Loss: 0.11032000929117203, BER : 0.04215234375\n",
            "Train[34/50][152/240] Loss: 0.10821323841810226, BER : 0.04125390625\n",
            "Train[34/50][153/240] Loss: 0.10916130244731903, BER : 0.04105859375\n",
            "Train[34/50][154/240] Loss: 0.10370483994483948, BER : 0.038859375\n",
            "Train[34/50][155/240] Loss: 0.1092647910118103, BER : 0.0409921875\n",
            "Train[34/50][156/240] Loss: 0.10745571553707123, BER : 0.04019140625\n",
            "Train[34/50][157/240] Loss: 0.10722237825393677, BER : 0.04006640625\n",
            "Train[34/50][158/240] Loss: 0.11154591292142868, BER : 0.04274609375\n",
            "Train[34/50][159/240] Loss: 0.10967271029949188, BER : 0.0415859375\n",
            "Train[34/50][160/240] Loss: 0.11026088893413544, BER : 0.04209375\n",
            "Train[34/50][161/240] Loss: 0.10819683969020844, BER : 0.04087890625\n",
            "Train[34/50][162/240] Loss: 0.11051477491855621, BER : 0.0421328125\n",
            "Train[34/50][163/240] Loss: 0.10875152051448822, BER : 0.04096875\n",
            "Train[34/50][164/240] Loss: 0.10889609158039093, BER : 0.04029296875\n",
            "Train[34/50][165/240] Loss: 0.11177513748407364, BER : 0.04272265625\n",
            "Train[34/50][166/240] Loss: 0.10744496434926987, BER : 0.04014453125\n",
            "Train[34/50][167/240] Loss: 0.10764465481042862, BER : 0.03997265625\n",
            "Train[34/50][168/240] Loss: 0.10753518342971802, BER : 0.03977734375\n",
            "Train[34/50][169/240] Loss: 0.10958819836378098, BER : 0.04123046875\n",
            "Train[34/50][170/240] Loss: 0.10840476304292679, BER : 0.0412578125\n",
            "Train[34/50][171/240] Loss: 0.10803677141666412, BER : 0.0400390625\n",
            "Train[34/50][172/240] Loss: 0.11081425845623016, BER : 0.04115625\n",
            "Train[34/50][173/240] Loss: 0.10993585735559464, BER : 0.04158203125\n",
            "Train[34/50][174/240] Loss: 0.11211861670017242, BER : 0.043546875\n",
            "Train[34/50][175/240] Loss: 0.10861770808696747, BER : 0.04137109375\n",
            "Train[34/50][176/240] Loss: 0.10689045488834381, BER : 0.040234375\n",
            "Train[34/50][177/240] Loss: 0.11122782528400421, BER : 0.0411953125\n",
            "Train[34/50][178/240] Loss: 0.10876926779747009, BER : 0.04071875\n",
            "Train[34/50][179/240] Loss: 0.10973044484853745, BER : 0.04201953125\n",
            "Train[34/50][180/240] Loss: 0.11268694698810577, BER : 0.04337890625\n",
            "Train[34/50][181/240] Loss: 0.10870043188333511, BER : 0.04108984375\n",
            "Train[34/50][182/240] Loss: 0.11066508293151855, BER : 0.04217578125\n",
            "Train[34/50][183/240] Loss: 0.10820877552032471, BER : 0.04073046875\n",
            "Train[34/50][184/240] Loss: 0.110136479139328, BER : 0.04133984375\n",
            "Train[34/50][185/240] Loss: 0.10946615040302277, BER : 0.04134375\n",
            "Train[34/50][186/240] Loss: 0.11217409372329712, BER : 0.04304296875\n",
            "Train[34/50][187/240] Loss: 0.10984404385089874, BER : 0.04099609375\n",
            "Train[34/50][188/240] Loss: 0.10830745100975037, BER : 0.03991015625\n",
            "Train[34/50][189/240] Loss: 0.10824932903051376, BER : 0.0410234375\n",
            "Train[34/50][190/240] Loss: 0.10713560879230499, BER : 0.03962109375\n",
            "Train[34/50][191/240] Loss: 0.10743065923452377, BER : 0.03983203125\n",
            "Train[34/50][192/240] Loss: 0.1074819266796112, BER : 0.04051171875\n",
            "Train[34/50][193/240] Loss: 0.1105741336941719, BER : 0.04204296875\n",
            "Train[34/50][194/240] Loss: 0.10607296228408813, BER : 0.03948828125\n",
            "Train[34/50][195/240] Loss: 0.10885410755872726, BER : 0.04139453125\n",
            "Train[34/50][196/240] Loss: 0.10857565701007843, BER : 0.04023828125\n",
            "Train[34/50][197/240] Loss: 0.11083189398050308, BER : 0.0414921875\n",
            "Train[34/50][198/240] Loss: 0.10943837463855743, BER : 0.04109765625\n",
            "Train[34/50][199/240] Loss: 0.10832449793815613, BER : 0.0403515625\n",
            "Train[34/50][200/240] Loss: 0.10681797564029694, BER : 0.04021484375\n",
            "Train[34/50][201/240] Loss: 0.10802234709262848, BER : 0.0397578125\n",
            "Train[34/50][202/240] Loss: 0.11088827252388, BER : 0.04268359375\n",
            "Train[34/50][203/240] Loss: 0.10673153400421143, BER : 0.03937109375\n",
            "Train[34/50][204/240] Loss: 0.10932089388370514, BER : 0.04157421875\n",
            "Train[34/50][205/240] Loss: 0.10855746269226074, BER : 0.04123828125\n",
            "Train[34/50][206/240] Loss: 0.10856346786022186, BER : 0.04112890625\n",
            "Train[34/50][207/240] Loss: 0.10702192783355713, BER : 0.03966796875\n",
            "Train[34/50][208/240] Loss: 0.10888031125068665, BER : 0.04081640625\n",
            "Train[34/50][209/240] Loss: 0.11214446276426315, BER : 0.042765625\n",
            "Train[34/50][210/240] Loss: 0.10720110684633255, BER : 0.04021484375\n",
            "Train[34/50][211/240] Loss: 0.11029239743947983, BER : 0.0412734375\n",
            "Train[34/50][212/240] Loss: 0.10612451285123825, BER : 0.038984375\n",
            "Train[34/50][213/240] Loss: 0.11011692881584167, BER : 0.0419609375\n",
            "Train[34/50][214/240] Loss: 0.10917115211486816, BER : 0.04116796875\n",
            "Train[34/50][215/240] Loss: 0.10713529586791992, BER : 0.04023828125\n",
            "Train[34/50][216/240] Loss: 0.10833758115768433, BER : 0.04075\n",
            "Train[34/50][217/240] Loss: 0.10829693078994751, BER : 0.0405\n",
            "Train[34/50][218/240] Loss: 0.1117919385433197, BER : 0.042796875\n",
            "Train[34/50][219/240] Loss: 0.10905063152313232, BER : 0.04087890625\n",
            "Train[34/50][220/240] Loss: 0.10638975352048874, BER : 0.039859375\n",
            "Train[34/50][221/240] Loss: 0.11165709048509598, BER : 0.04278515625\n",
            "Train[34/50][222/240] Loss: 0.10460375249385834, BER : 0.03819921875\n",
            "Train[34/50][223/240] Loss: 0.10865932703018188, BER : 0.041390625\n",
            "Train[34/50][224/240] Loss: 0.10751106590032578, BER : 0.0407578125\n",
            "Train[34/50][225/240] Loss: 0.1082783043384552, BER : 0.0410390625\n",
            "Train[34/50][226/240] Loss: 0.11086288839578629, BER : 0.0422734375\n",
            "Train[34/50][227/240] Loss: 0.1089121475815773, BER : 0.0417578125\n",
            "Train[34/50][228/240] Loss: 0.1091993898153305, BER : 0.0409140625\n",
            "Train[34/50][229/240] Loss: 0.10681752860546112, BER : 0.03945703125\n",
            "Train[34/50][230/240] Loss: 0.10864586383104324, BER : 0.04051171875\n",
            "Train[34/50][231/240] Loss: 0.10750044137239456, BER : 0.04015625\n",
            "Train[34/50][232/240] Loss: 0.10909222811460495, BER : 0.04119921875\n",
            "Train[34/50][233/240] Loss: 0.11174824088811874, BER : 0.04275390625\n",
            "Train[34/50][234/240] Loss: 0.11123743653297424, BER : 0.04209375\n",
            "Train[34/50][235/240] Loss: 0.10893832892179489, BER : 0.04108203125\n",
            "Train[34/50][236/240] Loss: 0.10973727703094482, BER : 0.04124609375\n",
            "Train[34/50][237/240] Loss: 0.10843294858932495, BER : 0.0409453125\n",
            "Train[34/50][238/240] Loss: 0.11125528812408447, BER : 0.04178515625\n",
            "Train[34/50][239/240] Loss: 0.11096671968698502, BER : 0.042546875\n",
            "Epoch [34/50] Train_Avg_loss(epoch): 0.29511\n",
            "-------------------------------------\n",
            "Test[34/50][0/40]  Loss: 0.0784197449684143, BER (test): 0.04185546875\n",
            "Test[34/50][1/40]  Loss: 0.07872617989778519, BER (test): 0.042953125\n",
            "Test[34/50][2/40]  Loss: 0.0788554698228836, BER (test): 0.04215234375\n",
            "Test[34/50][3/40]  Loss: 0.07783575356006622, BER (test): 0.04177734375\n",
            "Test[34/50][4/40]  Loss: 0.07897277176380157, BER (test): 0.04265625\n",
            "Test[34/50][5/40]  Loss: 0.07863582670688629, BER (test): 0.04262890625\n",
            "Test[34/50][6/40]  Loss: 0.07822126895189285, BER (test): 0.0423359375\n",
            "Test[34/50][7/40]  Loss: 0.07603643834590912, BER (test): 0.04064453125\n",
            "Test[34/50][8/40]  Loss: 0.07769546657800674, BER (test): 0.0417109375\n",
            "Test[34/50][9/40]  Loss: 0.07878164947032928, BER (test): 0.0428828125\n",
            "Test[34/50][10/40]  Loss: 0.07679358124732971, BER (test): 0.04176953125\n",
            "Test[34/50][11/40]  Loss: 0.0782008171081543, BER (test): 0.0423671875\n",
            "Test[34/50][12/40]  Loss: 0.07817594707012177, BER (test): 0.04226953125\n",
            "Test[34/50][13/40]  Loss: 0.07921632379293442, BER (test): 0.04358203125\n",
            "Test[34/50][14/40]  Loss: 0.0783778578042984, BER (test): 0.0422265625\n",
            "Test[34/50][15/40]  Loss: 0.07820241898298264, BER (test): 0.042234375\n",
            "Test[34/50][16/40]  Loss: 0.07785013318061829, BER (test): 0.04233984375\n",
            "Test[34/50][17/40]  Loss: 0.08191265165805817, BER (test): 0.04428515625\n",
            "Test[34/50][18/40]  Loss: 0.07836759090423584, BER (test): 0.0430546875\n",
            "Test[34/50][19/40]  Loss: 0.07927842438220978, BER (test): 0.04328515625\n",
            "Test[34/50][20/40]  Loss: 0.07433924078941345, BER (test): 0.03985546875\n",
            "Test[34/50][21/40]  Loss: 0.08008070290088654, BER (test): 0.043609375\n",
            "Test[34/50][22/40]  Loss: 0.07660023123025894, BER (test): 0.04165234375\n",
            "Test[34/50][23/40]  Loss: 0.08136015385389328, BER (test): 0.0431328125\n",
            "Test[34/50][24/40]  Loss: 0.07662076503038406, BER (test): 0.0413671875\n",
            "Test[34/50][25/40]  Loss: 0.07809431850910187, BER (test): 0.042703125\n",
            "Test[34/50][26/40]  Loss: 0.07786115258932114, BER (test): 0.04246875\n",
            "Test[34/50][27/40]  Loss: 0.07759818434715271, BER (test): 0.04258203125\n",
            "Test[34/50][28/40]  Loss: 0.07565096765756607, BER (test): 0.04076171875\n",
            "Test[34/50][29/40]  Loss: 0.0778895914554596, BER (test): 0.04190625\n",
            "Test[34/50][30/40]  Loss: 0.0770859643816948, BER (test): 0.04198828125\n",
            "Test[34/50][31/40]  Loss: 0.07660012692213058, BER (test): 0.041515625\n",
            "Test[34/50][32/40]  Loss: 0.07825738191604614, BER (test): 0.04337109375\n",
            "Test[34/50][33/40]  Loss: 0.07634901255369186, BER (test): 0.04135546875\n",
            "Test[34/50][34/40]  Loss: 0.07773321866989136, BER (test): 0.0421796875\n",
            "Test[34/50][35/40]  Loss: 0.07702536135911942, BER (test): 0.0422421875\n",
            "Test[34/50][36/40]  Loss: 0.07687180489301682, BER (test): 0.04153125\n",
            "Test[34/50][37/40]  Loss: 0.07858112454414368, BER (test): 0.04230078125\n",
            "Test[34/50][38/40]  Loss: 0.0782707929611206, BER (test): 0.04215625\n",
            "Test[34/50][39/40]  Loss: 0.07815992832183838, BER (test): 0.04208984375\n",
            "Test_Epoch [34/50] Test_Avg_loss(epoch): 0.07799\n",
            "Train[35/50][0/240] Loss: 0.10856375098228455, BER : 0.04123828125\n",
            "Train[35/50][1/240] Loss: 0.1080208420753479, BER : 0.04044921875\n",
            "Train[35/50][2/240] Loss: 0.10765856504440308, BER : 0.0409609375\n",
            "Train[35/50][3/240] Loss: 0.10865454375743866, BER : 0.0409375\n",
            "Train[35/50][4/240] Loss: 0.10508757084608078, BER : 0.0394140625\n",
            "Train[35/50][5/240] Loss: 0.11356078088283539, BER : 0.04298828125\n",
            "Train[35/50][6/240] Loss: 0.10707146674394608, BER : 0.040421875\n",
            "Train[35/50][7/240] Loss: 0.1060606837272644, BER : 0.039859375\n",
            "Train[35/50][8/240] Loss: 0.10521889477968216, BER : 0.03896875\n",
            "Train[35/50][9/240] Loss: 0.10569516569375992, BER : 0.039578125\n",
            "Train[35/50][10/240] Loss: 0.10966011881828308, BER : 0.04104296875\n",
            "Train[35/50][11/240] Loss: 0.10884106159210205, BER : 0.0409453125\n",
            "Train[35/50][12/240] Loss: 0.10787174105644226, BER : 0.04092578125\n",
            "Train[35/50][13/240] Loss: 0.10997262597084045, BER : 0.04169140625\n",
            "Train[35/50][14/240] Loss: 0.1084149032831192, BER : 0.04119921875\n",
            "Train[35/50][15/240] Loss: 0.10868729650974274, BER : 0.04165625\n",
            "Train[35/50][16/240] Loss: 0.10890639573335648, BER : 0.04134765625\n",
            "Train[35/50][17/240] Loss: 0.10798758268356323, BER : 0.04078515625\n",
            "Train[35/50][18/240] Loss: 0.10680194944143295, BER : 0.040140625\n",
            "Train[35/50][19/240] Loss: 0.10975813865661621, BER : 0.041265625\n",
            "Train[35/50][20/240] Loss: 0.1071963757276535, BER : 0.03990625\n",
            "Train[35/50][21/240] Loss: 0.11004196107387543, BER : 0.0415703125\n",
            "Train[35/50][22/240] Loss: 0.10583822429180145, BER : 0.03995703125\n",
            "Train[35/50][23/240] Loss: 0.10756151378154755, BER : 0.03981640625\n",
            "Train[35/50][24/240] Loss: 0.1100175529718399, BER : 0.0418203125\n",
            "Train[35/50][25/240] Loss: 0.10777722299098969, BER : 0.03967578125\n",
            "Train[35/50][26/240] Loss: 0.10751707851886749, BER : 0.039890625\n",
            "Train[35/50][27/240] Loss: 0.11051616072654724, BER : 0.04263671875\n",
            "Train[35/50][28/240] Loss: 0.10988098382949829, BER : 0.0413359375\n",
            "Train[35/50][29/240] Loss: 0.10721508413553238, BER : 0.04089453125\n",
            "Train[35/50][30/240] Loss: 0.10708785057067871, BER : 0.04019921875\n",
            "Train[35/50][31/240] Loss: 0.10854944586753845, BER : 0.04060546875\n",
            "Train[35/50][32/240] Loss: 0.10649755597114563, BER : 0.0398203125\n",
            "Train[35/50][33/240] Loss: 0.11106541752815247, BER : 0.04261328125\n",
            "Train[35/50][34/240] Loss: 0.1092001348733902, BER : 0.0416875\n",
            "Train[35/50][35/240] Loss: 0.10722927749156952, BER : 0.040265625\n",
            "Train[35/50][36/240] Loss: 0.10609716176986694, BER : 0.04034375\n",
            "Train[35/50][37/240] Loss: 0.10986341536045074, BER : 0.04194921875\n",
            "Train[35/50][38/240] Loss: 0.11103476583957672, BER : 0.04173828125\n",
            "Train[35/50][39/240] Loss: 0.10993745923042297, BER : 0.04194921875\n",
            "Train[35/50][40/240] Loss: 0.11055140197277069, BER : 0.04239453125\n",
            "Train[35/50][41/240] Loss: 0.1094372421503067, BER : 0.0414375\n",
            "Train[35/50][42/240] Loss: 0.10907948017120361, BER : 0.04157421875\n",
            "Train[35/50][43/240] Loss: 0.10823123157024384, BER : 0.04033203125\n",
            "Train[35/50][44/240] Loss: 0.10920807719230652, BER : 0.04128125\n",
            "Train[35/50][45/240] Loss: 0.10915765166282654, BER : 0.0413203125\n",
            "Train[35/50][46/240] Loss: 0.10725681483745575, BER : 0.04047265625\n",
            "Train[35/50][47/240] Loss: 0.10850633680820465, BER : 0.0406484375\n",
            "Train[35/50][48/240] Loss: 0.10855524241924286, BER : 0.04064453125\n",
            "Train[35/50][49/240] Loss: 0.10892539471387863, BER : 0.04117578125\n",
            "Train[35/50][50/240] Loss: 0.11149154603481293, BER : 0.04213671875\n",
            "Train[35/50][51/240] Loss: 0.1066831648349762, BER : 0.0400546875\n",
            "Train[35/50][52/240] Loss: 0.10999792069196701, BER : 0.0411328125\n",
            "Train[35/50][53/240] Loss: 0.10738369822502136, BER : 0.04036328125\n",
            "Train[35/50][54/240] Loss: 0.11004778742790222, BER : 0.04103515625\n",
            "Train[35/50][55/240] Loss: 0.11174330115318298, BER : 0.0423671875\n",
            "Train[35/50][56/240] Loss: 0.10708921402692795, BER : 0.0394140625\n",
            "Train[35/50][57/240] Loss: 0.1095123216509819, BER : 0.0412109375\n",
            "Train[35/50][58/240] Loss: 0.10934699326753616, BER : 0.0414921875\n",
            "Train[35/50][59/240] Loss: 0.10901015996932983, BER : 0.0404375\n",
            "Train[35/50][60/240] Loss: 0.11065781116485596, BER : 0.04227734375\n",
            "Train[35/50][61/240] Loss: 0.10793252289295197, BER : 0.041515625\n",
            "Train[35/50][62/240] Loss: 0.11103619635105133, BER : 0.0416171875\n",
            "Train[35/50][63/240] Loss: 0.1091172993183136, BER : 0.041\n",
            "Train[35/50][64/240] Loss: 0.10706324130296707, BER : 0.04012109375\n",
            "Train[35/50][65/240] Loss: 0.10904207825660706, BER : 0.041109375\n",
            "Train[35/50][66/240] Loss: 0.10969709604978561, BER : 0.04108203125\n",
            "Train[35/50][67/240] Loss: 0.10749691724777222, BER : 0.0403046875\n",
            "Train[35/50][68/240] Loss: 0.11108377575874329, BER : 0.0413828125\n",
            "Train[35/50][69/240] Loss: 0.10844103246927261, BER : 0.04041015625\n",
            "Train[35/50][70/240] Loss: 0.1101626604795456, BER : 0.0416953125\n",
            "Train[35/50][71/240] Loss: 0.10808844119310379, BER : 0.03994921875\n",
            "Train[35/50][72/240] Loss: 0.11031132191419601, BER : 0.04166796875\n",
            "Train[35/50][73/240] Loss: 0.10962347686290741, BER : 0.0411640625\n",
            "Train[35/50][74/240] Loss: 0.10597234964370728, BER : 0.03980078125\n",
            "Train[35/50][75/240] Loss: 0.10864920169115067, BER : 0.04075\n",
            "Train[35/50][76/240] Loss: 0.10955523699522018, BER : 0.040890625\n",
            "Train[35/50][77/240] Loss: 0.10882344841957092, BER : 0.0404140625\n",
            "Train[35/50][78/240] Loss: 0.10599847882986069, BER : 0.04013671875\n",
            "Train[35/50][79/240] Loss: 0.10768894851207733, BER : 0.04027734375\n",
            "Train[35/50][80/240] Loss: 0.11076633632183075, BER : 0.0421484375\n",
            "Train[35/50][81/240] Loss: 0.10832622647285461, BER : 0.0401171875\n",
            "Train[35/50][82/240] Loss: 0.10980609059333801, BER : 0.04049609375\n",
            "Train[35/50][83/240] Loss: 0.10863672196865082, BER : 0.04076953125\n",
            "Train[35/50][84/240] Loss: 0.10957743972539902, BER : 0.04213671875\n",
            "Train[35/50][85/240] Loss: 0.10566766560077667, BER : 0.0391875\n",
            "Train[35/50][86/240] Loss: 0.10776695609092712, BER : 0.04037890625\n",
            "Train[35/50][87/240] Loss: 0.11064179986715317, BER : 0.0422265625\n",
            "Train[35/50][88/240] Loss: 0.10987468808889389, BER : 0.04144140625\n",
            "Train[35/50][89/240] Loss: 0.1078905314207077, BER : 0.040828125\n",
            "Train[35/50][90/240] Loss: 0.10806958377361298, BER : 0.04054296875\n",
            "Train[35/50][91/240] Loss: 0.10590308904647827, BER : 0.0393515625\n",
            "Train[35/50][92/240] Loss: 0.11035381257534027, BER : 0.0415234375\n",
            "Train[35/50][93/240] Loss: 0.108075812458992, BER : 0.0401953125\n",
            "Train[35/50][94/240] Loss: 0.10887797176837921, BER : 0.0407109375\n",
            "Train[35/50][95/240] Loss: 0.10947498679161072, BER : 0.04126171875\n",
            "Train[35/50][96/240] Loss: 0.11128188669681549, BER : 0.0431640625\n",
            "Train[35/50][97/240] Loss: 0.10928306728601456, BER : 0.04133984375\n",
            "Train[35/50][98/240] Loss: 0.108797088265419, BER : 0.04124609375\n",
            "Train[35/50][99/240] Loss: 0.10864624381065369, BER : 0.04123046875\n",
            "Train[35/50][100/240] Loss: 0.11269741505384445, BER : 0.043046875\n",
            "Train[35/50][101/240] Loss: 0.10434100031852722, BER : 0.03915234375\n",
            "Train[35/50][102/240] Loss: 0.10540387779474258, BER : 0.03904296875\n",
            "Train[35/50][103/240] Loss: 0.10839495807886124, BER : 0.04076171875\n",
            "Train[35/50][104/240] Loss: 0.11085997521877289, BER : 0.0423359375\n",
            "Train[35/50][105/240] Loss: 0.10831640660762787, BER : 0.0406796875\n",
            "Train[35/50][106/240] Loss: 0.11327698826789856, BER : 0.04309375\n",
            "Train[35/50][107/240] Loss: 0.10846903920173645, BER : 0.0405859375\n",
            "Train[35/50][108/240] Loss: 0.11240766942501068, BER : 0.0435703125\n",
            "Train[35/50][109/240] Loss: 0.11165216565132141, BER : 0.04303125\n",
            "Train[35/50][110/240] Loss: 0.10701052099466324, BER : 0.0399453125\n",
            "Train[35/50][111/240] Loss: 0.1108514666557312, BER : 0.04162109375\n",
            "Train[35/50][112/240] Loss: 0.10878744721412659, BER : 0.0412421875\n",
            "Train[35/50][113/240] Loss: 0.10813596844673157, BER : 0.04084765625\n",
            "Train[35/50][114/240] Loss: 0.10555537045001984, BER : 0.03973828125\n",
            "Train[35/50][115/240] Loss: 0.10840927064418793, BER : 0.0403359375\n",
            "Train[35/50][116/240] Loss: 0.10736694931983948, BER : 0.039765625\n",
            "Train[35/50][117/240] Loss: 0.10861137509346008, BER : 0.03998046875\n",
            "Train[35/50][118/240] Loss: 0.10906460881233215, BER : 0.0410234375\n",
            "Train[35/50][119/240] Loss: 0.11009861528873444, BER : 0.04151171875\n",
            "Train[35/50][120/240] Loss: 0.11005590856075287, BER : 0.04060546875\n",
            "Train[35/50][121/240] Loss: 0.10975665599107742, BER : 0.04130078125\n",
            "Train[35/50][122/240] Loss: 0.10855010151863098, BER : 0.041453125\n",
            "Train[35/50][123/240] Loss: 0.10782033950090408, BER : 0.0406953125\n",
            "Train[35/50][124/240] Loss: 0.11189844459295273, BER : 0.0426640625\n",
            "Train[35/50][125/240] Loss: 0.10945666581392288, BER : 0.04182421875\n",
            "Train[35/50][126/240] Loss: 0.10864950716495514, BER : 0.040765625\n",
            "Train[35/50][127/240] Loss: 0.10705974698066711, BER : 0.0404375\n",
            "Train[35/50][128/240] Loss: 0.10853088647127151, BER : 0.0413125\n",
            "Train[35/50][129/240] Loss: 0.10772495716810226, BER : 0.040671875\n",
            "Train[35/50][130/240] Loss: 0.1084049716591835, BER : 0.04129296875\n",
            "Train[35/50][131/240] Loss: 0.1068926528096199, BER : 0.0402890625\n",
            "Train[35/50][132/240] Loss: 0.10789775848388672, BER : 0.04083203125\n",
            "Train[35/50][133/240] Loss: 0.10682952404022217, BER : 0.0397109375\n",
            "Train[35/50][134/240] Loss: 0.10595512390136719, BER : 0.0391015625\n",
            "Train[35/50][135/240] Loss: 0.10863596200942993, BER : 0.0408671875\n",
            "Train[35/50][136/240] Loss: 0.10906141251325607, BER : 0.0409921875\n",
            "Train[35/50][137/240] Loss: 0.10772402584552765, BER : 0.04026953125\n",
            "Train[35/50][138/240] Loss: 0.1091688945889473, BER : 0.04133203125\n",
            "Train[35/50][139/240] Loss: 0.10873251408338547, BER : 0.04069921875\n",
            "Train[35/50][140/240] Loss: 0.11042848974466324, BER : 0.04252734375\n",
            "Train[35/50][141/240] Loss: 0.1075521856546402, BER : 0.04049609375\n",
            "Train[35/50][142/240] Loss: 0.11004578322172165, BER : 0.0413125\n",
            "Train[35/50][143/240] Loss: 0.10631823539733887, BER : 0.03906640625\n",
            "Train[35/50][144/240] Loss: 0.10685809701681137, BER : 0.039984375\n",
            "Train[35/50][145/240] Loss: 0.11133897304534912, BER : 0.04230078125\n",
            "Train[35/50][146/240] Loss: 0.10706734657287598, BER : 0.03935546875\n",
            "Train[35/50][147/240] Loss: 0.11317223310470581, BER : 0.0427578125\n",
            "Train[35/50][148/240] Loss: 0.10832124203443527, BER : 0.04068359375\n",
            "Train[35/50][149/240] Loss: 0.1095656007528305, BER : 0.04238671875\n",
            "Train[35/50][150/240] Loss: 0.10940469801425934, BER : 0.041375\n",
            "Train[35/50][151/240] Loss: 0.10776437073945999, BER : 0.04075390625\n",
            "Train[35/50][152/240] Loss: 0.1098203957080841, BER : 0.0416953125\n",
            "Train[35/50][153/240] Loss: 0.10944858938455582, BER : 0.041921875\n",
            "Train[35/50][154/240] Loss: 0.10779640823602676, BER : 0.04093359375\n",
            "Train[35/50][155/240] Loss: 0.1106875017285347, BER : 0.04255078125\n",
            "Train[35/50][156/240] Loss: 0.10909263789653778, BER : 0.04108984375\n",
            "Train[35/50][157/240] Loss: 0.10693091154098511, BER : 0.040140625\n",
            "Train[35/50][158/240] Loss: 0.10678192973136902, BER : 0.0403125\n",
            "Train[35/50][159/240] Loss: 0.10756277292966843, BER : 0.04068359375\n",
            "Train[35/50][160/240] Loss: 0.10888475924730301, BER : 0.04057421875\n",
            "Train[35/50][161/240] Loss: 0.10971914231777191, BER : 0.04155859375\n",
            "Train[35/50][162/240] Loss: 0.10738822817802429, BER : 0.0401796875\n",
            "Train[35/50][163/240] Loss: 0.1043248325586319, BER : 0.03852734375\n",
            "Train[35/50][164/240] Loss: 0.10508411377668381, BER : 0.03895703125\n",
            "Train[35/50][165/240] Loss: 0.10718830674886703, BER : 0.04055859375\n",
            "Train[35/50][166/240] Loss: 0.10736837238073349, BER : 0.0399140625\n",
            "Train[35/50][167/240] Loss: 0.10880693048238754, BER : 0.04069921875\n",
            "Train[35/50][168/240] Loss: 0.10933481156826019, BER : 0.0404609375\n",
            "Train[35/50][169/240] Loss: 0.10624818503856659, BER : 0.039765625\n",
            "Train[35/50][170/240] Loss: 0.10956734418869019, BER : 0.04084765625\n",
            "Train[35/50][171/240] Loss: 0.10494588315486908, BER : 0.03890234375\n",
            "Train[35/50][172/240] Loss: 0.10924318432807922, BER : 0.04146875\n",
            "Train[35/50][173/240] Loss: 0.1086898073554039, BER : 0.04112890625\n",
            "Train[35/50][174/240] Loss: 0.10940853506326675, BER : 0.04175\n",
            "Train[35/50][175/240] Loss: 0.1078384667634964, BER : 0.03973046875\n",
            "Train[35/50][176/240] Loss: 0.10723677277565002, BER : 0.04009375\n",
            "Train[35/50][177/240] Loss: 0.10728328675031662, BER : 0.0401015625\n",
            "Train[35/50][178/240] Loss: 0.1089683473110199, BER : 0.04087109375\n",
            "Train[35/50][179/240] Loss: 0.10766056180000305, BER : 0.040703125\n",
            "Train[35/50][180/240] Loss: 0.10968579351902008, BER : 0.04127734375\n",
            "Train[35/50][181/240] Loss: 0.10637310147285461, BER : 0.040140625\n",
            "Train[35/50][182/240] Loss: 0.10610758513212204, BER : 0.0398828125\n",
            "Train[35/50][183/240] Loss: 0.10956648737192154, BER : 0.04159765625\n",
            "Train[35/50][184/240] Loss: 0.10807834565639496, BER : 0.0405625\n",
            "Train[35/50][185/240] Loss: 0.108536496758461, BER : 0.0416328125\n",
            "Train[35/50][186/240] Loss: 0.10609370470046997, BER : 0.03903125\n",
            "Train[35/50][187/240] Loss: 0.11066664755344391, BER : 0.042125\n",
            "Train[35/50][188/240] Loss: 0.10725386440753937, BER : 0.03995703125\n",
            "Train[35/50][189/240] Loss: 0.1105472594499588, BER : 0.0417578125\n",
            "Train[35/50][190/240] Loss: 0.11004598438739777, BER : 0.0417578125\n",
            "Train[35/50][191/240] Loss: 0.10752978920936584, BER : 0.04017578125\n",
            "Train[35/50][192/240] Loss: 0.10538895428180695, BER : 0.03940234375\n",
            "Train[35/50][193/240] Loss: 0.10759278386831284, BER : 0.04011328125\n",
            "Train[35/50][194/240] Loss: 0.11102650314569473, BER : 0.0419453125\n",
            "Train[35/50][195/240] Loss: 0.10594843327999115, BER : 0.040078125\n",
            "Train[35/50][196/240] Loss: 0.1067381352186203, BER : 0.03978125\n",
            "Train[35/50][197/240] Loss: 0.10985419154167175, BER : 0.04224609375\n",
            "Train[35/50][198/240] Loss: 0.10972674190998077, BER : 0.04159375\n",
            "Train[35/50][199/240] Loss: 0.11076416820287704, BER : 0.042296875\n",
            "Train[35/50][200/240] Loss: 0.10829876363277435, BER : 0.04066015625\n",
            "Train[35/50][201/240] Loss: 0.10760621726512909, BER : 0.04048046875\n",
            "Train[35/50][202/240] Loss: 0.10611984878778458, BER : 0.0398984375\n",
            "Train[35/50][203/240] Loss: 0.10912194103002548, BER : 0.04141796875\n",
            "Train[35/50][204/240] Loss: 0.10882732272148132, BER : 0.04123046875\n",
            "Train[35/50][205/240] Loss: 0.10578487068414688, BER : 0.03928515625\n",
            "Train[35/50][206/240] Loss: 0.10552555322647095, BER : 0.03924609375\n",
            "Train[35/50][207/240] Loss: 0.10950534790754318, BER : 0.04082421875\n",
            "Train[35/50][208/240] Loss: 0.10704565793275833, BER : 0.040140625\n",
            "Train[35/50][209/240] Loss: 0.10844093561172485, BER : 0.04100390625\n",
            "Train[35/50][210/240] Loss: 0.1065756306052208, BER : 0.040625\n",
            "Train[35/50][211/240] Loss: 0.1082531064748764, BER : 0.040640625\n",
            "Train[35/50][212/240] Loss: 0.1060929000377655, BER : 0.039421875\n",
            "Train[35/50][213/240] Loss: 0.10754718631505966, BER : 0.040671875\n",
            "Train[35/50][214/240] Loss: 0.11131107807159424, BER : 0.04187890625\n",
            "Train[35/50][215/240] Loss: 0.10664258152246475, BER : 0.04008203125\n",
            "Train[35/50][216/240] Loss: 0.10502295196056366, BER : 0.03973046875\n",
            "Train[35/50][217/240] Loss: 0.1077495589852333, BER : 0.0405546875\n",
            "Train[35/50][218/240] Loss: 0.10768625885248184, BER : 0.04003125\n",
            "Train[35/50][219/240] Loss: 0.10833177715539932, BER : 0.04098046875\n",
            "Train[35/50][220/240] Loss: 0.10651431232690811, BER : 0.0405\n",
            "Train[35/50][221/240] Loss: 0.10811898112297058, BER : 0.0403046875\n",
            "Train[35/50][222/240] Loss: 0.11027128249406815, BER : 0.04199609375\n",
            "Train[35/50][223/240] Loss: 0.10583651065826416, BER : 0.03873828125\n",
            "Train[35/50][224/240] Loss: 0.10801483690738678, BER : 0.0408125\n",
            "Train[35/50][225/240] Loss: 0.10278674960136414, BER : 0.03749609375\n",
            "Train[35/50][226/240] Loss: 0.10826136916875839, BER : 0.0411171875\n",
            "Train[35/50][227/240] Loss: 0.11021548509597778, BER : 0.04203515625\n",
            "Train[35/50][228/240] Loss: 0.10459381341934204, BER : 0.03850390625\n",
            "Train[35/50][229/240] Loss: 0.1076531782746315, BER : 0.0407734375\n",
            "Train[35/50][230/240] Loss: 0.1077926829457283, BER : 0.040890625\n",
            "Train[35/50][231/240] Loss: 0.10731671005487442, BER : 0.04038671875\n",
            "Train[35/50][232/240] Loss: 0.10638884454965591, BER : 0.03919140625\n",
            "Train[35/50][233/240] Loss: 0.10768915712833405, BER : 0.04039453125\n",
            "Train[35/50][234/240] Loss: 0.1076572835445404, BER : 0.04061328125\n",
            "Train[35/50][235/240] Loss: 0.10820909589529037, BER : 0.04105859375\n",
            "Train[35/50][236/240] Loss: 0.10861290991306305, BER : 0.0416484375\n",
            "Train[35/50][237/240] Loss: 0.10911774635314941, BER : 0.04134375\n",
            "Train[35/50][238/240] Loss: 0.10775208473205566, BER : 0.04148046875\n",
            "Train[35/50][239/240] Loss: 0.10839204490184784, BER : 0.04095703125\n",
            "Epoch [35/50] Train_Avg_loss(epoch): 0.28992\n",
            "-------------------------------------\n",
            "Test[35/50][0/40]  Loss: 0.08026954531669617, BER (test): 0.0430390625\n",
            "Test[35/50][1/40]  Loss: 0.07701446861028671, BER (test): 0.0410625\n",
            "Test[35/50][2/40]  Loss: 0.08108879625797272, BER (test): 0.0433515625\n",
            "Test[35/50][3/40]  Loss: 0.07668554782867432, BER (test): 0.0405\n",
            "Test[35/50][4/40]  Loss: 0.07983415573835373, BER (test): 0.0426953125\n",
            "Test[35/50][5/40]  Loss: 0.07894861698150635, BER (test): 0.04234375\n",
            "Test[35/50][6/40]  Loss: 0.08103297650814056, BER (test): 0.04353125\n",
            "Test[35/50][7/40]  Loss: 0.08125363290309906, BER (test): 0.04295703125\n",
            "Test[35/50][8/40]  Loss: 0.07819977402687073, BER (test): 0.0420546875\n",
            "Test[35/50][9/40]  Loss: 0.07888790965080261, BER (test): 0.0425390625\n",
            "Test[35/50][10/40]  Loss: 0.07932323962450027, BER (test): 0.04264453125\n",
            "Test[35/50][11/40]  Loss: 0.08006341010332108, BER (test): 0.0428046875\n",
            "Test[35/50][12/40]  Loss: 0.07872101664543152, BER (test): 0.042234375\n",
            "Test[35/50][13/40]  Loss: 0.08172932267189026, BER (test): 0.0436328125\n",
            "Test[35/50][14/40]  Loss: 0.0802544355392456, BER (test): 0.04351171875\n",
            "Test[35/50][15/40]  Loss: 0.079416923224926, BER (test): 0.04253125\n",
            "Test[35/50][16/40]  Loss: 0.08185149729251862, BER (test): 0.0444296875\n",
            "Test[35/50][17/40]  Loss: 0.07890874147415161, BER (test): 0.042609375\n",
            "Test[35/50][18/40]  Loss: 0.0819656103849411, BER (test): 0.043796875\n",
            "Test[35/50][19/40]  Loss: 0.08102516829967499, BER (test): 0.0433359375\n",
            "Test[35/50][20/40]  Loss: 0.08115559071302414, BER (test): 0.04324609375\n",
            "Test[35/50][21/40]  Loss: 0.07994668185710907, BER (test): 0.043078125\n",
            "Test[35/50][22/40]  Loss: 0.07852233946323395, BER (test): 0.04219140625\n",
            "Test[35/50][23/40]  Loss: 0.07981441915035248, BER (test): 0.043125\n",
            "Test[35/50][24/40]  Loss: 0.07595406472682953, BER (test): 0.040359375\n",
            "Test[35/50][25/40]  Loss: 0.08020002394914627, BER (test): 0.04309375\n",
            "Test[35/50][26/40]  Loss: 0.07796373218297958, BER (test): 0.0420390625\n",
            "Test[35/50][27/40]  Loss: 0.08302225172519684, BER (test): 0.044140625\n",
            "Test[35/50][28/40]  Loss: 0.078646719455719, BER (test): 0.042984375\n",
            "Test[35/50][29/40]  Loss: 0.07732481509447098, BER (test): 0.04148828125\n",
            "Test[35/50][30/40]  Loss: 0.07769478857517242, BER (test): 0.0414296875\n",
            "Test[35/50][31/40]  Loss: 0.07701807469129562, BER (test): 0.04119921875\n",
            "Test[35/50][32/40]  Loss: 0.07844816148281097, BER (test): 0.042703125\n",
            "Test[35/50][33/40]  Loss: 0.08001747727394104, BER (test): 0.04225\n",
            "Test[35/50][34/40]  Loss: 0.08009064197540283, BER (test): 0.04329296875\n",
            "Test[35/50][35/40]  Loss: 0.07917598634958267, BER (test): 0.04284765625\n",
            "Test[35/50][36/40]  Loss: 0.08203741163015366, BER (test): 0.04389453125\n",
            "Test[35/50][37/40]  Loss: 0.08051778376102448, BER (test): 0.042890625\n",
            "Test[35/50][38/40]  Loss: 0.07885001599788666, BER (test): 0.0423515625\n",
            "Test[35/50][39/40]  Loss: 0.07687297463417053, BER (test): 0.04121875\n",
            "Test_Epoch [35/50] Test_Avg_loss(epoch): 0.07949\n",
            "Train[36/50][0/240] Loss: 0.10620813816785812, BER : 0.03969140625\n",
            "Train[36/50][1/240] Loss: 0.10766658931970596, BER : 0.0403359375\n",
            "Train[36/50][2/240] Loss: 0.10709254443645477, BER : 0.04068359375\n",
            "Train[36/50][3/240] Loss: 0.10766670852899551, BER : 0.04001171875\n",
            "Train[36/50][4/240] Loss: 0.1054336428642273, BER : 0.03936328125\n",
            "Train[36/50][5/240] Loss: 0.10679983347654343, BER : 0.0399609375\n",
            "Train[36/50][6/240] Loss: 0.10649633407592773, BER : 0.03935546875\n",
            "Train[36/50][7/240] Loss: 0.1071263998746872, BER : 0.04118359375\n",
            "Train[36/50][8/240] Loss: 0.10791097581386566, BER : 0.04044140625\n",
            "Train[36/50][9/240] Loss: 0.10720372200012207, BER : 0.04094921875\n",
            "Train[36/50][10/240] Loss: 0.10771383345127106, BER : 0.04080859375\n",
            "Train[36/50][11/240] Loss: 0.10891073197126389, BER : 0.04155859375\n",
            "Train[36/50][12/240] Loss: 0.10738011449575424, BER : 0.04055859375\n",
            "Train[36/50][13/240] Loss: 0.10464112460613251, BER : 0.038515625\n",
            "Train[36/50][14/240] Loss: 0.10840700566768646, BER : 0.04066015625\n",
            "Train[36/50][15/240] Loss: 0.10689326375722885, BER : 0.04040625\n",
            "Train[36/50][16/240] Loss: 0.10710413753986359, BER : 0.039546875\n",
            "Train[36/50][17/240] Loss: 0.10863247513771057, BER : 0.04124609375\n",
            "Train[36/50][18/240] Loss: 0.10724327713251114, BER : 0.04048046875\n",
            "Train[36/50][19/240] Loss: 0.10913895070552826, BER : 0.04143359375\n",
            "Train[36/50][20/240] Loss: 0.1057584211230278, BER : 0.03925\n",
            "Train[36/50][21/240] Loss: 0.10846182703971863, BER : 0.04084375\n",
            "Train[36/50][22/240] Loss: 0.1100345104932785, BER : 0.04205859375\n",
            "Train[36/50][23/240] Loss: 0.11068310588598251, BER : 0.04143359375\n",
            "Train[36/50][24/240] Loss: 0.10712195932865143, BER : 0.0402265625\n",
            "Train[36/50][25/240] Loss: 0.10974392294883728, BER : 0.04208203125\n",
            "Train[36/50][26/240] Loss: 0.1061067059636116, BER : 0.04003515625\n",
            "Train[36/50][27/240] Loss: 0.10765990614891052, BER : 0.040671875\n",
            "Train[36/50][28/240] Loss: 0.10599790513515472, BER : 0.04040234375\n",
            "Train[36/50][29/240] Loss: 0.10648257285356522, BER : 0.03959765625\n",
            "Train[36/50][30/240] Loss: 0.10699491947889328, BER : 0.04040625\n",
            "Train[36/50][31/240] Loss: 0.10661951452493668, BER : 0.0394140625\n",
            "Train[36/50][32/240] Loss: 0.10835756361484528, BER : 0.04112890625\n",
            "Train[36/50][33/240] Loss: 0.10761193931102753, BER : 0.04014453125\n",
            "Train[36/50][34/240] Loss: 0.10495365411043167, BER : 0.03980078125\n",
            "Train[36/50][35/240] Loss: 0.10970461368560791, BER : 0.0417421875\n",
            "Train[36/50][36/240] Loss: 0.10914833843708038, BER : 0.04165234375\n",
            "Train[36/50][37/240] Loss: 0.10620836913585663, BER : 0.04015625\n",
            "Train[36/50][38/240] Loss: 0.10747645795345306, BER : 0.04026953125\n",
            "Train[36/50][39/240] Loss: 0.10704921185970306, BER : 0.0403125\n",
            "Train[36/50][40/240] Loss: 0.10676032304763794, BER : 0.04013671875\n",
            "Train[36/50][41/240] Loss: 0.10572697222232819, BER : 0.039375\n",
            "Train[36/50][42/240] Loss: 0.10638660192489624, BER : 0.039734375\n",
            "Train[36/50][43/240] Loss: 0.10918478667736053, BER : 0.04067578125\n",
            "Train[36/50][44/240] Loss: 0.1061052456498146, BER : 0.03886328125\n",
            "Train[36/50][45/240] Loss: 0.10924898833036423, BER : 0.04124609375\n",
            "Train[36/50][46/240] Loss: 0.10704672336578369, BER : 0.03996484375\n",
            "Train[36/50][47/240] Loss: 0.10755811631679535, BER : 0.04103125\n",
            "Train[36/50][48/240] Loss: 0.11025875806808472, BER : 0.042140625\n",
            "Train[36/50][49/240] Loss: 0.1076020747423172, BER : 0.039984375\n",
            "Train[36/50][50/240] Loss: 0.10681529343128204, BER : 0.039984375\n",
            "Train[36/50][51/240] Loss: 0.11057747900485992, BER : 0.0423671875\n",
            "Train[36/50][52/240] Loss: 0.10971912741661072, BER : 0.0414609375\n",
            "Train[36/50][53/240] Loss: 0.1087811291217804, BER : 0.04076953125\n",
            "Train[36/50][54/240] Loss: 0.10939492285251617, BER : 0.0414296875\n",
            "Train[36/50][55/240] Loss: 0.10507950186729431, BER : 0.038984375\n",
            "Train[36/50][56/240] Loss: 0.10780413448810577, BER : 0.04068359375\n",
            "Train[36/50][57/240] Loss: 0.10787321627140045, BER : 0.04008984375\n",
            "Train[36/50][58/240] Loss: 0.10838732123374939, BER : 0.04073046875\n",
            "Train[36/50][59/240] Loss: 0.10605894029140472, BER : 0.03978515625\n",
            "Train[36/50][60/240] Loss: 0.10919354856014252, BER : 0.04168359375\n",
            "Train[36/50][61/240] Loss: 0.10671801120042801, BER : 0.0406015625\n",
            "Train[36/50][62/240] Loss: 0.1064687967300415, BER : 0.0393046875\n",
            "Train[36/50][63/240] Loss: 0.10779667645692825, BER : 0.040828125\n",
            "Train[36/50][64/240] Loss: 0.1080673485994339, BER : 0.040984375\n",
            "Train[36/50][65/240] Loss: 0.10725275427103043, BER : 0.039546875\n",
            "Train[36/50][66/240] Loss: 0.10897500813007355, BER : 0.0413125\n",
            "Train[36/50][67/240] Loss: 0.10851462185382843, BER : 0.0408359375\n",
            "Train[36/50][68/240] Loss: 0.10843467712402344, BER : 0.04069140625\n",
            "Train[36/50][69/240] Loss: 0.10622307658195496, BER : 0.039515625\n",
            "Train[36/50][70/240] Loss: 0.10791099816560745, BER : 0.041078125\n",
            "Train[36/50][71/240] Loss: 0.10586310923099518, BER : 0.03988671875\n",
            "Train[36/50][72/240] Loss: 0.1066318079829216, BER : 0.03992578125\n",
            "Train[36/50][73/240] Loss: 0.1091456487774849, BER : 0.04105078125\n",
            "Train[36/50][74/240] Loss: 0.10793785750865936, BER : 0.04033984375\n",
            "Train[36/50][75/240] Loss: 0.10906676948070526, BER : 0.041140625\n",
            "Train[36/50][76/240] Loss: 0.10802008211612701, BER : 0.0406953125\n",
            "Train[36/50][77/240] Loss: 0.1091538816690445, BER : 0.04125\n",
            "Train[36/50][78/240] Loss: 0.11095115542411804, BER : 0.04196484375\n",
            "Train[36/50][79/240] Loss: 0.10769923031330109, BER : 0.0402109375\n",
            "Train[36/50][80/240] Loss: 0.10820776969194412, BER : 0.0411484375\n",
            "Train[36/50][81/240] Loss: 0.10792946815490723, BER : 0.04090234375\n",
            "Train[36/50][82/240] Loss: 0.10869625955820084, BER : 0.04092578125\n",
            "Train[36/50][83/240] Loss: 0.10795589536428452, BER : 0.040421875\n",
            "Train[36/50][84/240] Loss: 0.10787124931812286, BER : 0.0408671875\n",
            "Train[36/50][85/240] Loss: 0.10769900679588318, BER : 0.04056640625\n",
            "Train[36/50][86/240] Loss: 0.10487601906061172, BER : 0.03875\n",
            "Train[36/50][87/240] Loss: 0.10881956666707993, BER : 0.0412578125\n",
            "Train[36/50][88/240] Loss: 0.10420528054237366, BER : 0.037984375\n",
            "Train[36/50][89/240] Loss: 0.1068187803030014, BER : 0.04001953125\n",
            "Train[36/50][90/240] Loss: 0.10875251889228821, BER : 0.04119921875\n",
            "Train[36/50][91/240] Loss: 0.10594454407691956, BER : 0.0391640625\n",
            "Train[36/50][92/240] Loss: 0.10922172665596008, BER : 0.04113671875\n",
            "Train[36/50][93/240] Loss: 0.10793387144804001, BER : 0.04109375\n",
            "Train[36/50][94/240] Loss: 0.11092112213373184, BER : 0.04162890625\n",
            "Train[36/50][95/240] Loss: 0.10527293384075165, BER : 0.03978125\n",
            "Train[36/50][96/240] Loss: 0.10950227081775665, BER : 0.0417421875\n",
            "Train[36/50][97/240] Loss: 0.10775618255138397, BER : 0.0406328125\n",
            "Train[36/50][98/240] Loss: 0.1085672602057457, BER : 0.04062890625\n",
            "Train[36/50][99/240] Loss: 0.10672342032194138, BER : 0.03929296875\n",
            "Train[36/50][100/240] Loss: 0.10690029710531235, BER : 0.0402890625\n",
            "Train[36/50][101/240] Loss: 0.10586250573396683, BER : 0.0395546875\n",
            "Train[36/50][102/240] Loss: 0.10892714560031891, BER : 0.04122265625\n",
            "Train[36/50][103/240] Loss: 0.10836408287286758, BER : 0.04066796875\n",
            "Train[36/50][104/240] Loss: 0.11212095618247986, BER : 0.042640625\n",
            "Train[36/50][105/240] Loss: 0.1047414243221283, BER : 0.03928125\n",
            "Train[36/50][106/240] Loss: 0.11076975613832474, BER : 0.04252734375\n",
            "Train[36/50][107/240] Loss: 0.10788847506046295, BER : 0.04097265625\n",
            "Train[36/50][108/240] Loss: 0.10477815568447113, BER : 0.03848828125\n",
            "Train[36/50][109/240] Loss: 0.10853040963411331, BER : 0.04103125\n",
            "Train[36/50][110/240] Loss: 0.10681086033582687, BER : 0.03956640625\n",
            "Train[36/50][111/240] Loss: 0.1105271577835083, BER : 0.04163671875\n",
            "Train[36/50][112/240] Loss: 0.10748657584190369, BER : 0.0404609375\n",
            "Train[36/50][113/240] Loss: 0.10663370788097382, BER : 0.04018359375\n",
            "Train[36/50][114/240] Loss: 0.10600724816322327, BER : 0.039796875\n",
            "Train[36/50][115/240] Loss: 0.104652039706707, BER : 0.0396171875\n",
            "Train[36/50][116/240] Loss: 0.10727033764123917, BER : 0.0402890625\n",
            "Train[36/50][117/240] Loss: 0.10636329650878906, BER : 0.04033984375\n",
            "Train[36/50][118/240] Loss: 0.10904857516288757, BER : 0.04089453125\n",
            "Train[36/50][119/240] Loss: 0.10672985762357712, BER : 0.0397109375\n",
            "Train[36/50][120/240] Loss: 0.11119046807289124, BER : 0.04219921875\n",
            "Train[36/50][121/240] Loss: 0.10596144199371338, BER : 0.0393984375\n",
            "Train[36/50][122/240] Loss: 0.10921894758939743, BER : 0.041375\n",
            "Train[36/50][123/240] Loss: 0.10752607882022858, BER : 0.04007421875\n",
            "Train[36/50][124/240] Loss: 0.112221360206604, BER : 0.04304296875\n",
            "Train[36/50][125/240] Loss: 0.10616995394229889, BER : 0.03959765625\n",
            "Train[36/50][126/240] Loss: 0.10657217353582382, BER : 0.04107421875\n",
            "Train[36/50][127/240] Loss: 0.10551004856824875, BER : 0.03963671875\n",
            "Train[36/50][128/240] Loss: 0.11070125550031662, BER : 0.042203125\n",
            "Train[36/50][129/240] Loss: 0.10566779226064682, BER : 0.039109375\n",
            "Train[36/50][130/240] Loss: 0.10873943567276001, BER : 0.04066796875\n",
            "Train[36/50][131/240] Loss: 0.10857133567333221, BER : 0.04134375\n",
            "Train[36/50][132/240] Loss: 0.10904020071029663, BER : 0.041046875\n",
            "Train[36/50][133/240] Loss: 0.1071355864405632, BER : 0.04037109375\n",
            "Train[36/50][134/240] Loss: 0.10841929167509079, BER : 0.0413359375\n",
            "Train[36/50][135/240] Loss: 0.1080726683139801, BER : 0.04053515625\n",
            "Train[36/50][136/240] Loss: 0.1067667305469513, BER : 0.0396171875\n",
            "Train[36/50][137/240] Loss: 0.10584817081689835, BER : 0.04019921875\n",
            "Train[36/50][138/240] Loss: 0.1074160560965538, BER : 0.04025\n",
            "Train[36/50][139/240] Loss: 0.10705141723155975, BER : 0.04047265625\n",
            "Train[36/50][140/240] Loss: 0.10797744989395142, BER : 0.04071484375\n",
            "Train[36/50][141/240] Loss: 0.10319599509239197, BER : 0.03780078125\n",
            "Train[36/50][142/240] Loss: 0.10614918172359467, BER : 0.04024609375\n",
            "Train[36/50][143/240] Loss: 0.10680854320526123, BER : 0.04031640625\n",
            "Train[36/50][144/240] Loss: 0.10991893708705902, BER : 0.041734375\n",
            "Train[36/50][145/240] Loss: 0.10879623889923096, BER : 0.0414375\n",
            "Train[36/50][146/240] Loss: 0.10342623293399811, BER : 0.03817578125\n",
            "Train[36/50][147/240] Loss: 0.10577268153429031, BER : 0.03955859375\n",
            "Train[36/50][148/240] Loss: 0.10796955972909927, BER : 0.04083984375\n",
            "Train[36/50][149/240] Loss: 0.10879608988761902, BER : 0.04091015625\n",
            "Train[36/50][150/240] Loss: 0.10829910635948181, BER : 0.0413046875\n",
            "Train[36/50][151/240] Loss: 0.10307003557682037, BER : 0.03745703125\n",
            "Train[36/50][152/240] Loss: 0.11006039381027222, BER : 0.0420390625\n",
            "Train[36/50][153/240] Loss: 0.10947048664093018, BER : 0.041234375\n",
            "Train[36/50][154/240] Loss: 0.1052982360124588, BER : 0.03966796875\n",
            "Train[36/50][155/240] Loss: 0.10930546373128891, BER : 0.0416953125\n",
            "Train[36/50][156/240] Loss: 0.10667082667350769, BER : 0.0396796875\n",
            "Train[36/50][157/240] Loss: 0.10758497565984726, BER : 0.04032421875\n",
            "Train[36/50][158/240] Loss: 0.10875870287418365, BER : 0.0406875\n",
            "Train[36/50][159/240] Loss: 0.10850639641284943, BER : 0.0419765625\n",
            "Train[36/50][160/240] Loss: 0.10714371502399445, BER : 0.04099609375\n",
            "Train[36/50][161/240] Loss: 0.106378935277462, BER : 0.03978125\n",
            "Train[36/50][162/240] Loss: 0.11048033088445663, BER : 0.04137890625\n",
            "Train[36/50][163/240] Loss: 0.10681349039077759, BER : 0.04015234375\n",
            "Train[36/50][164/240] Loss: 0.10818041861057281, BER : 0.04129296875\n",
            "Train[36/50][165/240] Loss: 0.1059131920337677, BER : 0.0397265625\n",
            "Train[36/50][166/240] Loss: 0.10777422040700912, BER : 0.0411171875\n",
            "Train[36/50][167/240] Loss: 0.10572268068790436, BER : 0.04004296875\n",
            "Train[36/50][168/240] Loss: 0.1063624694943428, BER : 0.03998046875\n",
            "Train[36/50][169/240] Loss: 0.10754778236150742, BER : 0.04074609375\n",
            "Train[36/50][170/240] Loss: 0.10644695162773132, BER : 0.03942578125\n",
            "Train[36/50][171/240] Loss: 0.10806367546319962, BER : 0.04123046875\n",
            "Train[36/50][172/240] Loss: 0.10966937243938446, BER : 0.0416875\n",
            "Train[36/50][173/240] Loss: 0.10743406414985657, BER : 0.0403359375\n",
            "Train[36/50][174/240] Loss: 0.10562960058450699, BER : 0.0396640625\n",
            "Train[36/50][175/240] Loss: 0.10710754990577698, BER : 0.040765625\n",
            "Train[36/50][176/240] Loss: 0.10231463611125946, BER : 0.0376015625\n",
            "Train[36/50][177/240] Loss: 0.10634630173444748, BER : 0.04002734375\n",
            "Train[36/50][178/240] Loss: 0.10571135580539703, BER : 0.038625\n",
            "Train[36/50][179/240] Loss: 0.10895579308271408, BER : 0.04120703125\n",
            "Train[36/50][180/240] Loss: 0.10660088062286377, BER : 0.03937890625\n",
            "Train[36/50][181/240] Loss: 0.10712374746799469, BER : 0.040171875\n",
            "Train[36/50][182/240] Loss: 0.11109471321105957, BER : 0.04263671875\n",
            "Train[36/50][183/240] Loss: 0.10927878320217133, BER : 0.04132421875\n",
            "Train[36/50][184/240] Loss: 0.10776302218437195, BER : 0.04084765625\n",
            "Train[36/50][185/240] Loss: 0.11021953076124191, BER : 0.0420703125\n",
            "Train[36/50][186/240] Loss: 0.10472720861434937, BER : 0.0390546875\n",
            "Train[36/50][187/240] Loss: 0.10784792900085449, BER : 0.040203125\n",
            "Train[36/50][188/240] Loss: 0.10756973922252655, BER : 0.04059375\n",
            "Train[36/50][189/240] Loss: 0.10835827142000198, BER : 0.04094921875\n",
            "Train[36/50][190/240] Loss: 0.10854233801364899, BER : 0.04071484375\n",
            "Train[36/50][191/240] Loss: 0.10765047371387482, BER : 0.040265625\n",
            "Train[36/50][192/240] Loss: 0.10650905966758728, BER : 0.04028125\n",
            "Train[36/50][193/240] Loss: 0.10791785269975662, BER : 0.04051953125\n",
            "Train[36/50][194/240] Loss: 0.10739323496818542, BER : 0.0401875\n",
            "Train[36/50][195/240] Loss: 0.10762982815504074, BER : 0.04054296875\n",
            "Train[36/50][196/240] Loss: 0.10768038034439087, BER : 0.0404921875\n",
            "Train[36/50][197/240] Loss: 0.11283257603645325, BER : 0.04309765625\n",
            "Train[36/50][198/240] Loss: 0.10647633671760559, BER : 0.04012109375\n",
            "Train[36/50][199/240] Loss: 0.10834746807813644, BER : 0.0413359375\n",
            "Train[36/50][200/240] Loss: 0.10774296522140503, BER : 0.04075390625\n",
            "Train[36/50][201/240] Loss: 0.10711114853620529, BER : 0.03955859375\n",
            "Train[36/50][202/240] Loss: 0.10814741253852844, BER : 0.04076953125\n",
            "Train[36/50][203/240] Loss: 0.10648053884506226, BER : 0.0394375\n",
            "Train[36/50][204/240] Loss: 0.10873571038246155, BER : 0.04135546875\n",
            "Train[36/50][205/240] Loss: 0.10658226162195206, BER : 0.0395546875\n",
            "Train[36/50][206/240] Loss: 0.10988964140415192, BER : 0.0426015625\n",
            "Train[36/50][207/240] Loss: 0.10513508319854736, BER : 0.03887890625\n",
            "Train[36/50][208/240] Loss: 0.10776404291391373, BER : 0.04090625\n",
            "Train[36/50][209/240] Loss: 0.10813383758068085, BER : 0.04044921875\n",
            "Train[36/50][210/240] Loss: 0.10767161846160889, BER : 0.04047265625\n",
            "Train[36/50][211/240] Loss: 0.10946213454008102, BER : 0.041953125\n",
            "Train[36/50][212/240] Loss: 0.10742683708667755, BER : 0.0401796875\n",
            "Train[36/50][213/240] Loss: 0.10588325560092926, BER : 0.0391484375\n",
            "Train[36/50][214/240] Loss: 0.10698947310447693, BER : 0.04021484375\n",
            "Train[36/50][215/240] Loss: 0.10688051581382751, BER : 0.04002734375\n",
            "Train[36/50][216/240] Loss: 0.10709087550640106, BER : 0.04080078125\n",
            "Train[36/50][217/240] Loss: 0.10798904299736023, BER : 0.04080859375\n",
            "Train[36/50][218/240] Loss: 0.10739761590957642, BER : 0.04053125\n",
            "Train[36/50][219/240] Loss: 0.10825604945421219, BER : 0.04066796875\n",
            "Train[36/50][220/240] Loss: 0.10314631462097168, BER : 0.03825\n",
            "Train[36/50][221/240] Loss: 0.10730218887329102, BER : 0.03995703125\n",
            "Train[36/50][222/240] Loss: 0.10767460614442825, BER : 0.0415234375\n",
            "Train[36/50][223/240] Loss: 0.10422869771718979, BER : 0.038796875\n",
            "Train[36/50][224/240] Loss: 0.1044621393084526, BER : 0.0384921875\n",
            "Train[36/50][225/240] Loss: 0.1066184714436531, BER : 0.04041015625\n",
            "Train[36/50][226/240] Loss: 0.10662529617547989, BER : 0.04043359375\n",
            "Train[36/50][227/240] Loss: 0.10780603438615799, BER : 0.0413828125\n",
            "Train[36/50][228/240] Loss: 0.1071903258562088, BER : 0.04020703125\n",
            "Train[36/50][229/240] Loss: 0.10589095950126648, BER : 0.039765625\n",
            "Train[36/50][230/240] Loss: 0.10860937833786011, BER : 0.04148046875\n",
            "Train[36/50][231/240] Loss: 0.11046085506677628, BER : 0.042578125\n",
            "Train[36/50][232/240] Loss: 0.10712575167417526, BER : 0.03975390625\n",
            "Train[36/50][233/240] Loss: 0.10562677681446075, BER : 0.03973828125\n",
            "Train[36/50][234/240] Loss: 0.10727186501026154, BER : 0.03965234375\n",
            "Train[36/50][235/240] Loss: 0.10599114000797272, BER : 0.038796875\n",
            "Train[36/50][236/240] Loss: 0.1079196035861969, BER : 0.0412265625\n",
            "Train[36/50][237/240] Loss: 0.10644272714853287, BER : 0.039484375\n",
            "Train[36/50][238/240] Loss: 0.10599091649055481, BER : 0.03983984375\n",
            "Train[36/50][239/240] Loss: 0.10605063289403915, BER : 0.03994921875\n",
            "Epoch [36/50] Train_Avg_loss(epoch): 0.28499\n",
            "-------------------------------------\n",
            "Test[36/50][0/40]  Loss: 0.08185256272554398, BER (test): 0.04426171875\n",
            "Test[36/50][1/40]  Loss: 0.0774872824549675, BER (test): 0.0417421875\n",
            "Test[36/50][2/40]  Loss: 0.07995732873678207, BER (test): 0.0431640625\n",
            "Test[36/50][3/40]  Loss: 0.07993855327367783, BER (test): 0.042421875\n",
            "Test[36/50][4/40]  Loss: 0.07680758088827133, BER (test): 0.04090234375\n",
            "Test[36/50][5/40]  Loss: 0.07842251658439636, BER (test): 0.041859375\n",
            "Test[36/50][6/40]  Loss: 0.07867724448442459, BER (test): 0.0418515625\n",
            "Test[36/50][7/40]  Loss: 0.07676443457603455, BER (test): 0.0413046875\n",
            "Test[36/50][8/40]  Loss: 0.07824103534221649, BER (test): 0.04183984375\n",
            "Test[36/50][9/40]  Loss: 0.07960931956768036, BER (test): 0.0425078125\n",
            "Test[36/50][10/40]  Loss: 0.07867610454559326, BER (test): 0.04223828125\n",
            "Test[36/50][11/40]  Loss: 0.07849694043397903, BER (test): 0.04235546875\n",
            "Test[36/50][12/40]  Loss: 0.07778255641460419, BER (test): 0.0421953125\n",
            "Test[36/50][13/40]  Loss: 0.07832203060388565, BER (test): 0.0419921875\n",
            "Test[36/50][14/40]  Loss: 0.07661991566419601, BER (test): 0.04045703125\n",
            "Test[36/50][15/40]  Loss: 0.0792505145072937, BER (test): 0.042453125\n",
            "Test[36/50][16/40]  Loss: 0.07738590240478516, BER (test): 0.04194140625\n",
            "Test[36/50][17/40]  Loss: 0.0787813663482666, BER (test): 0.0416640625\n",
            "Test[36/50][18/40]  Loss: 0.07714135944843292, BER (test): 0.04094140625\n",
            "Test[36/50][19/40]  Loss: 0.07935038954019547, BER (test): 0.04215625\n",
            "Test[36/50][20/40]  Loss: 0.07898464053869247, BER (test): 0.04103515625\n",
            "Test[36/50][21/40]  Loss: 0.07867006957530975, BER (test): 0.04219140625\n",
            "Test[36/50][22/40]  Loss: 0.07607269287109375, BER (test): 0.04111328125\n",
            "Test[36/50][23/40]  Loss: 0.07940910011529922, BER (test): 0.04197265625\n",
            "Test[36/50][24/40]  Loss: 0.08002762496471405, BER (test): 0.04294921875\n",
            "Test[36/50][25/40]  Loss: 0.07754252851009369, BER (test): 0.041203125\n",
            "Test[36/50][26/40]  Loss: 0.08000274002552032, BER (test): 0.04326171875\n",
            "Test[36/50][27/40]  Loss: 0.0781218633055687, BER (test): 0.0417109375\n",
            "Test[36/50][28/40]  Loss: 0.07917781174182892, BER (test): 0.0427734375\n",
            "Test[36/50][29/40]  Loss: 0.07803548872470856, BER (test): 0.0413984375\n",
            "Test[36/50][30/40]  Loss: 0.07927854359149933, BER (test): 0.042703125\n",
            "Test[36/50][31/40]  Loss: 0.07860182970762253, BER (test): 0.0424453125\n",
            "Test[36/50][32/40]  Loss: 0.07916464656591415, BER (test): 0.0424140625\n",
            "Test[36/50][33/40]  Loss: 0.07812771201133728, BER (test): 0.0417578125\n",
            "Test[36/50][34/40]  Loss: 0.07765354216098785, BER (test): 0.04083984375\n",
            "Test[36/50][35/40]  Loss: 0.07632677257061005, BER (test): 0.04055859375\n",
            "Test[36/50][36/40]  Loss: 0.07685033977031708, BER (test): 0.04085546875\n",
            "Test[36/50][37/40]  Loss: 0.08035968989133835, BER (test): 0.0433828125\n",
            "Test[36/50][38/40]  Loss: 0.07990813255310059, BER (test): 0.04252734375\n",
            "Test[36/50][39/40]  Loss: 0.07905597984790802, BER (test): 0.04225\n",
            "Test_Epoch [36/50] Test_Avg_loss(epoch): 0.07852\n",
            "Train[37/50][0/240] Loss: 0.10950308293104172, BER : 0.0421171875\n",
            "Train[37/50][1/240] Loss: 0.10727465152740479, BER : 0.0398125\n",
            "Train[37/50][2/240] Loss: 0.10508601367473602, BER : 0.0395234375\n",
            "Train[37/50][3/240] Loss: 0.10791290551424026, BER : 0.04043359375\n",
            "Train[37/50][4/240] Loss: 0.10725080966949463, BER : 0.039609375\n",
            "Train[37/50][5/240] Loss: 0.10670456290245056, BER : 0.0399453125\n",
            "Train[37/50][6/240] Loss: 0.10648278892040253, BER : 0.03959765625\n",
            "Train[37/50][7/240] Loss: 0.10827043652534485, BER : 0.04161328125\n",
            "Train[37/50][8/240] Loss: 0.106843002140522, BER : 0.040296875\n",
            "Train[37/50][9/240] Loss: 0.10684698820114136, BER : 0.03974609375\n",
            "Train[37/50][10/240] Loss: 0.10980738699436188, BER : 0.0418359375\n",
            "Train[37/50][11/240] Loss: 0.10939358919858932, BER : 0.0418828125\n",
            "Train[37/50][12/240] Loss: 0.10796182602643967, BER : 0.040609375\n",
            "Train[37/50][13/240] Loss: 0.1096109002828598, BER : 0.0416328125\n",
            "Train[37/50][14/240] Loss: 0.10796473175287247, BER : 0.040796875\n",
            "Train[37/50][15/240] Loss: 0.1075688898563385, BER : 0.0403984375\n",
            "Train[37/50][16/240] Loss: 0.1073501855134964, BER : 0.0404375\n",
            "Train[37/50][17/240] Loss: 0.10701488703489304, BER : 0.039984375\n",
            "Train[37/50][18/240] Loss: 0.10462090373039246, BER : 0.039328125\n",
            "Train[37/50][19/240] Loss: 0.10905395448207855, BER : 0.0407734375\n",
            "Train[37/50][20/240] Loss: 0.10898350924253464, BER : 0.042046875\n",
            "Train[37/50][21/240] Loss: 0.10801161825656891, BER : 0.04096484375\n",
            "Train[37/50][22/240] Loss: 0.10678079724311829, BER : 0.04022265625\n",
            "Train[37/50][23/240] Loss: 0.10662123560905457, BER : 0.040078125\n",
            "Train[37/50][24/240] Loss: 0.10670101642608643, BER : 0.0399296875\n",
            "Train[37/50][25/240] Loss: 0.10330889374017715, BER : 0.038171875\n",
            "Train[37/50][26/240] Loss: 0.10752852261066437, BER : 0.0398984375\n",
            "Train[37/50][27/240] Loss: 0.10921099781990051, BER : 0.04126953125\n",
            "Train[37/50][28/240] Loss: 0.1061662957072258, BER : 0.03978125\n",
            "Train[37/50][29/240] Loss: 0.10729566961526871, BER : 0.0403359375\n",
            "Train[37/50][30/240] Loss: 0.1050853580236435, BER : 0.03973046875\n",
            "Train[37/50][31/240] Loss: 0.10695543140172958, BER : 0.04078515625\n",
            "Train[37/50][32/240] Loss: 0.10717430710792542, BER : 0.0406015625\n",
            "Train[37/50][33/240] Loss: 0.1072797179222107, BER : 0.04047265625\n",
            "Train[37/50][34/240] Loss: 0.10701599717140198, BER : 0.039796875\n",
            "Train[37/50][35/240] Loss: 0.1048591285943985, BER : 0.039140625\n",
            "Train[37/50][36/240] Loss: 0.10902240872383118, BER : 0.04087890625\n",
            "Train[37/50][37/240] Loss: 0.10659827291965485, BER : 0.0393203125\n",
            "Train[37/50][38/240] Loss: 0.10682176798582077, BER : 0.04009765625\n",
            "Train[37/50][39/240] Loss: 0.10590880364179611, BER : 0.0398125\n",
            "Train[37/50][40/240] Loss: 0.10823611170053482, BER : 0.04094140625\n",
            "Train[37/50][41/240] Loss: 0.10811508446931839, BER : 0.0407734375\n",
            "Train[37/50][42/240] Loss: 0.10831724107265472, BER : 0.04133984375\n",
            "Train[37/50][43/240] Loss: 0.10575589537620544, BER : 0.03968359375\n",
            "Train[37/50][44/240] Loss: 0.10799757391214371, BER : 0.0401015625\n",
            "Train[37/50][45/240] Loss: 0.10780956596136093, BER : 0.04066015625\n",
            "Train[37/50][46/240] Loss: 0.10648876428604126, BER : 0.040140625\n",
            "Train[37/50][47/240] Loss: 0.10759621113538742, BER : 0.04052734375\n",
            "Train[37/50][48/240] Loss: 0.1050133928656578, BER : 0.03974609375\n",
            "Train[37/50][49/240] Loss: 0.10657058656215668, BER : 0.03941015625\n",
            "Train[37/50][50/240] Loss: 0.10648760944604874, BER : 0.03919140625\n",
            "Train[37/50][51/240] Loss: 0.10534082353115082, BER : 0.03990234375\n",
            "Train[37/50][52/240] Loss: 0.10993986576795578, BER : 0.04194140625\n",
            "Train[37/50][53/240] Loss: 0.10802134871482849, BER : 0.0409609375\n",
            "Train[37/50][54/240] Loss: 0.10795704275369644, BER : 0.04034375\n",
            "Train[37/50][55/240] Loss: 0.10597433894872665, BER : 0.039453125\n",
            "Train[37/50][56/240] Loss: 0.10639271140098572, BER : 0.039625\n",
            "Train[37/50][57/240] Loss: 0.10829082876443863, BER : 0.0409609375\n",
            "Train[37/50][58/240] Loss: 0.11028233170509338, BER : 0.042015625\n",
            "Train[37/50][59/240] Loss: 0.1046028584241867, BER : 0.0388828125\n",
            "Train[37/50][60/240] Loss: 0.10570372641086578, BER : 0.04051171875\n",
            "Train[37/50][61/240] Loss: 0.10891464352607727, BER : 0.041015625\n",
            "Train[37/50][62/240] Loss: 0.1079060435295105, BER : 0.04038671875\n",
            "Train[37/50][63/240] Loss: 0.10454504936933517, BER : 0.03844921875\n",
            "Train[37/50][64/240] Loss: 0.10812857002019882, BER : 0.04062109375\n",
            "Train[37/50][65/240] Loss: 0.10776965320110321, BER : 0.04084765625\n",
            "Train[37/50][66/240] Loss: 0.10580761730670929, BER : 0.0396328125\n",
            "Train[37/50][67/240] Loss: 0.10471314191818237, BER : 0.03896484375\n",
            "Train[37/50][68/240] Loss: 0.106858029961586, BER : 0.0403046875\n",
            "Train[37/50][69/240] Loss: 0.1077396422624588, BER : 0.04028515625\n",
            "Train[37/50][70/240] Loss: 0.1096336841583252, BER : 0.04238671875\n",
            "Train[37/50][71/240] Loss: 0.10663915425539017, BER : 0.04035546875\n",
            "Train[37/50][72/240] Loss: 0.10610945522785187, BER : 0.03918359375\n",
            "Train[37/50][73/240] Loss: 0.10576967895030975, BER : 0.04011328125\n",
            "Train[37/50][74/240] Loss: 0.1070973128080368, BER : 0.04048046875\n",
            "Train[37/50][75/240] Loss: 0.10786805301904678, BER : 0.04040234375\n",
            "Train[37/50][76/240] Loss: 0.10691718012094498, BER : 0.04065625\n",
            "Train[37/50][77/240] Loss: 0.10706048458814621, BER : 0.0405390625\n",
            "Train[37/50][78/240] Loss: 0.10739348083734512, BER : 0.0406796875\n",
            "Train[37/50][79/240] Loss: 0.10668226331472397, BER : 0.0396875\n",
            "Train[37/50][80/240] Loss: 0.10582536458969116, BER : 0.03959765625\n",
            "Train[37/50][81/240] Loss: 0.10670194029808044, BER : 0.040484375\n",
            "Train[37/50][82/240] Loss: 0.10689936578273773, BER : 0.04029296875\n",
            "Train[37/50][83/240] Loss: 0.10938307642936707, BER : 0.04153125\n",
            "Train[37/50][84/240] Loss: 0.10654512047767639, BER : 0.04016796875\n",
            "Train[37/50][85/240] Loss: 0.10676369816064835, BER : 0.03949609375\n",
            "Train[37/50][86/240] Loss: 0.1069943904876709, BER : 0.04015234375\n",
            "Train[37/50][87/240] Loss: 0.1051664799451828, BER : 0.03948046875\n",
            "Train[37/50][88/240] Loss: 0.11143313348293304, BER : 0.042375\n",
            "Train[37/50][89/240] Loss: 0.10847234725952148, BER : 0.04059375\n",
            "Train[37/50][90/240] Loss: 0.10709501057863235, BER : 0.04036328125\n",
            "Train[37/50][91/240] Loss: 0.11032892763614655, BER : 0.0418515625\n",
            "Train[37/50][92/240] Loss: 0.10665043443441391, BER : 0.0400703125\n",
            "Train[37/50][93/240] Loss: 0.10458000004291534, BER : 0.03859375\n",
            "Train[37/50][94/240] Loss: 0.10702604055404663, BER : 0.03949609375\n",
            "Train[37/50][95/240] Loss: 0.10662171989679337, BER : 0.04020703125\n",
            "Train[37/50][96/240] Loss: 0.10721288621425629, BER : 0.04094140625\n",
            "Train[37/50][97/240] Loss: 0.10607673972845078, BER : 0.0396171875\n",
            "Train[37/50][98/240] Loss: 0.10610005259513855, BER : 0.03948828125\n",
            "Train[37/50][99/240] Loss: 0.1061028316617012, BER : 0.04020703125\n",
            "Train[37/50][100/240] Loss: 0.10565681755542755, BER : 0.038671875\n",
            "Train[37/50][101/240] Loss: 0.10490204393863678, BER : 0.03925\n",
            "Train[37/50][102/240] Loss: 0.10848596692085266, BER : 0.04069140625\n",
            "Train[37/50][103/240] Loss: 0.1113213524222374, BER : 0.042375\n",
            "Train[37/50][104/240] Loss: 0.11043007671833038, BER : 0.0418515625\n",
            "Train[37/50][105/240] Loss: 0.11013440042734146, BER : 0.0420703125\n",
            "Train[37/50][106/240] Loss: 0.10784052312374115, BER : 0.0406484375\n",
            "Train[37/50][107/240] Loss: 0.10992302000522614, BER : 0.0420390625\n",
            "Train[37/50][108/240] Loss: 0.10843761265277863, BER : 0.0411328125\n",
            "Train[37/50][109/240] Loss: 0.10972811281681061, BER : 0.0421328125\n",
            "Train[37/50][110/240] Loss: 0.10856439173221588, BER : 0.04091796875\n",
            "Train[37/50][111/240] Loss: 0.10714159160852432, BER : 0.04128515625\n",
            "Train[37/50][112/240] Loss: 0.1068599671125412, BER : 0.04025\n",
            "Train[37/50][113/240] Loss: 0.11056334525346756, BER : 0.04240234375\n",
            "Train[37/50][114/240] Loss: 0.10890378057956696, BER : 0.0414296875\n",
            "Train[37/50][115/240] Loss: 0.10759126394987106, BER : 0.04078125\n",
            "Train[37/50][116/240] Loss: 0.11044454574584961, BER : 0.0420625\n",
            "Train[37/50][117/240] Loss: 0.1066754162311554, BER : 0.0394765625\n",
            "Train[37/50][118/240] Loss: 0.10711044818162918, BER : 0.04065234375\n",
            "Train[37/50][119/240] Loss: 0.10695265233516693, BER : 0.0401640625\n",
            "Train[37/50][120/240] Loss: 0.10758763551712036, BER : 0.04103125\n",
            "Train[37/50][121/240] Loss: 0.1065085157752037, BER : 0.03937890625\n",
            "Train[37/50][122/240] Loss: 0.10792431235313416, BER : 0.040828125\n",
            "Train[37/50][123/240] Loss: 0.10693345963954926, BER : 0.04059375\n",
            "Train[37/50][124/240] Loss: 0.10824793577194214, BER : 0.04081640625\n",
            "Train[37/50][125/240] Loss: 0.10635185986757278, BER : 0.039578125\n",
            "Train[37/50][126/240] Loss: 0.10672583431005478, BER : 0.03990625\n",
            "Train[37/50][127/240] Loss: 0.11032943427562714, BER : 0.0412734375\n",
            "Train[37/50][128/240] Loss: 0.10648214817047119, BER : 0.04001171875\n",
            "Train[37/50][129/240] Loss: 0.10854102671146393, BER : 0.0411796875\n",
            "Train[37/50][130/240] Loss: 0.10782201588153839, BER : 0.04071875\n",
            "Train[37/50][131/240] Loss: 0.10616602003574371, BER : 0.04034375\n",
            "Train[37/50][132/240] Loss: 0.10698605328798294, BER : 0.0401015625\n",
            "Train[37/50][133/240] Loss: 0.10619841516017914, BER : 0.03930078125\n",
            "Train[37/50][134/240] Loss: 0.10848486423492432, BER : 0.0409140625\n",
            "Train[37/50][135/240] Loss: 0.10396860539913177, BER : 0.0384921875\n",
            "Train[37/50][136/240] Loss: 0.10466083884239197, BER : 0.03916796875\n",
            "Train[37/50][137/240] Loss: 0.10780979692935944, BER : 0.04035546875\n",
            "Train[37/50][138/240] Loss: 0.10813091695308685, BER : 0.04071484375\n",
            "Train[37/50][139/240] Loss: 0.10504636913537979, BER : 0.0389375\n",
            "Train[37/50][140/240] Loss: 0.10656481981277466, BER : 0.04044921875\n",
            "Train[37/50][141/240] Loss: 0.10585911571979523, BER : 0.03940625\n",
            "Train[37/50][142/240] Loss: 0.10739544034004211, BER : 0.04083203125\n",
            "Train[37/50][143/240] Loss: 0.10532957315444946, BER : 0.04007421875\n",
            "Train[37/50][144/240] Loss: 0.10957464575767517, BER : 0.0411015625\n",
            "Train[37/50][145/240] Loss: 0.10487339645624161, BER : 0.03967578125\n",
            "Train[37/50][146/240] Loss: 0.10772454738616943, BER : 0.04008984375\n",
            "Train[37/50][147/240] Loss: 0.1069856733083725, BER : 0.04055859375\n",
            "Train[37/50][148/240] Loss: 0.10729070752859116, BER : 0.04100390625\n",
            "Train[37/50][149/240] Loss: 0.10764607042074203, BER : 0.04059375\n",
            "Train[37/50][150/240] Loss: 0.10646674782037735, BER : 0.04066796875\n",
            "Train[37/50][151/240] Loss: 0.10605490207672119, BER : 0.03932421875\n",
            "Train[37/50][152/240] Loss: 0.10861494392156601, BER : 0.0413046875\n",
            "Train[37/50][153/240] Loss: 0.10636775195598602, BER : 0.03976953125\n",
            "Train[37/50][154/240] Loss: 0.10632060468196869, BER : 0.0396953125\n",
            "Train[37/50][155/240] Loss: 0.10623946040868759, BER : 0.03972265625\n",
            "Train[37/50][156/240] Loss: 0.11178137362003326, BER : 0.0433671875\n",
            "Train[37/50][157/240] Loss: 0.11061348766088486, BER : 0.0420390625\n",
            "Train[37/50][158/240] Loss: 0.10557360202074051, BER : 0.03944140625\n",
            "Train[37/50][159/240] Loss: 0.10510139167308807, BER : 0.03969921875\n",
            "Train[37/50][160/240] Loss: 0.10654515027999878, BER : 0.03953125\n",
            "Train[37/50][161/240] Loss: 0.10825082659721375, BER : 0.04144140625\n",
            "Train[37/50][162/240] Loss: 0.10739497095346451, BER : 0.04019140625\n",
            "Train[37/50][163/240] Loss: 0.10809546709060669, BER : 0.04048828125\n",
            "Train[37/50][164/240] Loss: 0.10792356729507446, BER : 0.0408828125\n",
            "Train[37/50][165/240] Loss: 0.10632754117250443, BER : 0.03994921875\n",
            "Train[37/50][166/240] Loss: 0.1069827675819397, BER : 0.040890625\n",
            "Train[37/50][167/240] Loss: 0.11099553108215332, BER : 0.042046875\n",
            "Train[37/50][168/240] Loss: 0.10692089796066284, BER : 0.04106640625\n",
            "Train[37/50][169/240] Loss: 0.10288971662521362, BER : 0.03798046875\n",
            "Train[37/50][170/240] Loss: 0.10506235808134079, BER : 0.03950390625\n",
            "Train[37/50][171/240] Loss: 0.10560055077075958, BER : 0.03931640625\n",
            "Train[37/50][172/240] Loss: 0.10947603732347488, BER : 0.0411953125\n",
            "Train[37/50][173/240] Loss: 0.1048072874546051, BER : 0.03896875\n",
            "Train[37/50][174/240] Loss: 0.10762035846710205, BER : 0.04074609375\n",
            "Train[37/50][175/240] Loss: 0.10594628006219864, BER : 0.03976171875\n",
            "Train[37/50][176/240] Loss: 0.10402466356754303, BER : 0.0391953125\n",
            "Train[37/50][177/240] Loss: 0.10498390346765518, BER : 0.03935546875\n",
            "Train[37/50][178/240] Loss: 0.10742268711328506, BER : 0.04003125\n",
            "Train[37/50][179/240] Loss: 0.10626734048128128, BER : 0.04015625\n",
            "Train[37/50][180/240] Loss: 0.10575104504823685, BER : 0.040015625\n",
            "Train[37/50][181/240] Loss: 0.10902632027864456, BER : 0.04153125\n",
            "Train[37/50][182/240] Loss: 0.1075984388589859, BER : 0.040328125\n",
            "Train[37/50][183/240] Loss: 0.10440102219581604, BER : 0.03876953125\n",
            "Train[37/50][184/240] Loss: 0.10973591357469559, BER : 0.04179296875\n",
            "Train[37/50][185/240] Loss: 0.10557934641838074, BER : 0.04003125\n",
            "Train[37/50][186/240] Loss: 0.10768294334411621, BER : 0.04117578125\n",
            "Train[37/50][187/240] Loss: 0.10640110075473785, BER : 0.0404609375\n",
            "Train[37/50][188/240] Loss: 0.10343857854604721, BER : 0.03883203125\n",
            "Train[37/50][189/240] Loss: 0.1060124859213829, BER : 0.03905078125\n",
            "Train[37/50][190/240] Loss: 0.10934822261333466, BER : 0.04190625\n",
            "Train[37/50][191/240] Loss: 0.10757137835025787, BER : 0.0408125\n",
            "Train[37/50][192/240] Loss: 0.10780426859855652, BER : 0.04126171875\n",
            "Train[37/50][193/240] Loss: 0.10580854117870331, BER : 0.03962109375\n",
            "Train[37/50][194/240] Loss: 0.10807149857282639, BER : 0.04102734375\n",
            "Train[37/50][195/240] Loss: 0.10923292487859726, BER : 0.0412421875\n",
            "Train[37/50][196/240] Loss: 0.10870321094989777, BER : 0.04115625\n",
            "Train[37/50][197/240] Loss: 0.10521301627159119, BER : 0.0393359375\n",
            "Train[37/50][198/240] Loss: 0.10545357316732407, BER : 0.03949609375\n",
            "Train[37/50][199/240] Loss: 0.10687839984893799, BER : 0.04073046875\n",
            "Train[37/50][200/240] Loss: 0.1064651757478714, BER : 0.0400234375\n",
            "Train[37/50][201/240] Loss: 0.10591457784175873, BER : 0.03961328125\n",
            "Train[37/50][202/240] Loss: 0.10582126677036285, BER : 0.0395\n",
            "Train[37/50][203/240] Loss: 0.10654102265834808, BER : 0.0408203125\n",
            "Train[37/50][204/240] Loss: 0.10635039955377579, BER : 0.03959765625\n",
            "Train[37/50][205/240] Loss: 0.10521899163722992, BER : 0.039140625\n",
            "Train[37/50][206/240] Loss: 0.1051170751452446, BER : 0.03840625\n",
            "Train[37/50][207/240] Loss: 0.10569445788860321, BER : 0.0403671875\n",
            "Train[37/50][208/240] Loss: 0.10775867104530334, BER : 0.04062109375\n",
            "Train[37/50][209/240] Loss: 0.10727979987859726, BER : 0.04082421875\n",
            "Train[37/50][210/240] Loss: 0.10440054535865784, BER : 0.03935546875\n",
            "Train[37/50][211/240] Loss: 0.10616152733564377, BER : 0.03988671875\n",
            "Train[37/50][212/240] Loss: 0.10282368212938309, BER : 0.03821484375\n",
            "Train[37/50][213/240] Loss: 0.10318771749734879, BER : 0.0371640625\n",
            "Train[37/50][214/240] Loss: 0.10316921770572662, BER : 0.03828515625\n",
            "Train[37/50][215/240] Loss: 0.10773247480392456, BER : 0.04058984375\n",
            "Train[37/50][216/240] Loss: 0.10628142952919006, BER : 0.04001953125\n",
            "Train[37/50][217/240] Loss: 0.10641094297170639, BER : 0.0402265625\n",
            "Train[37/50][218/240] Loss: 0.10817468911409378, BER : 0.04105859375\n",
            "Train[37/50][219/240] Loss: 0.10630683600902557, BER : 0.04002734375\n",
            "Train[37/50][220/240] Loss: 0.10581494122743607, BER : 0.04\n",
            "Train[37/50][221/240] Loss: 0.10658665746450424, BER : 0.03976171875\n",
            "Train[37/50][222/240] Loss: 0.10630950331687927, BER : 0.040234375\n",
            "Train[37/50][223/240] Loss: 0.1091780886054039, BER : 0.04082421875\n",
            "Train[37/50][224/240] Loss: 0.10698272287845612, BER : 0.04075390625\n",
            "Train[37/50][225/240] Loss: 0.1059226244688034, BER : 0.0401015625\n",
            "Train[37/50][226/240] Loss: 0.1067126914858818, BER : 0.040765625\n",
            "Train[37/50][227/240] Loss: 0.10935011506080627, BER : 0.04160546875\n",
            "Train[37/50][228/240] Loss: 0.1058795303106308, BER : 0.03977734375\n",
            "Train[37/50][229/240] Loss: 0.10523772984743118, BER : 0.038796875\n",
            "Train[37/50][230/240] Loss: 0.10439266264438629, BER : 0.0395\n",
            "Train[37/50][231/240] Loss: 0.10644128918647766, BER : 0.04016015625\n",
            "Train[37/50][232/240] Loss: 0.10617461055517197, BER : 0.03976953125\n",
            "Train[37/50][233/240] Loss: 0.10700703412294388, BER : 0.04093359375\n",
            "Train[37/50][234/240] Loss: 0.10474361479282379, BER : 0.03901953125\n",
            "Train[37/50][235/240] Loss: 0.10657370090484619, BER : 0.0400859375\n",
            "Train[37/50][236/240] Loss: 0.10545391589403152, BER : 0.03962890625\n",
            "Train[37/50][237/240] Loss: 0.10555491596460342, BER : 0.039515625\n",
            "Train[37/50][238/240] Loss: 0.10679255425930023, BER : 0.04050390625\n",
            "Train[37/50][239/240] Loss: 0.10401259362697601, BER : 0.039171875\n",
            "Epoch [37/50] Train_Avg_loss(epoch): 0.28031\n",
            "-------------------------------------\n",
            "Test[37/50][0/40]  Loss: 0.07827936857938766, BER (test): 0.0420546875\n",
            "Test[37/50][1/40]  Loss: 0.08055268973112106, BER (test): 0.0427734375\n",
            "Test[37/50][2/40]  Loss: 0.07914074510335922, BER (test): 0.04175390625\n",
            "Test[37/50][3/40]  Loss: 0.07903530448675156, BER (test): 0.04202734375\n",
            "Test[37/50][4/40]  Loss: 0.08038480579853058, BER (test): 0.04269140625\n",
            "Test[37/50][5/40]  Loss: 0.07990458607673645, BER (test): 0.04214453125\n",
            "Test[37/50][6/40]  Loss: 0.07895034551620483, BER (test): 0.0420703125\n",
            "Test[37/50][7/40]  Loss: 0.0784153938293457, BER (test): 0.0411171875\n",
            "Test[37/50][8/40]  Loss: 0.08042938262224197, BER (test): 0.0429296875\n",
            "Test[37/50][9/40]  Loss: 0.07874903082847595, BER (test): 0.041265625\n",
            "Test[37/50][10/40]  Loss: 0.07819626480340958, BER (test): 0.04143359375\n",
            "Test[37/50][11/40]  Loss: 0.08247344195842743, BER (test): 0.04334765625\n",
            "Test[37/50][12/40]  Loss: 0.07828795164823532, BER (test): 0.04110546875\n",
            "Test[37/50][13/40]  Loss: 0.07767745107412338, BER (test): 0.04096875\n",
            "Test[37/50][14/40]  Loss: 0.07759253680706024, BER (test): 0.04147265625\n",
            "Test[37/50][15/40]  Loss: 0.07763303071260452, BER (test): 0.04095703125\n",
            "Test[37/50][16/40]  Loss: 0.08064049482345581, BER (test): 0.04260546875\n",
            "Test[37/50][17/40]  Loss: 0.0797528624534607, BER (test): 0.04321875\n",
            "Test[37/50][18/40]  Loss: 0.07795257866382599, BER (test): 0.04130078125\n",
            "Test[37/50][19/40]  Loss: 0.08064089715480804, BER (test): 0.04337109375\n",
            "Test[37/50][20/40]  Loss: 0.08023884147405624, BER (test): 0.0424375\n",
            "Test[37/50][21/40]  Loss: 0.07993600517511368, BER (test): 0.042515625\n",
            "Test[37/50][22/40]  Loss: 0.07675254344940186, BER (test): 0.04016015625\n",
            "Test[37/50][23/40]  Loss: 0.08133672177791595, BER (test): 0.042890625\n",
            "Test[37/50][24/40]  Loss: 0.07779446244239807, BER (test): 0.0411953125\n",
            "Test[37/50][25/40]  Loss: 0.08193638175725937, BER (test): 0.0441328125\n",
            "Test[37/50][26/40]  Loss: 0.08103922754526138, BER (test): 0.0425546875\n",
            "Test[37/50][27/40]  Loss: 0.0802886039018631, BER (test): 0.04301171875\n",
            "Test[37/50][28/40]  Loss: 0.07898582518100739, BER (test): 0.0420625\n",
            "Test[37/50][29/40]  Loss: 0.08031868189573288, BER (test): 0.04274609375\n",
            "Test[37/50][30/40]  Loss: 0.08120973408222198, BER (test): 0.04306640625\n",
            "Test[37/50][31/40]  Loss: 0.07839895039796829, BER (test): 0.04180078125\n",
            "Test[37/50][32/40]  Loss: 0.07785913348197937, BER (test): 0.0412578125\n",
            "Test[37/50][33/40]  Loss: 0.07946790009737015, BER (test): 0.04212109375\n",
            "Test[37/50][34/40]  Loss: 0.07977407425642014, BER (test): 0.04255859375\n",
            "Test[37/50][35/40]  Loss: 0.07901273667812347, BER (test): 0.04199609375\n",
            "Test[37/50][36/40]  Loss: 0.08007660508155823, BER (test): 0.04290234375\n",
            "Test[37/50][37/40]  Loss: 0.08305604755878448, BER (test): 0.0433828125\n",
            "Test[37/50][38/40]  Loss: 0.07911385595798492, BER (test): 0.04099609375\n",
            "Test[37/50][39/40]  Loss: 0.07833383232355118, BER (test): 0.0415703125\n",
            "Test_Epoch [37/50] Test_Avg_loss(epoch): 0.07949\n",
            "Train[38/50][0/240] Loss: 0.10510724037885666, BER : 0.03976171875\n",
            "Train[38/50][1/240] Loss: 0.1042584702372551, BER : 0.03918359375\n",
            "Train[38/50][2/240] Loss: 0.10316590964794159, BER : 0.03856640625\n",
            "Train[38/50][3/240] Loss: 0.1053299680352211, BER : 0.0396953125\n",
            "Train[38/50][4/240] Loss: 0.10612566769123077, BER : 0.04055078125\n",
            "Train[38/50][5/240] Loss: 0.1020296961069107, BER : 0.037546875\n",
            "Train[38/50][6/240] Loss: 0.10444273054599762, BER : 0.0393984375\n",
            "Train[38/50][7/240] Loss: 0.10670259594917297, BER : 0.039859375\n",
            "Train[38/50][8/240] Loss: 0.10661561787128448, BER : 0.04\n",
            "Train[38/50][9/240] Loss: 0.10514159500598907, BER : 0.03992578125\n",
            "Train[38/50][10/240] Loss: 0.1058179959654808, BER : 0.03984765625\n",
            "Train[38/50][11/240] Loss: 0.10651301592588425, BER : 0.0404140625\n",
            "Train[38/50][12/240] Loss: 0.11064339429140091, BER : 0.0423671875\n",
            "Train[38/50][13/240] Loss: 0.1072772890329361, BER : 0.04047265625\n",
            "Train[38/50][14/240] Loss: 0.10470892488956451, BER : 0.03904296875\n",
            "Train[38/50][15/240] Loss: 0.1056402325630188, BER : 0.04020703125\n",
            "Train[38/50][16/240] Loss: 0.10799100995063782, BER : 0.04112890625\n",
            "Train[38/50][17/240] Loss: 0.1041952520608902, BER : 0.03893359375\n",
            "Train[38/50][18/240] Loss: 0.10453876107931137, BER : 0.03904296875\n",
            "Train[38/50][19/240] Loss: 0.10820461809635162, BER : 0.040359375\n",
            "Train[38/50][20/240] Loss: 0.10820287466049194, BER : 0.040671875\n",
            "Train[38/50][21/240] Loss: 0.11200285702943802, BER : 0.04213671875\n",
            "Train[38/50][22/240] Loss: 0.10986357182264328, BER : 0.04212890625\n",
            "Train[38/50][23/240] Loss: 0.10736103355884552, BER : 0.0403203125\n",
            "Train[38/50][24/240] Loss: 0.10604511946439743, BER : 0.040078125\n",
            "Train[38/50][25/240] Loss: 0.10461166501045227, BER : 0.0394140625\n",
            "Train[38/50][26/240] Loss: 0.10922795534133911, BER : 0.041546875\n",
            "Train[38/50][27/240] Loss: 0.10986965149641037, BER : 0.0423359375\n",
            "Train[38/50][28/240] Loss: 0.10433441400527954, BER : 0.03841015625\n",
            "Train[38/50][29/240] Loss: 0.10626605153083801, BER : 0.04003125\n",
            "Train[38/50][30/240] Loss: 0.1067284494638443, BER : 0.04033203125\n",
            "Train[38/50][31/240] Loss: 0.10613489151000977, BER : 0.0405625\n",
            "Train[38/50][32/240] Loss: 0.10815650969743729, BER : 0.04136328125\n",
            "Train[38/50][33/240] Loss: 0.10765817016363144, BER : 0.04046484375\n",
            "Train[38/50][34/240] Loss: 0.10801105946302414, BER : 0.0415625\n",
            "Train[38/50][35/240] Loss: 0.10601390898227692, BER : 0.03932421875\n",
            "Train[38/50][36/240] Loss: 0.10832029581069946, BER : 0.04140234375\n",
            "Train[38/50][37/240] Loss: 0.10844065994024277, BER : 0.0416328125\n",
            "Train[38/50][38/240] Loss: 0.10579630732536316, BER : 0.03978125\n",
            "Train[38/50][39/240] Loss: 0.10511421412229538, BER : 0.0392109375\n",
            "Train[38/50][40/240] Loss: 0.10623843967914581, BER : 0.040015625\n",
            "Train[38/50][41/240] Loss: 0.10612396895885468, BER : 0.039703125\n",
            "Train[38/50][42/240] Loss: 0.10385008156299591, BER : 0.0386328125\n",
            "Train[38/50][43/240] Loss: 0.10886265337467194, BER : 0.041296875\n",
            "Train[38/50][44/240] Loss: 0.10792168974876404, BER : 0.040875\n",
            "Train[38/50][45/240] Loss: 0.10877041518688202, BER : 0.041515625\n",
            "Train[38/50][46/240] Loss: 0.1078704372048378, BER : 0.04078515625\n",
            "Train[38/50][47/240] Loss: 0.10730710625648499, BER : 0.04108203125\n",
            "Train[38/50][48/240] Loss: 0.10920044779777527, BER : 0.04147265625\n",
            "Train[38/50][49/240] Loss: 0.10796131193637848, BER : 0.04069140625\n",
            "Train[38/50][50/240] Loss: 0.10614312440156937, BER : 0.04066796875\n",
            "Train[38/50][51/240] Loss: 0.10581396520137787, BER : 0.0398515625\n",
            "Train[38/50][52/240] Loss: 0.10227172076702118, BER : 0.037875\n",
            "Train[38/50][53/240] Loss: 0.1046891063451767, BER : 0.0391640625\n",
            "Train[38/50][54/240] Loss: 0.1111222431063652, BER : 0.04225\n",
            "Train[38/50][55/240] Loss: 0.10436767339706421, BER : 0.0384453125\n",
            "Train[38/50][56/240] Loss: 0.10821346938610077, BER : 0.04162890625\n",
            "Train[38/50][57/240] Loss: 0.10698942840099335, BER : 0.04025390625\n",
            "Train[38/50][58/240] Loss: 0.10848765075206757, BER : 0.04123046875\n",
            "Train[38/50][59/240] Loss: 0.10532210767269135, BER : 0.03923828125\n",
            "Train[38/50][60/240] Loss: 0.10466064512729645, BER : 0.03963671875\n",
            "Train[38/50][61/240] Loss: 0.10533890128135681, BER : 0.040046875\n",
            "Train[38/50][62/240] Loss: 0.10670626163482666, BER : 0.0404453125\n",
            "Train[38/50][63/240] Loss: 0.10475455224514008, BER : 0.03954296875\n",
            "Train[38/50][64/240] Loss: 0.1089000254869461, BER : 0.04115625\n",
            "Train[38/50][65/240] Loss: 0.10645788162946701, BER : 0.04022265625\n",
            "Train[38/50][66/240] Loss: 0.10674667358398438, BER : 0.0407109375\n",
            "Train[38/50][67/240] Loss: 0.10783135890960693, BER : 0.04053125\n",
            "Train[38/50][68/240] Loss: 0.10588676482439041, BER : 0.039625\n",
            "Train[38/50][69/240] Loss: 0.10486222058534622, BER : 0.03892578125\n",
            "Train[38/50][70/240] Loss: 0.1064285933971405, BER : 0.0398671875\n",
            "Train[38/50][71/240] Loss: 0.10467073321342468, BER : 0.03930078125\n",
            "Train[38/50][72/240] Loss: 0.10549768060445786, BER : 0.04019140625\n",
            "Train[38/50][73/240] Loss: 0.10625489056110382, BER : 0.0405078125\n",
            "Train[38/50][74/240] Loss: 0.10566777735948563, BER : 0.03983984375\n",
            "Train[38/50][75/240] Loss: 0.10500462353229523, BER : 0.0395234375\n",
            "Train[38/50][76/240] Loss: 0.10545913875102997, BER : 0.03911328125\n",
            "Train[38/50][77/240] Loss: 0.10638569295406342, BER : 0.03995703125\n",
            "Train[38/50][78/240] Loss: 0.10739710927009583, BER : 0.04105078125\n",
            "Train[38/50][79/240] Loss: 0.10526106506586075, BER : 0.039671875\n",
            "Train[38/50][80/240] Loss: 0.10625738650560379, BER : 0.03955078125\n",
            "Train[38/50][81/240] Loss: 0.10695777833461761, BER : 0.0395546875\n",
            "Train[38/50][82/240] Loss: 0.10770820826292038, BER : 0.0403125\n",
            "Train[38/50][83/240] Loss: 0.10967131704092026, BER : 0.0421171875\n",
            "Train[38/50][84/240] Loss: 0.107936330139637, BER : 0.04101171875\n",
            "Train[38/50][85/240] Loss: 0.10629040002822876, BER : 0.0400078125\n",
            "Train[38/50][86/240] Loss: 0.10810253024101257, BER : 0.04046875\n",
            "Train[38/50][87/240] Loss: 0.10723161697387695, BER : 0.04058984375\n",
            "Train[38/50][88/240] Loss: 0.10329776257276535, BER : 0.03769921875\n",
            "Train[38/50][89/240] Loss: 0.10584317147731781, BER : 0.0403125\n",
            "Train[38/50][90/240] Loss: 0.1053302064538002, BER : 0.03951953125\n",
            "Train[38/50][91/240] Loss: 0.10663403570652008, BER : 0.0400625\n",
            "Train[38/50][92/240] Loss: 0.10467611998319626, BER : 0.0392265625\n",
            "Train[38/50][93/240] Loss: 0.10872064530849457, BER : 0.04097265625\n",
            "Train[38/50][94/240] Loss: 0.10839244723320007, BER : 0.041578125\n",
            "Train[38/50][95/240] Loss: 0.10746437311172485, BER : 0.04106640625\n",
            "Train[38/50][96/240] Loss: 0.10799061506986618, BER : 0.0405546875\n",
            "Train[38/50][97/240] Loss: 0.1035599410533905, BER : 0.03873828125\n",
            "Train[38/50][98/240] Loss: 0.10487266629934311, BER : 0.03900390625\n",
            "Train[38/50][99/240] Loss: 0.10709259659051895, BER : 0.04026171875\n",
            "Train[38/50][100/240] Loss: 0.10572385787963867, BER : 0.04023046875\n",
            "Train[38/50][101/240] Loss: 0.106077179312706, BER : 0.0392421875\n",
            "Train[38/50][102/240] Loss: 0.10756111890077591, BER : 0.04084765625\n",
            "Train[38/50][103/240] Loss: 0.10631529986858368, BER : 0.03991796875\n",
            "Train[38/50][104/240] Loss: 0.1062995046377182, BER : 0.03990234375\n",
            "Train[38/50][105/240] Loss: 0.10772668570280075, BER : 0.04117578125\n",
            "Train[38/50][106/240] Loss: 0.10662924498319626, BER : 0.0404296875\n",
            "Train[38/50][107/240] Loss: 0.10465075075626373, BER : 0.03890234375\n",
            "Train[38/50][108/240] Loss: 0.10595209151506424, BER : 0.03942578125\n",
            "Train[38/50][109/240] Loss: 0.10642179101705551, BER : 0.040953125\n",
            "Train[38/50][110/240] Loss: 0.10688473284244537, BER : 0.04018359375\n",
            "Train[38/50][111/240] Loss: 0.10568590462207794, BER : 0.0398671875\n",
            "Train[38/50][112/240] Loss: 0.10353430360555649, BER : 0.03871484375\n",
            "Train[38/50][113/240] Loss: 0.10475262999534607, BER : 0.03869921875\n",
            "Train[38/50][114/240] Loss: 0.10663880407810211, BER : 0.03977734375\n",
            "Train[38/50][115/240] Loss: 0.10599949210882187, BER : 0.0391796875\n",
            "Train[38/50][116/240] Loss: 0.10714289546012878, BER : 0.0407109375\n",
            "Train[38/50][117/240] Loss: 0.10620766133069992, BER : 0.03974609375\n",
            "Train[38/50][118/240] Loss: 0.107463039457798, BER : 0.04130078125\n",
            "Train[38/50][119/240] Loss: 0.1068165972828865, BER : 0.03990234375\n",
            "Train[38/50][120/240] Loss: 0.10563680529594421, BER : 0.03943359375\n",
            "Train[38/50][121/240] Loss: 0.10670442879199982, BER : 0.03999609375\n",
            "Train[38/50][122/240] Loss: 0.10424506664276123, BER : 0.03881640625\n",
            "Train[38/50][123/240] Loss: 0.10763020068407059, BER : 0.04087109375\n",
            "Train[38/50][124/240] Loss: 0.10629329830408096, BER : 0.03940234375\n",
            "Train[38/50][125/240] Loss: 0.10480330139398575, BER : 0.039140625\n",
            "Train[38/50][126/240] Loss: 0.1040448546409607, BER : 0.0388359375\n",
            "Train[38/50][127/240] Loss: 0.10951592028141022, BER : 0.0406796875\n",
            "Train[38/50][128/240] Loss: 0.10500767827033997, BER : 0.03885546875\n",
            "Train[38/50][129/240] Loss: 0.10430034250020981, BER : 0.03930859375\n",
            "Train[38/50][130/240] Loss: 0.10493982583284378, BER : 0.03869921875\n",
            "Train[38/50][131/240] Loss: 0.10437802225351334, BER : 0.038921875\n",
            "Train[38/50][132/240] Loss: 0.10379578173160553, BER : 0.03932421875\n",
            "Train[38/50][133/240] Loss: 0.1035001128911972, BER : 0.03802734375\n",
            "Train[38/50][134/240] Loss: 0.10363049060106277, BER : 0.03799609375\n",
            "Train[38/50][135/240] Loss: 0.10652872920036316, BER : 0.04078125\n",
            "Train[38/50][136/240] Loss: 0.10585299134254456, BER : 0.039359375\n",
            "Train[38/50][137/240] Loss: 0.10656005889177322, BER : 0.04\n",
            "Train[38/50][138/240] Loss: 0.10709132999181747, BER : 0.040234375\n",
            "Train[38/50][139/240] Loss: 0.10572374612092972, BER : 0.0398359375\n",
            "Train[38/50][140/240] Loss: 0.10564745962619781, BER : 0.03899609375\n",
            "Train[38/50][141/240] Loss: 0.10606816411018372, BER : 0.03996875\n",
            "Train[38/50][142/240] Loss: 0.1049143373966217, BER : 0.0397109375\n",
            "Train[38/50][143/240] Loss: 0.10598093271255493, BER : 0.03996875\n",
            "Train[38/50][144/240] Loss: 0.10611101984977722, BER : 0.0395546875\n",
            "Train[38/50][145/240] Loss: 0.10588699579238892, BER : 0.040453125\n",
            "Train[38/50][146/240] Loss: 0.10421972721815109, BER : 0.03887890625\n",
            "Train[38/50][147/240] Loss: 0.10630692541599274, BER : 0.04021875\n",
            "Train[38/50][148/240] Loss: 0.1035294234752655, BER : 0.0387421875\n",
            "Train[38/50][149/240] Loss: 0.10674557834863663, BER : 0.04008984375\n",
            "Train[38/50][150/240] Loss: 0.10493569076061249, BER : 0.03941796875\n",
            "Train[38/50][151/240] Loss: 0.1056918129324913, BER : 0.0405625\n",
            "Train[38/50][152/240] Loss: 0.10571525990962982, BER : 0.03915625\n",
            "Train[38/50][153/240] Loss: 0.10776049643754959, BER : 0.0411171875\n",
            "Train[38/50][154/240] Loss: 0.10825340449810028, BER : 0.0412734375\n",
            "Train[38/50][155/240] Loss: 0.10343500971794128, BER : 0.03808984375\n",
            "Train[38/50][156/240] Loss: 0.10615012049674988, BER : 0.03945703125\n",
            "Train[38/50][157/240] Loss: 0.10407693684101105, BER : 0.0390234375\n",
            "Train[38/50][158/240] Loss: 0.1064860001206398, BER : 0.04043359375\n",
            "Train[38/50][159/240] Loss: 0.1050182431936264, BER : 0.039125\n",
            "Train[38/50][160/240] Loss: 0.10716693103313446, BER : 0.04046875\n",
            "Train[38/50][161/240] Loss: 0.1059044599533081, BER : 0.0393046875\n",
            "Train[38/50][162/240] Loss: 0.1058480516076088, BER : 0.04007421875\n",
            "Train[38/50][163/240] Loss: 0.10844068229198456, BER : 0.04109765625\n",
            "Train[38/50][164/240] Loss: 0.10610713064670563, BER : 0.04023828125\n",
            "Train[38/50][165/240] Loss: 0.10640700161457062, BER : 0.04\n",
            "Train[38/50][166/240] Loss: 0.1073451042175293, BER : 0.04094921875\n",
            "Train[38/50][167/240] Loss: 0.10532087832689285, BER : 0.03923828125\n",
            "Train[38/50][168/240] Loss: 0.10441217571496964, BER : 0.03888671875\n",
            "Train[38/50][169/240] Loss: 0.10895884037017822, BER : 0.04158203125\n",
            "Train[38/50][170/240] Loss: 0.10599564760923386, BER : 0.04064453125\n",
            "Train[38/50][171/240] Loss: 0.10543228685855865, BER : 0.0398203125\n",
            "Train[38/50][172/240] Loss: 0.10786423087120056, BER : 0.04069921875\n",
            "Train[38/50][173/240] Loss: 0.10746100544929504, BER : 0.04090625\n",
            "Train[38/50][174/240] Loss: 0.10778725147247314, BER : 0.04105078125\n",
            "Train[38/50][175/240] Loss: 0.10831710696220398, BER : 0.04169140625\n",
            "Train[38/50][176/240] Loss: 0.1073591336607933, BER : 0.04090234375\n",
            "Train[38/50][177/240] Loss: 0.10478199273347855, BER : 0.03916015625\n",
            "Train[38/50][178/240] Loss: 0.10683874785900116, BER : 0.04028515625\n",
            "Train[38/50][179/240] Loss: 0.10755599290132523, BER : 0.04153515625\n",
            "Train[38/50][180/240] Loss: 0.10519509017467499, BER : 0.039234375\n",
            "Train[38/50][181/240] Loss: 0.1076245903968811, BER : 0.0408515625\n",
            "Train[38/50][182/240] Loss: 0.10668837279081345, BER : 0.04063671875\n",
            "Train[38/50][183/240] Loss: 0.10664889216423035, BER : 0.03998046875\n",
            "Train[38/50][184/240] Loss: 0.1088266372680664, BER : 0.041375\n",
            "Train[38/50][185/240] Loss: 0.10357929766178131, BER : 0.03751171875\n",
            "Train[38/50][186/240] Loss: 0.10568304359912872, BER : 0.03973828125\n",
            "Train[38/50][187/240] Loss: 0.10576345771551132, BER : 0.039234375\n",
            "Train[38/50][188/240] Loss: 0.10704361647367477, BER : 0.03969140625\n",
            "Train[38/50][189/240] Loss: 0.10838591307401657, BER : 0.04112109375\n",
            "Train[38/50][190/240] Loss: 0.10590961575508118, BER : 0.040140625\n",
            "Train[38/50][191/240] Loss: 0.10390195995569229, BER : 0.038828125\n",
            "Train[38/50][192/240] Loss: 0.10465782880783081, BER : 0.03894140625\n",
            "Train[38/50][193/240] Loss: 0.10764524340629578, BER : 0.0411171875\n",
            "Train[38/50][194/240] Loss: 0.10387301445007324, BER : 0.0383203125\n",
            "Train[38/50][195/240] Loss: 0.10564813017845154, BER : 0.03991015625\n",
            "Train[38/50][196/240] Loss: 0.10637244582176208, BER : 0.039859375\n",
            "Train[38/50][197/240] Loss: 0.1047518402338028, BER : 0.03949609375\n",
            "Train[38/50][198/240] Loss: 0.10463640838861465, BER : 0.0388515625\n",
            "Train[38/50][199/240] Loss: 0.10699807107448578, BER : 0.04026171875\n",
            "Train[38/50][200/240] Loss: 0.10700491070747375, BER : 0.0394765625\n",
            "Train[38/50][201/240] Loss: 0.10537512600421906, BER : 0.0397265625\n",
            "Train[38/50][202/240] Loss: 0.10821624845266342, BER : 0.04113671875\n",
            "Train[38/50][203/240] Loss: 0.10398553311824799, BER : 0.038453125\n",
            "Train[38/50][204/240] Loss: 0.1089160144329071, BER : 0.04184765625\n",
            "Train[38/50][205/240] Loss: 0.10720860958099365, BER : 0.041015625\n",
            "Train[38/50][206/240] Loss: 0.1064365804195404, BER : 0.04053125\n",
            "Train[38/50][207/240] Loss: 0.10533388704061508, BER : 0.03900390625\n",
            "Train[38/50][208/240] Loss: 0.10186225175857544, BER : 0.0377734375\n",
            "Train[38/50][209/240] Loss: 0.10236571729183197, BER : 0.0378046875\n",
            "Train[38/50][210/240] Loss: 0.11052873730659485, BER : 0.04261328125\n",
            "Train[38/50][211/240] Loss: 0.1065380722284317, BER : 0.04048828125\n",
            "Train[38/50][212/240] Loss: 0.10497286170721054, BER : 0.03962890625\n",
            "Train[38/50][213/240] Loss: 0.10598957538604736, BER : 0.04073828125\n",
            "Train[38/50][214/240] Loss: 0.10559973120689392, BER : 0.03946484375\n",
            "Train[38/50][215/240] Loss: 0.10515976697206497, BER : 0.0394921875\n",
            "Train[38/50][216/240] Loss: 0.10598457604646683, BER : 0.03984765625\n",
            "Train[38/50][217/240] Loss: 0.10263802111148834, BER : 0.03812890625\n",
            "Train[38/50][218/240] Loss: 0.10514014959335327, BER : 0.0399375\n",
            "Train[38/50][219/240] Loss: 0.10490521043539047, BER : 0.03936328125\n",
            "Train[38/50][220/240] Loss: 0.10618072748184204, BER : 0.0406953125\n",
            "Train[38/50][221/240] Loss: 0.10788225382566452, BER : 0.04096484375\n",
            "Train[38/50][222/240] Loss: 0.10586004704236984, BER : 0.03964453125\n",
            "Train[38/50][223/240] Loss: 0.10791495442390442, BER : 0.0415\n",
            "Train[38/50][224/240] Loss: 0.10422245413064957, BER : 0.03973828125\n",
            "Train[38/50][225/240] Loss: 0.10408207774162292, BER : 0.03894921875\n",
            "Train[38/50][226/240] Loss: 0.10422760248184204, BER : 0.0388671875\n",
            "Train[38/50][227/240] Loss: 0.10641323029994965, BER : 0.0402578125\n",
            "Train[38/50][228/240] Loss: 0.10591093450784683, BER : 0.03975\n",
            "Train[38/50][229/240] Loss: 0.10619400441646576, BER : 0.04030859375\n",
            "Train[38/50][230/240] Loss: 0.10438084602355957, BER : 0.0395\n",
            "Train[38/50][231/240] Loss: 0.10522519052028656, BER : 0.03887890625\n",
            "Train[38/50][232/240] Loss: 0.10622941702604294, BER : 0.0401640625\n",
            "Train[38/50][233/240] Loss: 0.10857787728309631, BER : 0.04069921875\n",
            "Train[38/50][234/240] Loss: 0.10581211000680923, BER : 0.03948828125\n",
            "Train[38/50][235/240] Loss: 0.10297593474388123, BER : 0.03887890625\n",
            "Train[38/50][236/240] Loss: 0.10783509910106659, BER : 0.0409921875\n",
            "Train[38/50][237/240] Loss: 0.10481084883213043, BER : 0.0393828125\n",
            "Train[38/50][238/240] Loss: 0.10573391616344452, BER : 0.0398125\n",
            "Train[38/50][239/240] Loss: 0.10623499751091003, BER : 0.040515625\n",
            "Epoch [38/50] Train_Avg_loss(epoch): 0.27584\n",
            "-------------------------------------\n",
            "Test[38/50][0/40]  Loss: 0.08012057840824127, BER (test): 0.04218359375\n",
            "Test[38/50][1/40]  Loss: 0.0800478607416153, BER (test): 0.04136328125\n",
            "Test[38/50][2/40]  Loss: 0.08072233945131302, BER (test): 0.0415625\n",
            "Test[38/50][3/40]  Loss: 0.08085121214389801, BER (test): 0.0424453125\n",
            "Test[38/50][4/40]  Loss: 0.07964617758989334, BER (test): 0.04191015625\n",
            "Test[38/50][5/40]  Loss: 0.07883584499359131, BER (test): 0.04158984375\n",
            "Test[38/50][6/40]  Loss: 0.07936014980077744, BER (test): 0.0414453125\n",
            "Test[38/50][7/40]  Loss: 0.07948435097932816, BER (test): 0.04120703125\n",
            "Test[38/50][8/40]  Loss: 0.07877351343631744, BER (test): 0.040453125\n",
            "Test[38/50][9/40]  Loss: 0.08422911167144775, BER (test): 0.0442421875\n",
            "Test[38/50][10/40]  Loss: 0.08259668946266174, BER (test): 0.04260546875\n",
            "Test[38/50][11/40]  Loss: 0.0833965539932251, BER (test): 0.0440078125\n",
            "Test[38/50][12/40]  Loss: 0.08251338452100754, BER (test): 0.042953125\n",
            "Test[38/50][13/40]  Loss: 0.08202002942562103, BER (test): 0.043296875\n",
            "Test[38/50][14/40]  Loss: 0.08157681673765182, BER (test): 0.042203125\n",
            "Test[38/50][15/40]  Loss: 0.07965102791786194, BER (test): 0.04133984375\n",
            "Test[38/50][16/40]  Loss: 0.08203285932540894, BER (test): 0.0425625\n",
            "Test[38/50][17/40]  Loss: 0.08238868415355682, BER (test): 0.042875\n",
            "Test[38/50][18/40]  Loss: 0.0774201825261116, BER (test): 0.04042578125\n",
            "Test[38/50][19/40]  Loss: 0.0806170403957367, BER (test): 0.04231640625\n",
            "Test[38/50][20/40]  Loss: 0.07796989381313324, BER (test): 0.040390625\n",
            "Test[38/50][21/40]  Loss: 0.08161745220422745, BER (test): 0.04204296875\n",
            "Test[38/50][22/40]  Loss: 0.0799010619521141, BER (test): 0.0416328125\n",
            "Test[38/50][23/40]  Loss: 0.08028464019298553, BER (test): 0.042265625\n",
            "Test[38/50][24/40]  Loss: 0.0816095694899559, BER (test): 0.04237890625\n",
            "Test[38/50][25/40]  Loss: 0.07968434691429138, BER (test): 0.04117578125\n",
            "Test[38/50][26/40]  Loss: 0.08030770719051361, BER (test): 0.04219140625\n",
            "Test[38/50][27/40]  Loss: 0.08259449899196625, BER (test): 0.04378515625\n",
            "Test[38/50][28/40]  Loss: 0.08061829209327698, BER (test): 0.04159765625\n",
            "Test[38/50][29/40]  Loss: 0.08167824149131775, BER (test): 0.04249609375\n",
            "Test[38/50][30/40]  Loss: 0.07999304682016373, BER (test): 0.04216015625\n",
            "Test[38/50][31/40]  Loss: 0.08226066827774048, BER (test): 0.04267578125\n",
            "Test[38/50][32/40]  Loss: 0.08267071098089218, BER (test): 0.0432109375\n",
            "Test[38/50][33/40]  Loss: 0.08113328367471695, BER (test): 0.0426953125\n",
            "Test[38/50][34/40]  Loss: 0.08244667202234268, BER (test): 0.04252734375\n",
            "Test[38/50][35/40]  Loss: 0.08156000822782516, BER (test): 0.0430546875\n",
            "Test[38/50][36/40]  Loss: 0.081939697265625, BER (test): 0.043203125\n",
            "Test[38/50][37/40]  Loss: 0.07932736724615097, BER (test): 0.04078515625\n",
            "Test[38/50][38/40]  Loss: 0.08184507489204407, BER (test): 0.04268359375\n",
            "Test[38/50][39/40]  Loss: 0.08060687780380249, BER (test): 0.04209765625\n",
            "Test_Epoch [38/50] Test_Avg_loss(epoch): 0.08091\n",
            "Train[39/50][0/240] Loss: 0.10687953233718872, BER : 0.04134375\n",
            "Train[39/50][1/240] Loss: 0.10502399504184723, BER : 0.03926953125\n",
            "Train[39/50][2/240] Loss: 0.10572206974029541, BER : 0.0399140625\n",
            "Train[39/50][3/240] Loss: 0.10730069130659103, BER : 0.04061328125\n",
            "Train[39/50][4/240] Loss: 0.10515142977237701, BER : 0.04014453125\n",
            "Train[39/50][5/240] Loss: 0.10303311049938202, BER : 0.03887890625\n",
            "Train[39/50][6/240] Loss: 0.10414235293865204, BER : 0.03894140625\n",
            "Train[39/50][7/240] Loss: 0.10533028841018677, BER : 0.04010546875\n",
            "Train[39/50][8/240] Loss: 0.10224372893571854, BER : 0.03762109375\n",
            "Train[39/50][9/240] Loss: 0.10396292805671692, BER : 0.0385625\n",
            "Train[39/50][10/240] Loss: 0.10557331144809723, BER : 0.03996875\n",
            "Train[39/50][11/240] Loss: 0.10739252716302872, BER : 0.04127734375\n",
            "Train[39/50][12/240] Loss: 0.1062183678150177, BER : 0.04025390625\n",
            "Train[39/50][13/240] Loss: 0.10617975890636444, BER : 0.03940625\n",
            "Train[39/50][14/240] Loss: 0.10784424096345901, BER : 0.04156640625\n",
            "Train[39/50][15/240] Loss: 0.10551907867193222, BER : 0.039203125\n",
            "Train[39/50][16/240] Loss: 0.10774149745702744, BER : 0.04113671875\n",
            "Train[39/50][17/240] Loss: 0.10706120729446411, BER : 0.04057421875\n",
            "Train[39/50][18/240] Loss: 0.10672825574874878, BER : 0.0404140625\n",
            "Train[39/50][19/240] Loss: 0.1066390722990036, BER : 0.04026953125\n",
            "Train[39/50][20/240] Loss: 0.10405703634023666, BER : 0.03869140625\n",
            "Train[39/50][21/240] Loss: 0.10643095523118973, BER : 0.0403515625\n",
            "Train[39/50][22/240] Loss: 0.10694742947816849, BER : 0.04057421875\n",
            "Train[39/50][23/240] Loss: 0.10585883259773254, BER : 0.04005078125\n",
            "Train[39/50][24/240] Loss: 0.10670176148414612, BER : 0.04065625\n",
            "Train[39/50][25/240] Loss: 0.10682019591331482, BER : 0.0400078125\n",
            "Train[39/50][26/240] Loss: 0.10610398650169373, BER : 0.0408671875\n",
            "Train[39/50][27/240] Loss: 0.11045943200588226, BER : 0.04229296875\n",
            "Train[39/50][28/240] Loss: 0.10696019232273102, BER : 0.04139453125\n",
            "Train[39/50][29/240] Loss: 0.10726343095302582, BER : 0.0416328125\n",
            "Train[39/50][30/240] Loss: 0.10182748734951019, BER : 0.0369609375\n",
            "Train[39/50][31/240] Loss: 0.106488436460495, BER : 0.04\n",
            "Train[39/50][32/240] Loss: 0.10644499212503433, BER : 0.04044921875\n",
            "Train[39/50][33/240] Loss: 0.1064065545797348, BER : 0.0406640625\n",
            "Train[39/50][34/240] Loss: 0.10513756424188614, BER : 0.03996484375\n",
            "Train[39/50][35/240] Loss: 0.10506467521190643, BER : 0.0394296875\n",
            "Train[39/50][36/240] Loss: 0.10525473207235336, BER : 0.03929296875\n",
            "Train[39/50][37/240] Loss: 0.10550778359174728, BER : 0.03966796875\n",
            "Train[39/50][38/240] Loss: 0.10613827407360077, BER : 0.0395\n",
            "Train[39/50][39/240] Loss: 0.10830453783273697, BER : 0.04152734375\n",
            "Train[39/50][40/240] Loss: 0.10270090401172638, BER : 0.03816796875\n",
            "Train[39/50][41/240] Loss: 0.10584941506385803, BER : 0.0397109375\n",
            "Train[39/50][42/240] Loss: 0.10516821593046188, BER : 0.03922265625\n",
            "Train[39/50][43/240] Loss: 0.10733090341091156, BER : 0.03960546875\n",
            "Train[39/50][44/240] Loss: 0.10441185534000397, BER : 0.039203125\n",
            "Train[39/50][45/240] Loss: 0.1055733934044838, BER : 0.03942578125\n",
            "Train[39/50][46/240] Loss: 0.10482288897037506, BER : 0.0389921875\n",
            "Train[39/50][47/240] Loss: 0.10487468540668488, BER : 0.03844140625\n",
            "Train[39/50][48/240] Loss: 0.10827122628688812, BER : 0.04071484375\n",
            "Train[39/50][49/240] Loss: 0.10780687630176544, BER : 0.0411640625\n",
            "Train[39/50][50/240] Loss: 0.1074296087026596, BER : 0.04085546875\n",
            "Train[39/50][51/240] Loss: 0.10509521514177322, BER : 0.03996875\n",
            "Train[39/50][52/240] Loss: 0.1079966202378273, BER : 0.04053125\n",
            "Train[39/50][53/240] Loss: 0.10280360281467438, BER : 0.03809375\n",
            "Train[39/50][54/240] Loss: 0.10685417056083679, BER : 0.03960546875\n",
            "Train[39/50][55/240] Loss: 0.1028037890791893, BER : 0.03782421875\n",
            "Train[39/50][56/240] Loss: 0.10326369106769562, BER : 0.0383359375\n",
            "Train[39/50][57/240] Loss: 0.10928478091955185, BER : 0.04202734375\n",
            "Train[39/50][58/240] Loss: 0.10583920776844025, BER : 0.03994921875\n",
            "Train[39/50][59/240] Loss: 0.10160870105028152, BER : 0.03731640625\n",
            "Train[39/50][60/240] Loss: 0.10672048479318619, BER : 0.04032421875\n",
            "Train[39/50][61/240] Loss: 0.10487639158964157, BER : 0.03872265625\n",
            "Train[39/50][62/240] Loss: 0.10701548308134079, BER : 0.04081640625\n",
            "Train[39/50][63/240] Loss: 0.10605968534946442, BER : 0.04044921875\n",
            "Train[39/50][64/240] Loss: 0.10447181016206741, BER : 0.03996875\n",
            "Train[39/50][65/240] Loss: 0.10504502803087234, BER : 0.03898828125\n",
            "Train[39/50][66/240] Loss: 0.10494164377450943, BER : 0.0393203125\n",
            "Train[39/50][67/240] Loss: 0.10351460427045822, BER : 0.03894140625\n",
            "Train[39/50][68/240] Loss: 0.10686666518449783, BER : 0.0411484375\n",
            "Train[39/50][69/240] Loss: 0.10890214145183563, BER : 0.0414765625\n",
            "Train[39/50][70/240] Loss: 0.10557302832603455, BER : 0.03955078125\n",
            "Train[39/50][71/240] Loss: 0.10818003863096237, BER : 0.0408515625\n",
            "Train[39/50][72/240] Loss: 0.10653902590274811, BER : 0.04060546875\n",
            "Train[39/50][73/240] Loss: 0.10503491759300232, BER : 0.03959765625\n",
            "Train[39/50][74/240] Loss: 0.10664437711238861, BER : 0.0402890625\n",
            "Train[39/50][75/240] Loss: 0.10496865212917328, BER : 0.0386796875\n",
            "Train[39/50][76/240] Loss: 0.10843238979578018, BER : 0.041171875\n",
            "Train[39/50][77/240] Loss: 0.10534964501857758, BER : 0.0388671875\n",
            "Train[39/50][78/240] Loss: 0.1044001430273056, BER : 0.0390859375\n",
            "Train[39/50][79/240] Loss: 0.10291798412799835, BER : 0.03856640625\n",
            "Train[39/50][80/240] Loss: 0.10814735293388367, BER : 0.04123046875\n",
            "Train[39/50][81/240] Loss: 0.10928597301244736, BER : 0.04128125\n",
            "Train[39/50][82/240] Loss: 0.10716743022203445, BER : 0.0396875\n",
            "Train[39/50][83/240] Loss: 0.10472674667835236, BER : 0.03946875\n",
            "Train[39/50][84/240] Loss: 0.10446669161319733, BER : 0.0391875\n",
            "Train[39/50][85/240] Loss: 0.10890018194913864, BER : 0.0411953125\n",
            "Train[39/50][86/240] Loss: 0.10624749958515167, BER : 0.0403046875\n",
            "Train[39/50][87/240] Loss: 0.10842300206422806, BER : 0.0411796875\n",
            "Train[39/50][88/240] Loss: 0.10714859515428543, BER : 0.0405390625\n",
            "Train[39/50][89/240] Loss: 0.1070786714553833, BER : 0.04075\n",
            "Train[39/50][90/240] Loss: 0.10539735853672028, BER : 0.04012109375\n",
            "Train[39/50][91/240] Loss: 0.1064683198928833, BER : 0.04019140625\n",
            "Train[39/50][92/240] Loss: 0.10786932706832886, BER : 0.0406640625\n",
            "Train[39/50][93/240] Loss: 0.10374561697244644, BER : 0.0388984375\n",
            "Train[39/50][94/240] Loss: 0.10525412112474442, BER : 0.0392890625\n",
            "Train[39/50][95/240] Loss: 0.1061592549085617, BER : 0.040125\n",
            "Train[39/50][96/240] Loss: 0.10826516151428223, BER : 0.0413125\n",
            "Train[39/50][97/240] Loss: 0.10672181099653244, BER : 0.03922265625\n",
            "Train[39/50][98/240] Loss: 0.10992339998483658, BER : 0.04150390625\n",
            "Train[39/50][99/240] Loss: 0.10682348906993866, BER : 0.0401171875\n",
            "Train[39/50][100/240] Loss: 0.1050662249326706, BER : 0.038921875\n",
            "Train[39/50][101/240] Loss: 0.10889813303947449, BER : 0.04151953125\n",
            "Train[39/50][102/240] Loss: 0.11004370450973511, BER : 0.0418828125\n",
            "Train[39/50][103/240] Loss: 0.10565802454948425, BER : 0.03965234375\n",
            "Train[39/50][104/240] Loss: 0.10521608591079712, BER : 0.04023046875\n",
            "Train[39/50][105/240] Loss: 0.10358444601297379, BER : 0.0381953125\n",
            "Train[39/50][106/240] Loss: 0.10587065666913986, BER : 0.039515625\n",
            "Train[39/50][107/240] Loss: 0.10754801332950592, BER : 0.04069140625\n",
            "Train[39/50][108/240] Loss: 0.1060243770480156, BER : 0.03923046875\n",
            "Train[39/50][109/240] Loss: 0.10425850749015808, BER : 0.03881640625\n",
            "Train[39/50][110/240] Loss: 0.10843253135681152, BER : 0.04084375\n",
            "Train[39/50][111/240] Loss: 0.10823683440685272, BER : 0.04121875\n",
            "Train[39/50][112/240] Loss: 0.10702666640281677, BER : 0.04033203125\n",
            "Train[39/50][113/240] Loss: 0.10490627586841583, BER : 0.03945703125\n",
            "Train[39/50][114/240] Loss: 0.1076265498995781, BER : 0.04094140625\n",
            "Train[39/50][115/240] Loss: 0.10546006262302399, BER : 0.03927734375\n",
            "Train[39/50][116/240] Loss: 0.10300923138856888, BER : 0.0386796875\n",
            "Train[39/50][117/240] Loss: 0.10625830292701721, BER : 0.0398984375\n",
            "Train[39/50][118/240] Loss: 0.10621042549610138, BER : 0.04001171875\n",
            "Train[39/50][119/240] Loss: 0.10621583461761475, BER : 0.0394296875\n",
            "Train[39/50][120/240] Loss: 0.10469846427440643, BER : 0.0394609375\n",
            "Train[39/50][121/240] Loss: 0.10634110867977142, BER : 0.03986328125\n",
            "Train[39/50][122/240] Loss: 0.10642077773809433, BER : 0.04086328125\n",
            "Train[39/50][123/240] Loss: 0.10511554777622223, BER : 0.03956640625\n",
            "Train[39/50][124/240] Loss: 0.10581882297992706, BER : 0.03908203125\n",
            "Train[39/50][125/240] Loss: 0.10486030578613281, BER : 0.03856640625\n",
            "Train[39/50][126/240] Loss: 0.10686875879764557, BER : 0.0399453125\n",
            "Train[39/50][127/240] Loss: 0.10391417145729065, BER : 0.03830859375\n",
            "Train[39/50][128/240] Loss: 0.10685227066278458, BER : 0.04060546875\n",
            "Train[39/50][129/240] Loss: 0.10541507601737976, BER : 0.03933984375\n",
            "Train[39/50][130/240] Loss: 0.10720226913690567, BER : 0.04057421875\n",
            "Train[39/50][131/240] Loss: 0.10903578251600266, BER : 0.04133984375\n",
            "Train[39/50][132/240] Loss: 0.107930988073349, BER : 0.04097265625\n",
            "Train[39/50][133/240] Loss: 0.1078159362077713, BER : 0.04048828125\n",
            "Train[39/50][134/240] Loss: 0.10370007157325745, BER : 0.038546875\n",
            "Train[39/50][135/240] Loss: 0.10467878729104996, BER : 0.03935546875\n",
            "Train[39/50][136/240] Loss: 0.10333073139190674, BER : 0.038078125\n",
            "Train[39/50][137/240] Loss: 0.10623354464769363, BER : 0.039375\n",
            "Train[39/50][138/240] Loss: 0.10751594603061676, BER : 0.04017578125\n",
            "Train[39/50][139/240] Loss: 0.10652987658977509, BER : 0.040578125\n",
            "Train[39/50][140/240] Loss: 0.10746090859174728, BER : 0.040140625\n",
            "Train[39/50][141/240] Loss: 0.10466784238815308, BER : 0.03948046875\n",
            "Train[39/50][142/240] Loss: 0.10525566339492798, BER : 0.03923046875\n",
            "Train[39/50][143/240] Loss: 0.10438836365938187, BER : 0.0384609375\n",
            "Train[39/50][144/240] Loss: 0.10476535558700562, BER : 0.03926171875\n",
            "Train[39/50][145/240] Loss: 0.10638867318630219, BER : 0.04028125\n",
            "Train[39/50][146/240] Loss: 0.10532620549201965, BER : 0.039578125\n",
            "Train[39/50][147/240] Loss: 0.10494783520698547, BER : 0.03906640625\n",
            "Train[39/50][148/240] Loss: 0.10505088418722153, BER : 0.0393046875\n",
            "Train[39/50][149/240] Loss: 0.10353510081768036, BER : 0.03840625\n",
            "Train[39/50][150/240] Loss: 0.1080499142408371, BER : 0.0415\n",
            "Train[39/50][151/240] Loss: 0.10704457014799118, BER : 0.039796875\n",
            "Train[39/50][152/240] Loss: 0.10561078041791916, BER : 0.0391953125\n",
            "Train[39/50][153/240] Loss: 0.10469283163547516, BER : 0.038921875\n",
            "Train[39/50][154/240] Loss: 0.1063607782125473, BER : 0.039953125\n",
            "Train[39/50][155/240] Loss: 0.10432139039039612, BER : 0.03828125\n",
            "Train[39/50][156/240] Loss: 0.10836738348007202, BER : 0.04174609375\n",
            "Train[39/50][157/240] Loss: 0.10759066045284271, BER : 0.04138671875\n",
            "Train[39/50][158/240] Loss: 0.10504497587680817, BER : 0.03919140625\n",
            "Train[39/50][159/240] Loss: 0.10501068830490112, BER : 0.03885546875\n",
            "Train[39/50][160/240] Loss: 0.10302257537841797, BER : 0.03858203125\n",
            "Train[39/50][161/240] Loss: 0.10486870259046555, BER : 0.039859375\n",
            "Train[39/50][162/240] Loss: 0.10695779323577881, BER : 0.04090625\n",
            "Train[39/50][163/240] Loss: 0.10376980900764465, BER : 0.0385078125\n",
            "Train[39/50][164/240] Loss: 0.10551155358552933, BER : 0.03959765625\n",
            "Train[39/50][165/240] Loss: 0.10633350908756256, BER : 0.03947265625\n",
            "Train[39/50][166/240] Loss: 0.10350047051906586, BER : 0.03828125\n",
            "Train[39/50][167/240] Loss: 0.10704344511032104, BER : 0.0407890625\n",
            "Train[39/50][168/240] Loss: 0.10460926592350006, BER : 0.039859375\n",
            "Train[39/50][169/240] Loss: 0.10712693631649017, BER : 0.04058984375\n",
            "Train[39/50][170/240] Loss: 0.10235343873500824, BER : 0.03769921875\n",
            "Train[39/50][171/240] Loss: 0.1046232059597969, BER : 0.03920703125\n",
            "Train[39/50][172/240] Loss: 0.10676121711730957, BER : 0.04069921875\n",
            "Train[39/50][173/240] Loss: 0.1042533591389656, BER : 0.039046875\n",
            "Train[39/50][174/240] Loss: 0.10602805018424988, BER : 0.0401484375\n",
            "Train[39/50][175/240] Loss: 0.10754033178091049, BER : 0.04053125\n",
            "Train[39/50][176/240] Loss: 0.1058342382311821, BER : 0.03980078125\n",
            "Train[39/50][177/240] Loss: 0.10555900633335114, BER : 0.03999609375\n",
            "Train[39/50][178/240] Loss: 0.10468682646751404, BER : 0.0390703125\n",
            "Train[39/50][179/240] Loss: 0.1024932786822319, BER : 0.0386796875\n",
            "Train[39/50][180/240] Loss: 0.1070263683795929, BER : 0.040140625\n",
            "Train[39/50][181/240] Loss: 0.10388251394033432, BER : 0.03857421875\n",
            "Train[39/50][182/240] Loss: 0.1036185696721077, BER : 0.03812109375\n",
            "Train[39/50][183/240] Loss: 0.10785487294197083, BER : 0.04141796875\n",
            "Train[39/50][184/240] Loss: 0.1034933403134346, BER : 0.03863671875\n",
            "Train[39/50][185/240] Loss: 0.10845649242401123, BER : 0.04089453125\n",
            "Train[39/50][186/240] Loss: 0.10654222220182419, BER : 0.04076171875\n",
            "Train[39/50][187/240] Loss: 0.10659216344356537, BER : 0.04076953125\n",
            "Train[39/50][188/240] Loss: 0.10500208288431168, BER : 0.039703125\n",
            "Train[39/50][189/240] Loss: 0.10527315735816956, BER : 0.04042578125\n",
            "Train[39/50][190/240] Loss: 0.10655348002910614, BER : 0.04058203125\n",
            "Train[39/50][191/240] Loss: 0.10229621082544327, BER : 0.0384140625\n",
            "Train[39/50][192/240] Loss: 0.1044340506196022, BER : 0.03840234375\n",
            "Train[39/50][193/240] Loss: 0.10636694729328156, BER : 0.039515625\n",
            "Train[39/50][194/240] Loss: 0.1058650091290474, BER : 0.0404921875\n",
            "Train[39/50][195/240] Loss: 0.10578777641057968, BER : 0.0397109375\n",
            "Train[39/50][196/240] Loss: 0.10381679236888885, BER : 0.0387890625\n",
            "Train[39/50][197/240] Loss: 0.10632479190826416, BER : 0.03986328125\n",
            "Train[39/50][198/240] Loss: 0.10629327595233917, BER : 0.04086328125\n",
            "Train[39/50][199/240] Loss: 0.10684841126203537, BER : 0.040171875\n",
            "Train[39/50][200/240] Loss: 0.10979223996400833, BER : 0.04150390625\n",
            "Train[39/50][201/240] Loss: 0.10719794034957886, BER : 0.0403203125\n",
            "Train[39/50][202/240] Loss: 0.10627850145101547, BER : 0.040203125\n",
            "Train[39/50][203/240] Loss: 0.10323536396026611, BER : 0.0383671875\n",
            "Train[39/50][204/240] Loss: 0.10573171824216843, BER : 0.0390546875\n",
            "Train[39/50][205/240] Loss: 0.10624013096094131, BER : 0.04012109375\n",
            "Train[39/50][206/240] Loss: 0.1039213314652443, BER : 0.0385078125\n",
            "Train[39/50][207/240] Loss: 0.10654331743717194, BER : 0.0402890625\n",
            "Train[39/50][208/240] Loss: 0.10290271788835526, BER : 0.0383046875\n",
            "Train[39/50][209/240] Loss: 0.1025165244936943, BER : 0.03794921875\n",
            "Train[39/50][210/240] Loss: 0.10289222747087479, BER : 0.03815625\n",
            "Train[39/50][211/240] Loss: 0.10312707722187042, BER : 0.03826953125\n",
            "Train[39/50][212/240] Loss: 0.10644511878490448, BER : 0.04069921875\n",
            "Train[39/50][213/240] Loss: 0.10604394972324371, BER : 0.03970703125\n",
            "Train[39/50][214/240] Loss: 0.10416582971811295, BER : 0.03878125\n",
            "Train[39/50][215/240] Loss: 0.10703358054161072, BER : 0.04087890625\n",
            "Train[39/50][216/240] Loss: 0.10436485707759857, BER : 0.03921484375\n",
            "Train[39/50][217/240] Loss: 0.10494846850633621, BER : 0.04005078125\n",
            "Train[39/50][218/240] Loss: 0.10546150803565979, BER : 0.03935546875\n",
            "Train[39/50][219/240] Loss: 0.10446575284004211, BER : 0.03808203125\n",
            "Train[39/50][220/240] Loss: 0.10594074428081512, BER : 0.03957421875\n",
            "Train[39/50][221/240] Loss: 0.10617396235466003, BER : 0.0404140625\n",
            "Train[39/50][222/240] Loss: 0.10428819060325623, BER : 0.03840625\n",
            "Train[39/50][223/240] Loss: 0.10546139627695084, BER : 0.03953515625\n",
            "Train[39/50][224/240] Loss: 0.1077909842133522, BER : 0.0408828125\n",
            "Train[39/50][225/240] Loss: 0.1047782227396965, BER : 0.0392421875\n",
            "Train[39/50][226/240] Loss: 0.10492852330207825, BER : 0.03951171875\n",
            "Train[39/50][227/240] Loss: 0.10489822179079056, BER : 0.03943359375\n",
            "Train[39/50][228/240] Loss: 0.10504535585641861, BER : 0.039921875\n",
            "Train[39/50][229/240] Loss: 0.10277155786752701, BER : 0.0381328125\n",
            "Train[39/50][230/240] Loss: 0.1052468866109848, BER : 0.03944921875\n",
            "Train[39/50][231/240] Loss: 0.10569813847541809, BER : 0.0398984375\n",
            "Train[39/50][232/240] Loss: 0.10749127715826035, BER : 0.0405546875\n",
            "Train[39/50][233/240] Loss: 0.10772333294153214, BER : 0.04107421875\n",
            "Train[39/50][234/240] Loss: 0.10660049319267273, BER : 0.04019140625\n",
            "Train[39/50][235/240] Loss: 0.10508511960506439, BER : 0.03930859375\n",
            "Train[39/50][236/240] Loss: 0.1039799302816391, BER : 0.039390625\n",
            "Train[39/50][237/240] Loss: 0.10694919526576996, BER : 0.040578125\n",
            "Train[39/50][238/240] Loss: 0.10444260388612747, BER : 0.0389375\n",
            "Train[39/50][239/240] Loss: 0.10420727729797363, BER : 0.03899609375\n",
            "Epoch [39/50] Train_Avg_loss(epoch): 0.27159\n",
            "-------------------------------------\n",
            "Test[39/50][0/40]  Loss: 0.07779993116855621, BER (test): 0.04188671875\n",
            "Test[39/50][1/40]  Loss: 0.0769256204366684, BER (test): 0.04101171875\n",
            "Test[39/50][2/40]  Loss: 0.0776999369263649, BER (test): 0.04144140625\n",
            "Test[39/50][3/40]  Loss: 0.07779280096292496, BER (test): 0.041890625\n",
            "Test[39/50][4/40]  Loss: 0.07639115303754807, BER (test): 0.0413828125\n",
            "Test[39/50][5/40]  Loss: 0.0784231573343277, BER (test): 0.04231640625\n",
            "Test[39/50][6/40]  Loss: 0.07658937573432922, BER (test): 0.04168359375\n",
            "Test[39/50][7/40]  Loss: 0.0793888121843338, BER (test): 0.04290625\n",
            "Test[39/50][8/40]  Loss: 0.07743765413761139, BER (test): 0.04187890625\n",
            "Test[39/50][9/40]  Loss: 0.0801476538181305, BER (test): 0.04318359375\n",
            "Test[39/50][10/40]  Loss: 0.07794073224067688, BER (test): 0.0422421875\n",
            "Test[39/50][11/40]  Loss: 0.07851219177246094, BER (test): 0.04197265625\n",
            "Test[39/50][12/40]  Loss: 0.07875905930995941, BER (test): 0.04312890625\n",
            "Test[39/50][13/40]  Loss: 0.07753494381904602, BER (test): 0.0414609375\n",
            "Test[39/50][14/40]  Loss: 0.07798204571008682, BER (test): 0.0417578125\n",
            "Test[39/50][15/40]  Loss: 0.07823341339826584, BER (test): 0.0421328125\n",
            "Test[39/50][16/40]  Loss: 0.07795437425374985, BER (test): 0.04215234375\n",
            "Test[39/50][17/40]  Loss: 0.07983492314815521, BER (test): 0.0427890625\n",
            "Test[39/50][18/40]  Loss: 0.07795524597167969, BER (test): 0.04250390625\n",
            "Test[39/50][19/40]  Loss: 0.07778339833021164, BER (test): 0.04241015625\n",
            "Test[39/50][20/40]  Loss: 0.07932563126087189, BER (test): 0.0428515625\n",
            "Test[39/50][21/40]  Loss: 0.07689335197210312, BER (test): 0.04149609375\n",
            "Test[39/50][22/40]  Loss: 0.08095551282167435, BER (test): 0.04316796875\n",
            "Test[39/50][23/40]  Loss: 0.07608189433813095, BER (test): 0.0404296875\n",
            "Test[39/50][24/40]  Loss: 0.07853789627552032, BER (test): 0.04191796875\n",
            "Test[39/50][25/40]  Loss: 0.07993970066308975, BER (test): 0.0431953125\n",
            "Test[39/50][26/40]  Loss: 0.07843051105737686, BER (test): 0.04341796875\n",
            "Test[39/50][27/40]  Loss: 0.07767903059720993, BER (test): 0.0421171875\n",
            "Test[39/50][28/40]  Loss: 0.07700355350971222, BER (test): 0.04199609375\n",
            "Test[39/50][29/40]  Loss: 0.0784483551979065, BER (test): 0.04240234375\n",
            "Test[39/50][30/40]  Loss: 0.07978738099336624, BER (test): 0.04312890625\n",
            "Test[39/50][31/40]  Loss: 0.07744546234607697, BER (test): 0.0423125\n",
            "Test[39/50][32/40]  Loss: 0.0786023661494255, BER (test): 0.04308203125\n",
            "Test[39/50][33/40]  Loss: 0.07575465738773346, BER (test): 0.04109375\n",
            "Test[39/50][34/40]  Loss: 0.07837850600481033, BER (test): 0.0426640625\n",
            "Test[39/50][35/40]  Loss: 0.07738576829433441, BER (test): 0.04155859375\n",
            "Test[39/50][36/40]  Loss: 0.08120249211788177, BER (test): 0.04372265625\n",
            "Test[39/50][37/40]  Loss: 0.07919501513242722, BER (test): 0.04287109375\n",
            "Test[39/50][38/40]  Loss: 0.07579518854618073, BER (test): 0.04071875\n",
            "Test[39/50][39/40]  Loss: 0.08231788873672485, BER (test): 0.0445\n",
            "Test_Epoch [39/50] Test_Avg_loss(epoch): 0.07826\n",
            "Train[40/50][0/240] Loss: 0.10581892728805542, BER : 0.0399609375\n",
            "Train[40/50][1/240] Loss: 0.10632943361997604, BER : 0.0404375\n",
            "Train[40/50][2/240] Loss: 0.10356555879116058, BER : 0.03872265625\n",
            "Train[40/50][3/240] Loss: 0.10405432432889938, BER : 0.0388125\n",
            "Train[40/50][4/240] Loss: 0.10455584526062012, BER : 0.038953125\n",
            "Train[40/50][5/240] Loss: 0.10214938223361969, BER : 0.03769921875\n",
            "Train[40/50][6/240] Loss: 0.10257593542337418, BER : 0.0378828125\n",
            "Train[40/50][7/240] Loss: 0.10369394719600677, BER : 0.038265625\n",
            "Train[40/50][8/240] Loss: 0.1073104739189148, BER : 0.04150390625\n",
            "Train[40/50][9/240] Loss: 0.10501786321401596, BER : 0.0395703125\n",
            "Train[40/50][10/240] Loss: 0.10524176061153412, BER : 0.03987109375\n",
            "Train[40/50][11/240] Loss: 0.10430315136909485, BER : 0.03920703125\n",
            "Train[40/50][12/240] Loss: 0.10079838335514069, BER : 0.0371875\n",
            "Train[40/50][13/240] Loss: 0.10248367488384247, BER : 0.03882421875\n",
            "Train[40/50][14/240] Loss: 0.10535271465778351, BER : 0.0400703125\n",
            "Train[40/50][15/240] Loss: 0.10371673852205276, BER : 0.0385625\n",
            "Train[40/50][16/240] Loss: 0.10586052387952805, BER : 0.0395\n",
            "Train[40/50][17/240] Loss: 0.10386653989553452, BER : 0.0386015625\n",
            "Train[40/50][18/240] Loss: 0.10574688017368317, BER : 0.04075390625\n",
            "Train[40/50][19/240] Loss: 0.1082453802227974, BER : 0.040859375\n",
            "Train[40/50][20/240] Loss: 0.10649731755256653, BER : 0.0408515625\n",
            "Train[40/50][21/240] Loss: 0.10700991749763489, BER : 0.0403125\n",
            "Train[40/50][22/240] Loss: 0.10761700570583344, BER : 0.0415546875\n",
            "Train[40/50][23/240] Loss: 0.10537506639957428, BER : 0.03972265625\n",
            "Train[40/50][24/240] Loss: 0.10636192560195923, BER : 0.04034375\n",
            "Train[40/50][25/240] Loss: 0.10569404065608978, BER : 0.04026953125\n",
            "Train[40/50][26/240] Loss: 0.10307729244232178, BER : 0.038515625\n",
            "Train[40/50][27/240] Loss: 0.10616320371627808, BER : 0.04026171875\n",
            "Train[40/50][28/240] Loss: 0.10520830750465393, BER : 0.03981640625\n",
            "Train[40/50][29/240] Loss: 0.10466747730970383, BER : 0.0387265625\n",
            "Train[40/50][30/240] Loss: 0.10583975911140442, BER : 0.04059375\n",
            "Train[40/50][31/240] Loss: 0.10597091168165207, BER : 0.03953515625\n",
            "Train[40/50][32/240] Loss: 0.10649730265140533, BER : 0.04071484375\n",
            "Train[40/50][33/240] Loss: 0.10444945842027664, BER : 0.0397265625\n",
            "Train[40/50][34/240] Loss: 0.10641750693321228, BER : 0.03980859375\n",
            "Train[40/50][35/240] Loss: 0.10589901357889175, BER : 0.0405546875\n",
            "Train[40/50][36/240] Loss: 0.10458239167928696, BER : 0.0392734375\n",
            "Train[40/50][37/240] Loss: 0.10456360131502151, BER : 0.039046875\n",
            "Train[40/50][38/240] Loss: 0.1062224805355072, BER : 0.0400234375\n",
            "Train[40/50][39/240] Loss: 0.1005847379565239, BER : 0.03690625\n",
            "Train[40/50][40/240] Loss: 0.10369065403938293, BER : 0.0390546875\n",
            "Train[40/50][41/240] Loss: 0.10850460082292557, BER : 0.04144921875\n",
            "Train[40/50][42/240] Loss: 0.10383621603250504, BER : 0.03923046875\n",
            "Train[40/50][43/240] Loss: 0.10483681410551071, BER : 0.03883203125\n",
            "Train[40/50][44/240] Loss: 0.10706321150064468, BER : 0.040234375\n",
            "Train[40/50][45/240] Loss: 0.10453112423419952, BER : 0.038921875\n",
            "Train[40/50][46/240] Loss: 0.10486412793397903, BER : 0.03977734375\n",
            "Train[40/50][47/240] Loss: 0.10315976291894913, BER : 0.03837890625\n",
            "Train[40/50][48/240] Loss: 0.10753170400857925, BER : 0.0409375\n",
            "Train[40/50][49/240] Loss: 0.10513284802436829, BER : 0.0396328125\n",
            "Train[40/50][50/240] Loss: 0.10664740204811096, BER : 0.0405234375\n",
            "Train[40/50][51/240] Loss: 0.10728444159030914, BER : 0.0409609375\n",
            "Train[40/50][52/240] Loss: 0.1032273918390274, BER : 0.0383046875\n",
            "Train[40/50][53/240] Loss: 0.1049451231956482, BER : 0.03975390625\n",
            "Train[40/50][54/240] Loss: 0.10360755026340485, BER : 0.038203125\n",
            "Train[40/50][55/240] Loss: 0.1066020205616951, BER : 0.04054296875\n",
            "Train[40/50][56/240] Loss: 0.10411814600229263, BER : 0.03903125\n",
            "Train[40/50][57/240] Loss: 0.10386216640472412, BER : 0.0398046875\n",
            "Train[40/50][58/240] Loss: 0.10283538699150085, BER : 0.03877734375\n",
            "Train[40/50][59/240] Loss: 0.1046377494931221, BER : 0.03929296875\n",
            "Train[40/50][60/240] Loss: 0.10370846092700958, BER : 0.03887109375\n",
            "Train[40/50][61/240] Loss: 0.10445024073123932, BER : 0.03894921875\n",
            "Train[40/50][62/240] Loss: 0.10193199664354324, BER : 0.03790625\n",
            "Train[40/50][63/240] Loss: 0.10413189232349396, BER : 0.03861328125\n",
            "Train[40/50][64/240] Loss: 0.10422047227621078, BER : 0.03899609375\n",
            "Train[40/50][65/240] Loss: 0.10619751363992691, BER : 0.03987890625\n",
            "Train[40/50][66/240] Loss: 0.10656046867370605, BER : 0.03980859375\n",
            "Train[40/50][67/240] Loss: 0.10198947787284851, BER : 0.03748046875\n",
            "Train[40/50][68/240] Loss: 0.10462238639593124, BER : 0.038875\n",
            "Train[40/50][69/240] Loss: 0.10454705357551575, BER : 0.03914453125\n",
            "Train[40/50][70/240] Loss: 0.10188932716846466, BER : 0.03723828125\n",
            "Train[40/50][71/240] Loss: 0.1077270582318306, BER : 0.04133984375\n",
            "Train[40/50][72/240] Loss: 0.103798046708107, BER : 0.03903125\n",
            "Train[40/50][73/240] Loss: 0.10487192869186401, BER : 0.0393359375\n",
            "Train[40/50][74/240] Loss: 0.1065029576420784, BER : 0.0396484375\n",
            "Train[40/50][75/240] Loss: 0.10589513927698135, BER : 0.03965625\n",
            "Train[40/50][76/240] Loss: 0.10602862387895584, BER : 0.04040625\n",
            "Train[40/50][77/240] Loss: 0.10612237453460693, BER : 0.040375\n",
            "Train[40/50][78/240] Loss: 0.10634011030197144, BER : 0.03987109375\n",
            "Train[40/50][79/240] Loss: 0.10727216303348541, BER : 0.0407734375\n",
            "Train[40/50][80/240] Loss: 0.10511477291584015, BER : 0.03934765625\n",
            "Train[40/50][81/240] Loss: 0.10489919781684875, BER : 0.03967578125\n",
            "Train[40/50][82/240] Loss: 0.10470110177993774, BER : 0.03966796875\n",
            "Train[40/50][83/240] Loss: 0.10686257481575012, BER : 0.04041796875\n",
            "Train[40/50][84/240] Loss: 0.10512746870517731, BER : 0.03947265625\n",
            "Train[40/50][85/240] Loss: 0.10579599440097809, BER : 0.03996875\n",
            "Train[40/50][86/240] Loss: 0.10594683885574341, BER : 0.0396875\n",
            "Train[40/50][87/240] Loss: 0.10699816048145294, BER : 0.040703125\n",
            "Train[40/50][88/240] Loss: 0.10455642640590668, BER : 0.03917578125\n",
            "Train[40/50][89/240] Loss: 0.10310089588165283, BER : 0.03840234375\n",
            "Train[40/50][90/240] Loss: 0.10608667135238647, BER : 0.0403203125\n",
            "Train[40/50][91/240] Loss: 0.10402411967515945, BER : 0.03883203125\n",
            "Train[40/50][92/240] Loss: 0.10537832975387573, BER : 0.039359375\n",
            "Train[40/50][93/240] Loss: 0.1076434999704361, BER : 0.0404921875\n",
            "Train[40/50][94/240] Loss: 0.10967259109020233, BER : 0.0421875\n",
            "Train[40/50][95/240] Loss: 0.1089830994606018, BER : 0.041640625\n",
            "Train[40/50][96/240] Loss: 0.10307341068983078, BER : 0.03794140625\n",
            "Train[40/50][97/240] Loss: 0.10582463443279266, BER : 0.0400625\n",
            "Train[40/50][98/240] Loss: 0.10601653158664703, BER : 0.0396484375\n",
            "Train[40/50][99/240] Loss: 0.10403725504875183, BER : 0.03915625\n",
            "Train[40/50][100/240] Loss: 0.10297459363937378, BER : 0.03876171875\n",
            "Train[40/50][101/240] Loss: 0.10489903390407562, BER : 0.03901953125\n",
            "Train[40/50][102/240] Loss: 0.10617741197347641, BER : 0.0397421875\n",
            "Train[40/50][103/240] Loss: 0.10532685369253159, BER : 0.040140625\n",
            "Train[40/50][104/240] Loss: 0.10846052318811417, BER : 0.0406796875\n",
            "Train[40/50][105/240] Loss: 0.11078617721796036, BER : 0.0416484375\n",
            "Train[40/50][106/240] Loss: 0.1049637645483017, BER : 0.03958984375\n",
            "Train[40/50][107/240] Loss: 0.10490986704826355, BER : 0.04057421875\n",
            "Train[40/50][108/240] Loss: 0.10572998970746994, BER : 0.03911328125\n",
            "Train[40/50][109/240] Loss: 0.10508072376251221, BER : 0.03983984375\n",
            "Train[40/50][110/240] Loss: 0.10855226218700409, BER : 0.0417109375\n",
            "Train[40/50][111/240] Loss: 0.1038886159658432, BER : 0.0390625\n",
            "Train[40/50][112/240] Loss: 0.1054254025220871, BER : 0.03971484375\n",
            "Train[40/50][113/240] Loss: 0.10592512786388397, BER : 0.03998046875\n",
            "Train[40/50][114/240] Loss: 0.10467573255300522, BER : 0.03958984375\n",
            "Train[40/50][115/240] Loss: 0.1048494204878807, BER : 0.03932421875\n",
            "Train[40/50][116/240] Loss: 0.10390634834766388, BER : 0.038578125\n",
            "Train[40/50][117/240] Loss: 0.10575640201568604, BER : 0.0394453125\n",
            "Train[40/50][118/240] Loss: 0.10734658688306808, BER : 0.0403515625\n",
            "Train[40/50][119/240] Loss: 0.10540731996297836, BER : 0.039515625\n",
            "Train[40/50][120/240] Loss: 0.10317128151655197, BER : 0.037515625\n",
            "Train[40/50][121/240] Loss: 0.10699936002492905, BER : 0.04048828125\n",
            "Train[40/50][122/240] Loss: 0.10653528571128845, BER : 0.0405703125\n",
            "Train[40/50][123/240] Loss: 0.10558579862117767, BER : 0.03968359375\n",
            "Train[40/50][124/240] Loss: 0.1041816994547844, BER : 0.03882421875\n",
            "Train[40/50][125/240] Loss: 0.10512300580739975, BER : 0.03948046875\n",
            "Train[40/50][126/240] Loss: 0.10553882271051407, BER : 0.03930859375\n",
            "Train[40/50][127/240] Loss: 0.10230634361505508, BER : 0.0377734375\n",
            "Train[40/50][128/240] Loss: 0.10474392026662827, BER : 0.040046875\n",
            "Train[40/50][129/240] Loss: 0.10191019624471664, BER : 0.0381328125\n",
            "Train[40/50][130/240] Loss: 0.10390089452266693, BER : 0.0392734375\n",
            "Train[40/50][131/240] Loss: 0.10676851868629456, BER : 0.04098046875\n",
            "Train[40/50][132/240] Loss: 0.10578690469264984, BER : 0.04012109375\n",
            "Train[40/50][133/240] Loss: 0.10634616762399673, BER : 0.03971875\n",
            "Train[40/50][134/240] Loss: 0.10316668450832367, BER : 0.03888671875\n",
            "Train[40/50][135/240] Loss: 0.1071176826953888, BER : 0.0406171875\n",
            "Train[40/50][136/240] Loss: 0.10375235229730606, BER : 0.03894921875\n",
            "Train[40/50][137/240] Loss: 0.1038939580321312, BER : 0.038859375\n",
            "Train[40/50][138/240] Loss: 0.10550305992364883, BER : 0.0387734375\n",
            "Train[40/50][139/240] Loss: 0.10507337749004364, BER : 0.03973828125\n",
            "Train[40/50][140/240] Loss: 0.10402973741292953, BER : 0.037984375\n",
            "Train[40/50][141/240] Loss: 0.1041799783706665, BER : 0.03944921875\n",
            "Train[40/50][142/240] Loss: 0.10062786936759949, BER : 0.03646875\n",
            "Train[40/50][143/240] Loss: 0.10583341866731644, BER : 0.0397421875\n",
            "Train[40/50][144/240] Loss: 0.10408186912536621, BER : 0.03875390625\n",
            "Train[40/50][145/240] Loss: 0.10558944195508957, BER : 0.04019921875\n",
            "Train[40/50][146/240] Loss: 0.10578878968954086, BER : 0.03999609375\n",
            "Train[40/50][147/240] Loss: 0.10615049302577972, BER : 0.0400078125\n",
            "Train[40/50][148/240] Loss: 0.10511887073516846, BER : 0.03939453125\n",
            "Train[40/50][149/240] Loss: 0.1052672266960144, BER : 0.0395\n",
            "Train[40/50][150/240] Loss: 0.10543100535869598, BER : 0.03887890625\n",
            "Train[40/50][151/240] Loss: 0.10408438742160797, BER : 0.039484375\n",
            "Train[40/50][152/240] Loss: 0.10365955531597137, BER : 0.038890625\n",
            "Train[40/50][153/240] Loss: 0.10569676011800766, BER : 0.040296875\n",
            "Train[40/50][154/240] Loss: 0.10397348552942276, BER : 0.038828125\n",
            "Train[40/50][155/240] Loss: 0.10458444058895111, BER : 0.039078125\n",
            "Train[40/50][156/240] Loss: 0.10711503028869629, BER : 0.04073046875\n",
            "Train[40/50][157/240] Loss: 0.10631121695041656, BER : 0.0404453125\n",
            "Train[40/50][158/240] Loss: 0.10335426777601242, BER : 0.03893359375\n",
            "Train[40/50][159/240] Loss: 0.10340544581413269, BER : 0.0383203125\n",
            "Train[40/50][160/240] Loss: 0.10484558343887329, BER : 0.03948046875\n",
            "Train[40/50][161/240] Loss: 0.10456980019807816, BER : 0.0395546875\n",
            "Train[40/50][162/240] Loss: 0.1061481311917305, BER : 0.04037890625\n",
            "Train[40/50][163/240] Loss: 0.10125487297773361, BER : 0.03716796875\n",
            "Train[40/50][164/240] Loss: 0.1037859171628952, BER : 0.03812890625\n",
            "Train[40/50][165/240] Loss: 0.10349091142416, BER : 0.038609375\n",
            "Train[40/50][166/240] Loss: 0.10535300523042679, BER : 0.04009765625\n",
            "Train[40/50][167/240] Loss: 0.1083243265748024, BER : 0.0411484375\n",
            "Train[40/50][168/240] Loss: 0.10485486686229706, BER : 0.0405078125\n",
            "Train[40/50][169/240] Loss: 0.10392184555530548, BER : 0.03916796875\n",
            "Train[40/50][170/240] Loss: 0.10250507295131683, BER : 0.03866015625\n",
            "Train[40/50][171/240] Loss: 0.10459385067224503, BER : 0.03905078125\n",
            "Train[40/50][172/240] Loss: 0.10486430674791336, BER : 0.03959765625\n",
            "Train[40/50][173/240] Loss: 0.10494370013475418, BER : 0.0393828125\n",
            "Train[40/50][174/240] Loss: 0.10423168540000916, BER : 0.038921875\n",
            "Train[40/50][175/240] Loss: 0.10621055215597153, BER : 0.0401328125\n",
            "Train[40/50][176/240] Loss: 0.10586483776569366, BER : 0.04033984375\n",
            "Train[40/50][177/240] Loss: 0.10655096173286438, BER : 0.040296875\n",
            "Train[40/50][178/240] Loss: 0.10678461939096451, BER : 0.04083984375\n",
            "Train[40/50][179/240] Loss: 0.10477479547262192, BER : 0.03965234375\n",
            "Train[40/50][180/240] Loss: 0.10588644444942474, BER : 0.04024609375\n",
            "Train[40/50][181/240] Loss: 0.10466639697551727, BER : 0.039265625\n",
            "Train[40/50][182/240] Loss: 0.10244831442832947, BER : 0.03717578125\n",
            "Train[40/50][183/240] Loss: 0.10515884310007095, BER : 0.0394609375\n",
            "Train[40/50][184/240] Loss: 0.10610402375459671, BER : 0.040734375\n",
            "Train[40/50][185/240] Loss: 0.1058468148112297, BER : 0.04006640625\n",
            "Train[40/50][186/240] Loss: 0.10400344431400299, BER : 0.03872265625\n",
            "Train[40/50][187/240] Loss: 0.10568466037511826, BER : 0.0399453125\n",
            "Train[40/50][188/240] Loss: 0.10655057430267334, BER : 0.03956640625\n",
            "Train[40/50][189/240] Loss: 0.10414986312389374, BER : 0.03878515625\n",
            "Train[40/50][190/240] Loss: 0.10773099958896637, BER : 0.04080859375\n",
            "Train[40/50][191/240] Loss: 0.10308001935482025, BER : 0.03853515625\n",
            "Train[40/50][192/240] Loss: 0.10569532215595245, BER : 0.0395625\n",
            "Train[40/50][193/240] Loss: 0.10484251379966736, BER : 0.04019921875\n",
            "Train[40/50][194/240] Loss: 0.10558350384235382, BER : 0.0398828125\n",
            "Train[40/50][195/240] Loss: 0.10530804097652435, BER : 0.0398359375\n",
            "Train[40/50][196/240] Loss: 0.10466183722019196, BER : 0.03908203125\n",
            "Train[40/50][197/240] Loss: 0.1023164689540863, BER : 0.03801171875\n",
            "Train[40/50][198/240] Loss: 0.10100030153989792, BER : 0.03755078125\n",
            "Train[40/50][199/240] Loss: 0.10726697742938995, BER : 0.04158203125\n",
            "Train[40/50][200/240] Loss: 0.10099700093269348, BER : 0.03762109375\n",
            "Train[40/50][201/240] Loss: 0.10622227191925049, BER : 0.040046875\n",
            "Train[40/50][202/240] Loss: 0.10486374795436859, BER : 0.04058984375\n",
            "Train[40/50][203/240] Loss: 0.10318990796804428, BER : 0.038671875\n",
            "Train[40/50][204/240] Loss: 0.10354459285736084, BER : 0.039453125\n",
            "Train[40/50][205/240] Loss: 0.10394798219203949, BER : 0.03910546875\n",
            "Train[40/50][206/240] Loss: 0.10581405460834503, BER : 0.0396796875\n",
            "Train[40/50][207/240] Loss: 0.10557952523231506, BER : 0.0397265625\n",
            "Train[40/50][208/240] Loss: 0.10451914370059967, BER : 0.03953125\n",
            "Train[40/50][209/240] Loss: 0.10409406572580338, BER : 0.0395703125\n",
            "Train[40/50][210/240] Loss: 0.10597635805606842, BER : 0.0392421875\n",
            "Train[40/50][211/240] Loss: 0.10781517624855042, BER : 0.04065625\n",
            "Train[40/50][212/240] Loss: 0.10426652431488037, BER : 0.03884765625\n",
            "Train[40/50][213/240] Loss: 0.10647463798522949, BER : 0.04028515625\n",
            "Train[40/50][214/240] Loss: 0.10549526661634445, BER : 0.0404765625\n",
            "Train[40/50][215/240] Loss: 0.10271133482456207, BER : 0.0379140625\n",
            "Train[40/50][216/240] Loss: 0.10567423701286316, BER : 0.0397578125\n",
            "Train[40/50][217/240] Loss: 0.10254091769456863, BER : 0.0380625\n",
            "Train[40/50][218/240] Loss: 0.10286195576190948, BER : 0.0383671875\n",
            "Train[40/50][219/240] Loss: 0.10239128768444061, BER : 0.03759375\n",
            "Train[40/50][220/240] Loss: 0.10424713790416718, BER : 0.0397890625\n",
            "Train[40/50][221/240] Loss: 0.10422218590974808, BER : 0.03953125\n",
            "Train[40/50][222/240] Loss: 0.10601051896810532, BER : 0.0397109375\n",
            "Train[40/50][223/240] Loss: 0.10512172430753708, BER : 0.0401171875\n",
            "Train[40/50][224/240] Loss: 0.10506975650787354, BER : 0.03994140625\n",
            "Train[40/50][225/240] Loss: 0.10493132472038269, BER : 0.039875\n",
            "Train[40/50][226/240] Loss: 0.1049487367272377, BER : 0.04001953125\n",
            "Train[40/50][227/240] Loss: 0.10634557157754898, BER : 0.0397265625\n",
            "Train[40/50][228/240] Loss: 0.1044241189956665, BER : 0.03962890625\n",
            "Train[40/50][229/240] Loss: 0.10581019520759583, BER : 0.0394921875\n",
            "Train[40/50][230/240] Loss: 0.10504208505153656, BER : 0.04040625\n",
            "Train[40/50][231/240] Loss: 0.10594531148672104, BER : 0.03969921875\n",
            "Train[40/50][232/240] Loss: 0.10394808650016785, BER : 0.0389765625\n",
            "Train[40/50][233/240] Loss: 0.10468754172325134, BER : 0.0393359375\n",
            "Train[40/50][234/240] Loss: 0.10548780858516693, BER : 0.03972265625\n",
            "Train[40/50][235/240] Loss: 0.10343699157238007, BER : 0.037890625\n",
            "Train[40/50][236/240] Loss: 0.10580267757177353, BER : 0.04011328125\n",
            "Train[40/50][237/240] Loss: 0.10184037685394287, BER : 0.0371328125\n",
            "Train[40/50][238/240] Loss: 0.10397515445947647, BER : 0.03959375\n",
            "Train[40/50][239/240] Loss: 0.1041048914194107, BER : 0.03891015625\n",
            "Epoch [40/50] Train_Avg_loss(epoch): 0.26753\n",
            "-------------------------------------\n",
            "Test[40/50][0/40]  Loss: 0.0771460011601448, BER (test): 0.04262890625\n",
            "Test[40/50][1/40]  Loss: 0.07598154246807098, BER (test): 0.04258203125\n",
            "Test[40/50][2/40]  Loss: 0.07554727047681808, BER (test): 0.04212890625\n",
            "Test[40/50][3/40]  Loss: 0.07338627427816391, BER (test): 0.040734375\n",
            "Test[40/50][4/40]  Loss: 0.07455326616764069, BER (test): 0.04160546875\n",
            "Test[40/50][5/40]  Loss: 0.07354243844747543, BER (test): 0.04044921875\n",
            "Test[40/50][6/40]  Loss: 0.07388879358768463, BER (test): 0.04047265625\n",
            "Test[40/50][7/40]  Loss: 0.07497081905603409, BER (test): 0.041609375\n",
            "Test[40/50][8/40]  Loss: 0.07349973917007446, BER (test): 0.04105078125\n",
            "Test[40/50][9/40]  Loss: 0.07634418457746506, BER (test): 0.04312890625\n",
            "Test[40/50][10/40]  Loss: 0.07597573101520538, BER (test): 0.04227734375\n",
            "Test[40/50][11/40]  Loss: 0.07749719172716141, BER (test): 0.04301953125\n",
            "Test[40/50][12/40]  Loss: 0.07777563482522964, BER (test): 0.0438359375\n",
            "Test[40/50][13/40]  Loss: 0.07386735826730728, BER (test): 0.041828125\n",
            "Test[40/50][14/40]  Loss: 0.07532081007957458, BER (test): 0.04178515625\n",
            "Test[40/50][15/40]  Loss: 0.07552582025527954, BER (test): 0.04226171875\n",
            "Test[40/50][16/40]  Loss: 0.07450296729803085, BER (test): 0.0413125\n",
            "Test[40/50][17/40]  Loss: 0.07370217889547348, BER (test): 0.04146484375\n",
            "Test[40/50][18/40]  Loss: 0.07515888661146164, BER (test): 0.04176171875\n",
            "Test[40/50][19/40]  Loss: 0.07538582384586334, BER (test): 0.04255078125\n",
            "Test[40/50][20/40]  Loss: 0.0744595155119896, BER (test): 0.041890625\n",
            "Test[40/50][21/40]  Loss: 0.07237686216831207, BER (test): 0.04055078125\n",
            "Test[40/50][22/40]  Loss: 0.07439256459474564, BER (test): 0.041109375\n",
            "Test[40/50][23/40]  Loss: 0.07387389242649078, BER (test): 0.04148828125\n",
            "Test[40/50][24/40]  Loss: 0.07573825120925903, BER (test): 0.0421953125\n",
            "Test[40/50][25/40]  Loss: 0.07449808716773987, BER (test): 0.0415859375\n",
            "Test[40/50][26/40]  Loss: 0.07428937405347824, BER (test): 0.04090234375\n",
            "Test[40/50][27/40]  Loss: 0.07402292639017105, BER (test): 0.04105859375\n",
            "Test[40/50][28/40]  Loss: 0.07661086320877075, BER (test): 0.04255078125\n",
            "Test[40/50][29/40]  Loss: 0.07550652325153351, BER (test): 0.042359375\n",
            "Test[40/50][30/40]  Loss: 0.07416635006666183, BER (test): 0.0414609375\n",
            "Test[40/50][31/40]  Loss: 0.0761886015534401, BER (test): 0.0427734375\n",
            "Test[40/50][32/40]  Loss: 0.07595514506101608, BER (test): 0.043140625\n",
            "Test[40/50][33/40]  Loss: 0.07521481812000275, BER (test): 0.04212109375\n",
            "Test[40/50][34/40]  Loss: 0.07540680468082428, BER (test): 0.04162109375\n",
            "Test[40/50][35/40]  Loss: 0.07653840631246567, BER (test): 0.042921875\n",
            "Test[40/50][36/40]  Loss: 0.07494059950113297, BER (test): 0.0424609375\n",
            "Test[40/50][37/40]  Loss: 0.07442338764667511, BER (test): 0.04190625\n",
            "Test[40/50][38/40]  Loss: 0.07772248983383179, BER (test): 0.04320703125\n",
            "Test[40/50][39/40]  Loss: 0.07240062206983566, BER (test): 0.04071484375\n",
            "Test_Epoch [40/50] Test_Avg_loss(epoch): 0.07506\n",
            "Train[41/50][0/240] Loss: 0.10498170554637909, BER : 0.039671875\n",
            "Train[41/50][1/240] Loss: 0.1022610142827034, BER : 0.0383515625\n",
            "Train[41/50][2/240] Loss: 0.10761740803718567, BER : 0.0410390625\n",
            "Train[41/50][3/240] Loss: 0.10237101465463638, BER : 0.03809765625\n",
            "Train[41/50][4/240] Loss: 0.10443203151226044, BER : 0.03922265625\n",
            "Train[41/50][5/240] Loss: 0.10458410531282425, BER : 0.03905859375\n",
            "Train[41/50][6/240] Loss: 0.10506770014762878, BER : 0.04021484375\n",
            "Train[41/50][7/240] Loss: 0.10495707392692566, BER : 0.03910546875\n",
            "Train[41/50][8/240] Loss: 0.10858741402626038, BER : 0.0416015625\n",
            "Train[41/50][9/240] Loss: 0.105169877409935, BER : 0.0398671875\n",
            "Train[41/50][10/240] Loss: 0.10456707328557968, BER : 0.0394609375\n",
            "Train[41/50][11/240] Loss: 0.1045045331120491, BER : 0.0391953125\n",
            "Train[41/50][12/240] Loss: 0.10353277623653412, BER : 0.03907421875\n",
            "Train[41/50][13/240] Loss: 0.10614835470914841, BER : 0.0408828125\n",
            "Train[41/50][14/240] Loss: 0.10333998501300812, BER : 0.0388515625\n",
            "Train[41/50][15/240] Loss: 0.10515780746936798, BER : 0.03954296875\n",
            "Train[41/50][16/240] Loss: 0.10149095952510834, BER : 0.037859375\n",
            "Train[41/50][17/240] Loss: 0.10556389391422272, BER : 0.03952734375\n",
            "Train[41/50][18/240] Loss: 0.1051831841468811, BER : 0.03955078125\n",
            "Train[41/50][19/240] Loss: 0.10263260453939438, BER : 0.03936328125\n",
            "Train[41/50][20/240] Loss: 0.10728432238101959, BER : 0.04099609375\n",
            "Train[41/50][21/240] Loss: 0.10582029819488525, BER : 0.0401875\n",
            "Train[41/50][22/240] Loss: 0.10668350756168365, BER : 0.04082421875\n",
            "Train[41/50][23/240] Loss: 0.10510790348052979, BER : 0.04056640625\n",
            "Train[41/50][24/240] Loss: 0.1040944755077362, BER : 0.03939453125\n",
            "Train[41/50][25/240] Loss: 0.10156123340129852, BER : 0.03783203125\n",
            "Train[41/50][26/240] Loss: 0.10150071978569031, BER : 0.0376796875\n",
            "Train[41/50][27/240] Loss: 0.10561150312423706, BER : 0.03984765625\n",
            "Train[41/50][28/240] Loss: 0.102542445063591, BER : 0.038\n",
            "Train[41/50][29/240] Loss: 0.10523946583271027, BER : 0.0400546875\n",
            "Train[41/50][30/240] Loss: 0.10419293493032455, BER : 0.03911328125\n",
            "Train[41/50][31/240] Loss: 0.10536463558673859, BER : 0.03959765625\n",
            "Train[41/50][32/240] Loss: 0.10433220118284225, BER : 0.03903125\n",
            "Train[41/50][33/240] Loss: 0.10169847309589386, BER : 0.0372578125\n",
            "Train[41/50][34/240] Loss: 0.10016606748104095, BER : 0.03714453125\n",
            "Train[41/50][35/240] Loss: 0.10290537774562836, BER : 0.03882421875\n",
            "Train[41/50][36/240] Loss: 0.10421424359083176, BER : 0.0386640625\n",
            "Train[41/50][37/240] Loss: 0.1078612357378006, BER : 0.04129296875\n",
            "Train[41/50][38/240] Loss: 0.10895037651062012, BER : 0.04180859375\n",
            "Train[41/50][39/240] Loss: 0.10553759336471558, BER : 0.04016796875\n",
            "Train[41/50][40/240] Loss: 0.10376586019992828, BER : 0.039\n",
            "Train[41/50][41/240] Loss: 0.10338173061609268, BER : 0.03940625\n",
            "Train[41/50][42/240] Loss: 0.10583774745464325, BER : 0.04059375\n",
            "Train[41/50][43/240] Loss: 0.1014145091176033, BER : 0.0376640625\n",
            "Train[41/50][44/240] Loss: 0.10558897256851196, BER : 0.04071875\n",
            "Train[41/50][45/240] Loss: 0.10133811831474304, BER : 0.03802734375\n",
            "Train[41/50][46/240] Loss: 0.10461907088756561, BER : 0.039390625\n",
            "Train[41/50][47/240] Loss: 0.105147585272789, BER : 0.039734375\n",
            "Train[41/50][48/240] Loss: 0.10212457925081253, BER : 0.03839453125\n",
            "Train[41/50][49/240] Loss: 0.10466229170560837, BER : 0.03923828125\n",
            "Train[41/50][50/240] Loss: 0.10345660150051117, BER : 0.038296875\n",
            "Train[41/50][51/240] Loss: 0.10568366944789886, BER : 0.04060546875\n",
            "Train[41/50][52/240] Loss: 0.10429921746253967, BER : 0.039296875\n",
            "Train[41/50][53/240] Loss: 0.10489477962255478, BER : 0.03933203125\n",
            "Train[41/50][54/240] Loss: 0.10342546552419662, BER : 0.03857421875\n",
            "Train[41/50][55/240] Loss: 0.10400710999965668, BER : 0.0391015625\n",
            "Train[41/50][56/240] Loss: 0.10246660560369492, BER : 0.03833203125\n",
            "Train[41/50][57/240] Loss: 0.10678686201572418, BER : 0.04082421875\n",
            "Train[41/50][58/240] Loss: 0.10537207126617432, BER : 0.03987890625\n",
            "Train[41/50][59/240] Loss: 0.10329354554414749, BER : 0.0384609375\n",
            "Train[41/50][60/240] Loss: 0.10623650252819061, BER : 0.03983203125\n",
            "Train[41/50][61/240] Loss: 0.10512147843837738, BER : 0.03941796875\n",
            "Train[41/50][62/240] Loss: 0.1029755249619484, BER : 0.0385546875\n",
            "Train[41/50][63/240] Loss: 0.10732271522283554, BER : 0.0407421875\n",
            "Train[41/50][64/240] Loss: 0.1025392934679985, BER : 0.038546875\n",
            "Train[41/50][65/240] Loss: 0.1042223796248436, BER : 0.03934375\n",
            "Train[41/50][66/240] Loss: 0.10507594794034958, BER : 0.03947265625\n",
            "Train[41/50][67/240] Loss: 0.10223374515771866, BER : 0.03826953125\n",
            "Train[41/50][68/240] Loss: 0.10394949465990067, BER : 0.039546875\n",
            "Train[41/50][69/240] Loss: 0.10634870827198029, BER : 0.04056640625\n",
            "Train[41/50][70/240] Loss: 0.10209375619888306, BER : 0.0375\n",
            "Train[41/50][71/240] Loss: 0.10232070088386536, BER : 0.03840625\n",
            "Train[41/50][72/240] Loss: 0.10267406702041626, BER : 0.03865234375\n",
            "Train[41/50][73/240] Loss: 0.10374321788549423, BER : 0.03905859375\n",
            "Train[41/50][74/240] Loss: 0.1029130220413208, BER : 0.038140625\n",
            "Train[41/50][75/240] Loss: 0.10375145822763443, BER : 0.0383203125\n",
            "Train[41/50][76/240] Loss: 0.10381710529327393, BER : 0.03871484375\n",
            "Train[41/50][77/240] Loss: 0.10423511266708374, BER : 0.039375\n",
            "Train[41/50][78/240] Loss: 0.10382186621427536, BER : 0.039078125\n",
            "Train[41/50][79/240] Loss: 0.1037212535738945, BER : 0.0390078125\n",
            "Train[41/50][80/240] Loss: 0.10424408316612244, BER : 0.03891015625\n",
            "Train[41/50][81/240] Loss: 0.10548043996095657, BER : 0.040140625\n",
            "Train[41/50][82/240] Loss: 0.10774456709623337, BER : 0.04153515625\n",
            "Train[41/50][83/240] Loss: 0.10455971956253052, BER : 0.0393515625\n",
            "Train[41/50][84/240] Loss: 0.10490982234477997, BER : 0.03998828125\n",
            "Train[41/50][85/240] Loss: 0.10553410649299622, BER : 0.04003515625\n",
            "Train[41/50][86/240] Loss: 0.1051914244890213, BER : 0.0396640625\n",
            "Train[41/50][87/240] Loss: 0.10486511886119843, BER : 0.03908984375\n",
            "Train[41/50][88/240] Loss: 0.10405845940113068, BER : 0.03921484375\n",
            "Train[41/50][89/240] Loss: 0.10487014055252075, BER : 0.03951953125\n",
            "Train[41/50][90/240] Loss: 0.10507085919380188, BER : 0.03927734375\n",
            "Train[41/50][91/240] Loss: 0.10594576597213745, BER : 0.04008203125\n",
            "Train[41/50][92/240] Loss: 0.10368731617927551, BER : 0.0386171875\n",
            "Train[41/50][93/240] Loss: 0.10253369808197021, BER : 0.037828125\n",
            "Train[41/50][94/240] Loss: 0.10489355027675629, BER : 0.03962109375\n",
            "Train[41/50][95/240] Loss: 0.10677189379930496, BER : 0.04033984375\n",
            "Train[41/50][96/240] Loss: 0.10578209161758423, BER : 0.0391875\n",
            "Train[41/50][97/240] Loss: 0.10201409459114075, BER : 0.03796484375\n",
            "Train[41/50][98/240] Loss: 0.10485333204269409, BER : 0.0391640625\n",
            "Train[41/50][99/240] Loss: 0.1029401570558548, BER : 0.03859765625\n",
            "Train[41/50][100/240] Loss: 0.1022506058216095, BER : 0.0383125\n",
            "Train[41/50][101/240] Loss: 0.10095331817865372, BER : 0.036796875\n",
            "Train[41/50][102/240] Loss: 0.10276460647583008, BER : 0.03798828125\n",
            "Train[41/50][103/240] Loss: 0.10518837720155716, BER : 0.04009375\n",
            "Train[41/50][104/240] Loss: 0.10285311192274094, BER : 0.03852734375\n",
            "Train[41/50][105/240] Loss: 0.10658282041549683, BER : 0.04068359375\n",
            "Train[41/50][106/240] Loss: 0.10545425117015839, BER : 0.04041796875\n",
            "Train[41/50][107/240] Loss: 0.10368870198726654, BER : 0.0383828125\n",
            "Train[41/50][108/240] Loss: 0.10615606606006622, BER : 0.0402890625\n",
            "Train[41/50][109/240] Loss: 0.10507790744304657, BER : 0.03878515625\n",
            "Train[41/50][110/240] Loss: 0.1023561879992485, BER : 0.03803125\n",
            "Train[41/50][111/240] Loss: 0.10177392512559891, BER : 0.0376640625\n",
            "Train[41/50][112/240] Loss: 0.10465368628501892, BER : 0.0388671875\n",
            "Train[41/50][113/240] Loss: 0.10587044060230255, BER : 0.04099609375\n",
            "Train[41/50][114/240] Loss: 0.104494608938694, BER : 0.038828125\n",
            "Train[41/50][115/240] Loss: 0.10539518296718597, BER : 0.0399375\n",
            "Train[41/50][116/240] Loss: 0.10592620074748993, BER : 0.039890625\n",
            "Train[41/50][117/240] Loss: 0.10477340221405029, BER : 0.03951171875\n",
            "Train[41/50][118/240] Loss: 0.10282254219055176, BER : 0.03875\n",
            "Train[41/50][119/240] Loss: 0.10457053035497665, BER : 0.03946484375\n",
            "Train[41/50][120/240] Loss: 0.10497911274433136, BER : 0.03952734375\n",
            "Train[41/50][121/240] Loss: 0.1032867357134819, BER : 0.0387578125\n",
            "Train[41/50][122/240] Loss: 0.10657713562250137, BER : 0.03980078125\n",
            "Train[41/50][123/240] Loss: 0.1042688712477684, BER : 0.03912109375\n",
            "Train[41/50][124/240] Loss: 0.10629788041114807, BER : 0.04084375\n",
            "Train[41/50][125/240] Loss: 0.1049719899892807, BER : 0.03981640625\n",
            "Train[41/50][126/240] Loss: 0.10326217114925385, BER : 0.03951953125\n",
            "Train[41/50][127/240] Loss: 0.10744550824165344, BER : 0.041109375\n",
            "Train[41/50][128/240] Loss: 0.10364126414060593, BER : 0.03850390625\n",
            "Train[41/50][129/240] Loss: 0.10632804036140442, BER : 0.0402734375\n",
            "Train[41/50][130/240] Loss: 0.10576088726520538, BER : 0.03991015625\n",
            "Train[41/50][131/240] Loss: 0.1049896702170372, BER : 0.0399765625\n",
            "Train[41/50][132/240] Loss: 0.10513483732938766, BER : 0.03995703125\n",
            "Train[41/50][133/240] Loss: 0.10307003557682037, BER : 0.03882421875\n",
            "Train[41/50][134/240] Loss: 0.10321047157049179, BER : 0.038984375\n",
            "Train[41/50][135/240] Loss: 0.10464794188737869, BER : 0.0394296875\n",
            "Train[41/50][136/240] Loss: 0.10530837625265121, BER : 0.03918359375\n",
            "Train[41/50][137/240] Loss: 0.10747827589511871, BER : 0.04023046875\n",
            "Train[41/50][138/240] Loss: 0.10642451047897339, BER : 0.04018359375\n",
            "Train[41/50][139/240] Loss: 0.10342271625995636, BER : 0.03927734375\n",
            "Train[41/50][140/240] Loss: 0.10517904162406921, BER : 0.03952734375\n",
            "Train[41/50][141/240] Loss: 0.10289086401462555, BER : 0.03860546875\n",
            "Train[41/50][142/240] Loss: 0.10237376391887665, BER : 0.03771875\n",
            "Train[41/50][143/240] Loss: 0.10306721925735474, BER : 0.0381796875\n",
            "Train[41/50][144/240] Loss: 0.10544393211603165, BER : 0.03994140625\n",
            "Train[41/50][145/240] Loss: 0.10657007992267609, BER : 0.03996875\n",
            "Train[41/50][146/240] Loss: 0.1049785390496254, BER : 0.03929296875\n",
            "Train[41/50][147/240] Loss: 0.10457693785429001, BER : 0.039375\n",
            "Train[41/50][148/240] Loss: 0.10410334914922714, BER : 0.03921484375\n",
            "Train[41/50][149/240] Loss: 0.10306883603334427, BER : 0.038671875\n",
            "Train[41/50][150/240] Loss: 0.10172650218009949, BER : 0.03763671875\n",
            "Train[41/50][151/240] Loss: 0.10627224296331406, BER : 0.0395859375\n",
            "Train[41/50][152/240] Loss: 0.10862265527248383, BER : 0.0424375\n",
            "Train[41/50][153/240] Loss: 0.10343679785728455, BER : 0.03903125\n",
            "Train[41/50][154/240] Loss: 0.10527226328849792, BER : 0.0399375\n",
            "Train[41/50][155/240] Loss: 0.10307583212852478, BER : 0.0379375\n",
            "Train[41/50][156/240] Loss: 0.1023855209350586, BER : 0.03842578125\n",
            "Train[41/50][157/240] Loss: 0.1036839485168457, BER : 0.0383515625\n",
            "Train[41/50][158/240] Loss: 0.104938805103302, BER : 0.03965234375\n",
            "Train[41/50][159/240] Loss: 0.1056617945432663, BER : 0.03917578125\n",
            "Train[41/50][160/240] Loss: 0.10633823275566101, BER : 0.0405234375\n",
            "Train[41/50][161/240] Loss: 0.10416394472122192, BER : 0.03967578125\n",
            "Train[41/50][162/240] Loss: 0.10531429946422577, BER : 0.0397890625\n",
            "Train[41/50][163/240] Loss: 0.10447683930397034, BER : 0.039375\n",
            "Train[41/50][164/240] Loss: 0.10301822423934937, BER : 0.038625\n",
            "Train[41/50][165/240] Loss: 0.1056511402130127, BER : 0.04001171875\n",
            "Train[41/50][166/240] Loss: 0.10951727628707886, BER : 0.0417734375\n",
            "Train[41/50][167/240] Loss: 0.10480262339115143, BER : 0.0396484375\n",
            "Train[41/50][168/240] Loss: 0.10609118640422821, BER : 0.04046484375\n",
            "Train[41/50][169/240] Loss: 0.1033296287059784, BER : 0.0389140625\n",
            "Train[41/50][170/240] Loss: 0.10216959565877914, BER : 0.0380546875\n",
            "Train[41/50][171/240] Loss: 0.10406006127595901, BER : 0.0388984375\n",
            "Train[41/50][172/240] Loss: 0.1041971743106842, BER : 0.03974609375\n",
            "Train[41/50][173/240] Loss: 0.10388020426034927, BER : 0.03875390625\n",
            "Train[41/50][174/240] Loss: 0.10391958802938461, BER : 0.038578125\n",
            "Train[41/50][175/240] Loss: 0.10790258646011353, BER : 0.041078125\n",
            "Train[41/50][176/240] Loss: 0.10553288459777832, BER : 0.040046875\n",
            "Train[41/50][177/240] Loss: 0.10437968373298645, BER : 0.03886328125\n",
            "Train[41/50][178/240] Loss: 0.1058528870344162, BER : 0.0404453125\n",
            "Train[41/50][179/240] Loss: 0.10312315076589584, BER : 0.03836328125\n",
            "Train[41/50][180/240] Loss: 0.10408024489879608, BER : 0.03941796875\n",
            "Train[41/50][181/240] Loss: 0.10609153658151627, BER : 0.0404453125\n",
            "Train[41/50][182/240] Loss: 0.10467133671045303, BER : 0.039078125\n",
            "Train[41/50][183/240] Loss: 0.10494720935821533, BER : 0.03995703125\n",
            "Train[41/50][184/240] Loss: 0.10478371381759644, BER : 0.03925\n",
            "Train[41/50][185/240] Loss: 0.10336033254861832, BER : 0.03934765625\n",
            "Train[41/50][186/240] Loss: 0.1074962466955185, BER : 0.041640625\n",
            "Train[41/50][187/240] Loss: 0.10351721197366714, BER : 0.0383203125\n",
            "Train[41/50][188/240] Loss: 0.10486901551485062, BER : 0.03925390625\n",
            "Train[41/50][189/240] Loss: 0.10647550970315933, BER : 0.040859375\n",
            "Train[41/50][190/240] Loss: 0.10426640510559082, BER : 0.03883984375\n",
            "Train[41/50][191/240] Loss: 0.10442590713500977, BER : 0.038734375\n",
            "Train[41/50][192/240] Loss: 0.10294309258460999, BER : 0.03778515625\n",
            "Train[41/50][193/240] Loss: 0.10647598654031754, BER : 0.04039453125\n",
            "Train[41/50][194/240] Loss: 0.10349878668785095, BER : 0.03879296875\n",
            "Train[41/50][195/240] Loss: 0.10504190623760223, BER : 0.0394296875\n",
            "Train[41/50][196/240] Loss: 0.104683518409729, BER : 0.03994921875\n",
            "Train[41/50][197/240] Loss: 0.10303647816181183, BER : 0.03826953125\n",
            "Train[41/50][198/240] Loss: 0.10513077676296234, BER : 0.03941015625\n",
            "Train[41/50][199/240] Loss: 0.10243817418813705, BER : 0.038484375\n",
            "Train[41/50][200/240] Loss: 0.10553433746099472, BER : 0.0398125\n",
            "Train[41/50][201/240] Loss: 0.10313870012760162, BER : 0.03876953125\n",
            "Train[41/50][202/240] Loss: 0.10811708867549896, BER : 0.0415859375\n",
            "Train[41/50][203/240] Loss: 0.10220132023096085, BER : 0.03784375\n",
            "Train[41/50][204/240] Loss: 0.10460197925567627, BER : 0.03913671875\n",
            "Train[41/50][205/240] Loss: 0.10468260943889618, BER : 0.0395390625\n",
            "Train[41/50][206/240] Loss: 0.10302387923002243, BER : 0.038828125\n",
            "Train[41/50][207/240] Loss: 0.10239023715257645, BER : 0.03796484375\n",
            "Train[41/50][208/240] Loss: 0.10080328583717346, BER : 0.03728515625\n",
            "Train[41/50][209/240] Loss: 0.10224443674087524, BER : 0.03793359375\n",
            "Train[41/50][210/240] Loss: 0.10266359895467758, BER : 0.03871484375\n",
            "Train[41/50][211/240] Loss: 0.10515999048948288, BER : 0.03958984375\n",
            "Train[41/50][212/240] Loss: 0.1035131886601448, BER : 0.0381640625\n",
            "Train[41/50][213/240] Loss: 0.10117772221565247, BER : 0.03711328125\n",
            "Train[41/50][214/240] Loss: 0.10495279729366302, BER : 0.03926953125\n",
            "Train[41/50][215/240] Loss: 0.10869566351175308, BER : 0.0418515625\n",
            "Train[41/50][216/240] Loss: 0.10574844479560852, BER : 0.04019921875\n",
            "Train[41/50][217/240] Loss: 0.10497651249170303, BER : 0.03998828125\n",
            "Train[41/50][218/240] Loss: 0.10244718939065933, BER : 0.03758984375\n",
            "Train[41/50][219/240] Loss: 0.10301537066698074, BER : 0.03835546875\n",
            "Train[41/50][220/240] Loss: 0.10541842132806778, BER : 0.03937109375\n",
            "Train[41/50][221/240] Loss: 0.10461180657148361, BER : 0.0395703125\n",
            "Train[41/50][222/240] Loss: 0.10709704458713531, BER : 0.04139453125\n",
            "Train[41/50][223/240] Loss: 0.10316405445337296, BER : 0.03858984375\n",
            "Train[41/50][224/240] Loss: 0.1038132980465889, BER : 0.04005859375\n",
            "Train[41/50][225/240] Loss: 0.10488900542259216, BER : 0.03944140625\n",
            "Train[41/50][226/240] Loss: 0.1058921292424202, BER : 0.03964453125\n",
            "Train[41/50][227/240] Loss: 0.10346081107854843, BER : 0.038703125\n",
            "Train[41/50][228/240] Loss: 0.10428875684738159, BER : 0.039265625\n",
            "Train[41/50][229/240] Loss: 0.10557195544242859, BER : 0.03998046875\n",
            "Train[41/50][230/240] Loss: 0.10242164880037308, BER : 0.037671875\n",
            "Train[41/50][231/240] Loss: 0.10161992907524109, BER : 0.03763671875\n",
            "Train[41/50][232/240] Loss: 0.10498032718896866, BER : 0.03948828125\n",
            "Train[41/50][233/240] Loss: 0.10325656831264496, BER : 0.03932421875\n",
            "Train[41/50][234/240] Loss: 0.1082061231136322, BER : 0.04198046875\n",
            "Train[41/50][235/240] Loss: 0.10379485040903091, BER : 0.0389375\n",
            "Train[41/50][236/240] Loss: 0.10511605441570282, BER : 0.04044140625\n",
            "Train[41/50][237/240] Loss: 0.10409113764762878, BER : 0.04013671875\n",
            "Train[41/50][238/240] Loss: 0.1035834401845932, BER : 0.03941796875\n",
            "Train[41/50][239/240] Loss: 0.1039419025182724, BER : 0.03980078125\n",
            "Epoch [41/50] Train_Avg_loss(epoch): 0.26365\n",
            "-------------------------------------\n",
            "Test[41/50][0/40]  Loss: 0.07796342670917511, BER (test): 0.0414453125\n",
            "Test[41/50][1/40]  Loss: 0.07738839089870453, BER (test): 0.04226171875\n",
            "Test[41/50][2/40]  Loss: 0.0773930549621582, BER (test): 0.04195703125\n",
            "Test[41/50][3/40]  Loss: 0.07410932332277298, BER (test): 0.04036328125\n",
            "Test[41/50][4/40]  Loss: 0.07840502262115479, BER (test): 0.04253515625\n",
            "Test[41/50][5/40]  Loss: 0.07512175291776657, BER (test): 0.0407421875\n",
            "Test[41/50][6/40]  Loss: 0.07791700959205627, BER (test): 0.0428984375\n",
            "Test[41/50][7/40]  Loss: 0.07785583287477493, BER (test): 0.04196875\n",
            "Test[41/50][8/40]  Loss: 0.07659974694252014, BER (test): 0.041640625\n",
            "Test[41/50][9/40]  Loss: 0.07590124011039734, BER (test): 0.04124609375\n",
            "Test[41/50][10/40]  Loss: 0.07520642876625061, BER (test): 0.040859375\n",
            "Test[41/50][11/40]  Loss: 0.07492495328187943, BER (test): 0.04063671875\n",
            "Test[41/50][12/40]  Loss: 0.08021393418312073, BER (test): 0.0435234375\n",
            "Test[41/50][13/40]  Loss: 0.07452303916215897, BER (test): 0.040015625\n",
            "Test[41/50][14/40]  Loss: 0.07949317991733551, BER (test): 0.04346875\n",
            "Test[41/50][15/40]  Loss: 0.0761803388595581, BER (test): 0.04148046875\n",
            "Test[41/50][16/40]  Loss: 0.0784391313791275, BER (test): 0.04281640625\n",
            "Test[41/50][17/40]  Loss: 0.07581660151481628, BER (test): 0.04064453125\n",
            "Test[41/50][18/40]  Loss: 0.07505565136671066, BER (test): 0.0403203125\n",
            "Test[41/50][19/40]  Loss: 0.07535846531391144, BER (test): 0.04057421875\n",
            "Test[41/50][20/40]  Loss: 0.07834065705537796, BER (test): 0.0419921875\n",
            "Test[41/50][21/40]  Loss: 0.07770407199859619, BER (test): 0.04248046875\n",
            "Test[41/50][22/40]  Loss: 0.07729759812355042, BER (test): 0.041921875\n",
            "Test[41/50][23/40]  Loss: 0.07156505435705185, BER (test): 0.03915234375\n",
            "Test[41/50][24/40]  Loss: 0.07881813496351242, BER (test): 0.042796875\n",
            "Test[41/50][25/40]  Loss: 0.0757942721247673, BER (test): 0.0403203125\n",
            "Test[41/50][26/40]  Loss: 0.07457831501960754, BER (test): 0.04053125\n",
            "Test[41/50][27/40]  Loss: 0.07541909068822861, BER (test): 0.041046875\n",
            "Test[41/50][28/40]  Loss: 0.07618614286184311, BER (test): 0.04140234375\n",
            "Test[41/50][29/40]  Loss: 0.07620798051357269, BER (test): 0.0408984375\n",
            "Test[41/50][30/40]  Loss: 0.07443388551473618, BER (test): 0.03953125\n",
            "Test[41/50][31/40]  Loss: 0.07349441945552826, BER (test): 0.0396640625\n",
            "Test[41/50][32/40]  Loss: 0.07419832795858383, BER (test): 0.04050390625\n",
            "Test[41/50][33/40]  Loss: 0.07651983946561813, BER (test): 0.041890625\n",
            "Test[41/50][34/40]  Loss: 0.07586828619241714, BER (test): 0.04091015625\n",
            "Test[41/50][35/40]  Loss: 0.07486177980899811, BER (test): 0.04044921875\n",
            "Test[41/50][36/40]  Loss: 0.0741889476776123, BER (test): 0.04028515625\n",
            "Test[41/50][37/40]  Loss: 0.07726239413022995, BER (test): 0.04175390625\n",
            "Test[41/50][38/40]  Loss: 0.07574450224637985, BER (test): 0.04173828125\n",
            "Test[41/50][39/40]  Loss: 0.07786336541175842, BER (test): 0.04208203125\n",
            "Test_Epoch [41/50] Test_Avg_loss(epoch): 0.07626\n",
            "Train[42/50][0/240] Loss: 0.10421733558177948, BER : 0.0390546875\n",
            "Train[42/50][1/240] Loss: 0.10525627434253693, BER : 0.03946484375\n",
            "Train[42/50][2/240] Loss: 0.10484960675239563, BER : 0.03994140625\n",
            "Train[42/50][3/240] Loss: 0.10572285205125809, BER : 0.04012890625\n",
            "Train[42/50][4/240] Loss: 0.10572537034749985, BER : 0.0395625\n",
            "Train[42/50][5/240] Loss: 0.1032501608133316, BER : 0.03896484375\n",
            "Train[42/50][6/240] Loss: 0.10432569682598114, BER : 0.03894140625\n",
            "Train[42/50][7/240] Loss: 0.10466397553682327, BER : 0.04008203125\n",
            "Train[42/50][8/240] Loss: 0.10514280945062637, BER : 0.0406875\n",
            "Train[42/50][9/240] Loss: 0.10686880350112915, BER : 0.040828125\n",
            "Train[42/50][10/240] Loss: 0.10394862294197083, BER : 0.039703125\n",
            "Train[42/50][11/240] Loss: 0.10348072648048401, BER : 0.03849609375\n",
            "Train[42/50][12/240] Loss: 0.10220126807689667, BER : 0.03796875\n",
            "Train[42/50][13/240] Loss: 0.10340613126754761, BER : 0.038125\n",
            "Train[42/50][14/240] Loss: 0.1052108034491539, BER : 0.039546875\n",
            "Train[42/50][15/240] Loss: 0.10817041993141174, BER : 0.04170703125\n",
            "Train[42/50][16/240] Loss: 0.10412481427192688, BER : 0.03848046875\n",
            "Train[42/50][17/240] Loss: 0.10544567555189133, BER : 0.039171875\n",
            "Train[42/50][18/240] Loss: 0.10575403273105621, BER : 0.03956640625\n",
            "Train[42/50][19/240] Loss: 0.10438063740730286, BER : 0.03906640625\n",
            "Train[42/50][20/240] Loss: 0.10345611721277237, BER : 0.0384609375\n",
            "Train[42/50][21/240] Loss: 0.10516814887523651, BER : 0.04007421875\n",
            "Train[42/50][22/240] Loss: 0.10400724411010742, BER : 0.03905859375\n",
            "Train[42/50][23/240] Loss: 0.10259054601192474, BER : 0.038765625\n",
            "Train[42/50][24/240] Loss: 0.10529661178588867, BER : 0.03969140625\n",
            "Train[42/50][25/240] Loss: 0.10456793010234833, BER : 0.03955859375\n",
            "Train[42/50][26/240] Loss: 0.10375148057937622, BER : 0.03871484375\n",
            "Train[42/50][27/240] Loss: 0.10382980853319168, BER : 0.03895703125\n",
            "Train[42/50][28/240] Loss: 0.10627688467502594, BER : 0.0410234375\n",
            "Train[42/50][29/240] Loss: 0.1040147915482521, BER : 0.039609375\n",
            "Train[42/50][30/240] Loss: 0.10688532143831253, BER : 0.04045703125\n",
            "Train[42/50][31/240] Loss: 0.10295330733060837, BER : 0.03847265625\n",
            "Train[42/50][32/240] Loss: 0.10242386907339096, BER : 0.03830859375\n",
            "Train[42/50][33/240] Loss: 0.1031670793890953, BER : 0.0390703125\n",
            "Train[42/50][34/240] Loss: 0.10717812180519104, BER : 0.04120703125\n",
            "Train[42/50][35/240] Loss: 0.10559893399477005, BER : 0.03998828125\n",
            "Train[42/50][36/240] Loss: 0.10415598005056381, BER : 0.0395546875\n",
            "Train[42/50][37/240] Loss: 0.10098833590745926, BER : 0.03740625\n",
            "Train[42/50][38/240] Loss: 0.1047249436378479, BER : 0.03941015625\n",
            "Train[42/50][39/240] Loss: 0.10391123592853546, BER : 0.03919140625\n",
            "Train[42/50][40/240] Loss: 0.10326332598924637, BER : 0.0385546875\n",
            "Train[42/50][41/240] Loss: 0.10357477515935898, BER : 0.03884375\n",
            "Train[42/50][42/240] Loss: 0.10307846963405609, BER : 0.0389453125\n",
            "Train[42/50][43/240] Loss: 0.10262949764728546, BER : 0.03838671875\n",
            "Train[42/50][44/240] Loss: 0.10236115008592606, BER : 0.0388046875\n",
            "Train[42/50][45/240] Loss: 0.10274858772754669, BER : 0.03876953125\n",
            "Train[42/50][46/240] Loss: 0.10701420903205872, BER : 0.041421875\n",
            "Train[42/50][47/240] Loss: 0.10265373438596725, BER : 0.03885546875\n",
            "Train[42/50][48/240] Loss: 0.10271480679512024, BER : 0.03821484375\n",
            "Train[42/50][49/240] Loss: 0.10101591050624847, BER : 0.037421875\n",
            "Train[42/50][50/240] Loss: 0.10196741670370102, BER : 0.03766796875\n",
            "Train[42/50][51/240] Loss: 0.10460802167654037, BER : 0.03924609375\n",
            "Train[42/50][52/240] Loss: 0.10461664944887161, BER : 0.03953515625\n",
            "Train[42/50][53/240] Loss: 0.10514457523822784, BER : 0.0394921875\n",
            "Train[42/50][54/240] Loss: 0.10271535813808441, BER : 0.03878125\n",
            "Train[42/50][55/240] Loss: 0.10389736294746399, BER : 0.0385625\n",
            "Train[42/50][56/240] Loss: 0.10482439398765564, BER : 0.0394609375\n",
            "Train[42/50][57/240] Loss: 0.10492216050624847, BER : 0.04058203125\n",
            "Train[42/50][58/240] Loss: 0.10399655252695084, BER : 0.038796875\n",
            "Train[42/50][59/240] Loss: 0.10006734728813171, BER : 0.0374140625\n",
            "Train[42/50][60/240] Loss: 0.10651495307683945, BER : 0.03993359375\n",
            "Train[42/50][61/240] Loss: 0.10329928994178772, BER : 0.03897265625\n",
            "Train[42/50][62/240] Loss: 0.10203573852777481, BER : 0.03805078125\n",
            "Train[42/50][63/240] Loss: 0.10662007331848145, BER : 0.0405703125\n",
            "Train[42/50][64/240] Loss: 0.10427295416593552, BER : 0.0390546875\n",
            "Train[42/50][65/240] Loss: 0.10370609164237976, BER : 0.038125\n",
            "Train[42/50][66/240] Loss: 0.10041411966085434, BER : 0.03676953125\n",
            "Train[42/50][67/240] Loss: 0.10601755231618881, BER : 0.0404140625\n",
            "Train[42/50][68/240] Loss: 0.10277128964662552, BER : 0.03872265625\n",
            "Train[42/50][69/240] Loss: 0.10363584756851196, BER : 0.03948828125\n",
            "Train[42/50][70/240] Loss: 0.10199728608131409, BER : 0.03759765625\n",
            "Train[42/50][71/240] Loss: 0.1038098931312561, BER : 0.03894921875\n",
            "Train[42/50][72/240] Loss: 0.10171818733215332, BER : 0.03804296875\n",
            "Train[42/50][73/240] Loss: 0.10773255676031113, BER : 0.041\n",
            "Train[42/50][74/240] Loss: 0.10472448170185089, BER : 0.0389140625\n",
            "Train[42/50][75/240] Loss: 0.10240869224071503, BER : 0.03825390625\n",
            "Train[42/50][76/240] Loss: 0.10337752103805542, BER : 0.0381875\n",
            "Train[42/50][77/240] Loss: 0.10546986758708954, BER : 0.0399140625\n",
            "Train[42/50][78/240] Loss: 0.10377015173435211, BER : 0.039046875\n",
            "Train[42/50][79/240] Loss: 0.10396547615528107, BER : 0.0393515625\n",
            "Train[42/50][80/240] Loss: 0.10251916199922562, BER : 0.0379921875\n",
            "Train[42/50][81/240] Loss: 0.10208502411842346, BER : 0.038671875\n",
            "Train[42/50][82/240] Loss: 0.10424622148275375, BER : 0.03950390625\n",
            "Train[42/50][83/240] Loss: 0.1016455739736557, BER : 0.037546875\n",
            "Train[42/50][84/240] Loss: 0.10486505180597305, BER : 0.0399140625\n",
            "Train[42/50][85/240] Loss: 0.10671760141849518, BER : 0.040484375\n",
            "Train[42/50][86/240] Loss: 0.10540361702442169, BER : 0.03969140625\n",
            "Train[42/50][87/240] Loss: 0.10508707910776138, BER : 0.0394609375\n",
            "Train[42/50][88/240] Loss: 0.10266438126564026, BER : 0.038484375\n",
            "Train[42/50][89/240] Loss: 0.10077394545078278, BER : 0.03696875\n",
            "Train[42/50][90/240] Loss: 0.10455048084259033, BER : 0.039328125\n",
            "Train[42/50][91/240] Loss: 0.10836076736450195, BER : 0.0421953125\n",
            "Train[42/50][92/240] Loss: 0.10197290033102036, BER : 0.0373984375\n",
            "Train[42/50][93/240] Loss: 0.10382772982120514, BER : 0.0397421875\n",
            "Train[42/50][94/240] Loss: 0.10566583275794983, BER : 0.03966796875\n",
            "Train[42/50][95/240] Loss: 0.10139033198356628, BER : 0.03837109375\n",
            "Train[42/50][96/240] Loss: 0.10322491824626923, BER : 0.0383125\n",
            "Train[42/50][97/240] Loss: 0.10579952597618103, BER : 0.04044140625\n",
            "Train[42/50][98/240] Loss: 0.10532154887914658, BER : 0.03957421875\n",
            "Train[42/50][99/240] Loss: 0.10721646994352341, BER : 0.04109765625\n",
            "Train[42/50][100/240] Loss: 0.10454104095697403, BER : 0.03901953125\n",
            "Train[42/50][101/240] Loss: 0.1049419492483139, BER : 0.0394765625\n",
            "Train[42/50][102/240] Loss: 0.1023600846529007, BER : 0.0382421875\n",
            "Train[42/50][103/240] Loss: 0.10128044337034225, BER : 0.03733984375\n",
            "Train[42/50][104/240] Loss: 0.10713130980730057, BER : 0.0408671875\n",
            "Train[42/50][105/240] Loss: 0.10320599377155304, BER : 0.0381328125\n",
            "Train[42/50][106/240] Loss: 0.10367678105831146, BER : 0.0391328125\n",
            "Train[42/50][107/240] Loss: 0.1026742085814476, BER : 0.038359375\n",
            "Train[42/50][108/240] Loss: 0.103030264377594, BER : 0.0390703125\n",
            "Train[42/50][109/240] Loss: 0.1024809405207634, BER : 0.0373671875\n",
            "Train[42/50][110/240] Loss: 0.10374142974615097, BER : 0.0387109375\n",
            "Train[42/50][111/240] Loss: 0.10647230595350266, BER : 0.03975\n",
            "Train[42/50][112/240] Loss: 0.10535190999507904, BER : 0.0397421875\n",
            "Train[42/50][113/240] Loss: 0.10522105544805527, BER : 0.03985546875\n",
            "Train[42/50][114/240] Loss: 0.10227502882480621, BER : 0.0388125\n",
            "Train[42/50][115/240] Loss: 0.10476413369178772, BER : 0.03946875\n",
            "Train[42/50][116/240] Loss: 0.10516537725925446, BER : 0.0395390625\n",
            "Train[42/50][117/240] Loss: 0.10514915734529495, BER : 0.03933203125\n",
            "Train[42/50][118/240] Loss: 0.10253510624170303, BER : 0.0380546875\n",
            "Train[42/50][119/240] Loss: 0.10341953486204147, BER : 0.038703125\n",
            "Train[42/50][120/240] Loss: 0.10282449424266815, BER : 0.038703125\n",
            "Train[42/50][121/240] Loss: 0.10272964835166931, BER : 0.03830078125\n",
            "Train[42/50][122/240] Loss: 0.1053057461977005, BER : 0.03994140625\n",
            "Train[42/50][123/240] Loss: 0.1029701828956604, BER : 0.03816015625\n",
            "Train[42/50][124/240] Loss: 0.10341167449951172, BER : 0.0391796875\n",
            "Train[42/50][125/240] Loss: 0.1044597327709198, BER : 0.039390625\n",
            "Train[42/50][126/240] Loss: 0.10337307304143906, BER : 0.03810546875\n",
            "Train[42/50][127/240] Loss: 0.10321718454360962, BER : 0.0387734375\n",
            "Train[42/50][128/240] Loss: 0.10225917398929596, BER : 0.0379921875\n",
            "Train[42/50][129/240] Loss: 0.10526609420776367, BER : 0.04018359375\n",
            "Train[42/50][130/240] Loss: 0.10467053949832916, BER : 0.03953515625\n",
            "Train[42/50][131/240] Loss: 0.10478481650352478, BER : 0.03984765625\n",
            "Train[42/50][132/240] Loss: 0.10419714450836182, BER : 0.03985546875\n",
            "Train[42/50][133/240] Loss: 0.10559403896331787, BER : 0.0398359375\n",
            "Train[42/50][134/240] Loss: 0.10112838447093964, BER : 0.0373359375\n",
            "Train[42/50][135/240] Loss: 0.10418295860290527, BER : 0.03887109375\n",
            "Train[42/50][136/240] Loss: 0.10131210088729858, BER : 0.037296875\n",
            "Train[42/50][137/240] Loss: 0.10469706356525421, BER : 0.03908203125\n",
            "Train[42/50][138/240] Loss: 0.10926882922649384, BER : 0.04234375\n",
            "Train[42/50][139/240] Loss: 0.10828327387571335, BER : 0.0416171875\n",
            "Train[42/50][140/240] Loss: 0.10432250797748566, BER : 0.03955859375\n",
            "Train[42/50][141/240] Loss: 0.10237277299165726, BER : 0.03752734375\n",
            "Train[42/50][142/240] Loss: 0.10198456048965454, BER : 0.03826953125\n",
            "Train[42/50][143/240] Loss: 0.1042516678571701, BER : 0.03919921875\n",
            "Train[42/50][144/240] Loss: 0.10302378237247467, BER : 0.0386640625\n",
            "Train[42/50][145/240] Loss: 0.10536082088947296, BER : 0.039484375\n",
            "Train[42/50][146/240] Loss: 0.10472913086414337, BER : 0.03925390625\n",
            "Train[42/50][147/240] Loss: 0.10434341430664062, BER : 0.03934765625\n",
            "Train[42/50][148/240] Loss: 0.10471786558628082, BER : 0.04013671875\n",
            "Train[42/50][149/240] Loss: 0.10609917342662811, BER : 0.04028125\n",
            "Train[42/50][150/240] Loss: 0.10442247986793518, BER : 0.03929296875\n",
            "Train[42/50][151/240] Loss: 0.10308169573545456, BER : 0.038640625\n",
            "Train[42/50][152/240] Loss: 0.10430724918842316, BER : 0.03951953125\n",
            "Train[42/50][153/240] Loss: 0.10023725032806396, BER : 0.03704296875\n",
            "Train[42/50][154/240] Loss: 0.10497353225946426, BER : 0.04011328125\n",
            "Train[42/50][155/240] Loss: 0.10687261074781418, BER : 0.04073828125\n",
            "Train[42/50][156/240] Loss: 0.10108835995197296, BER : 0.03811328125\n",
            "Train[42/50][157/240] Loss: 0.1038438230752945, BER : 0.039984375\n",
            "Train[42/50][158/240] Loss: 0.10413289070129395, BER : 0.0394296875\n",
            "Train[42/50][159/240] Loss: 0.10131452977657318, BER : 0.0374765625\n",
            "Train[42/50][160/240] Loss: 0.10406872630119324, BER : 0.03895703125\n",
            "Train[42/50][161/240] Loss: 0.10396820306777954, BER : 0.0388671875\n",
            "Train[42/50][162/240] Loss: 0.10261364281177521, BER : 0.03858203125\n",
            "Train[42/50][163/240] Loss: 0.10454493761062622, BER : 0.03903125\n",
            "Train[42/50][164/240] Loss: 0.10404431074857712, BER : 0.03865625\n",
            "Train[42/50][165/240] Loss: 0.10442353039979935, BER : 0.03946484375\n",
            "Train[42/50][166/240] Loss: 0.10101374983787537, BER : 0.03745703125\n",
            "Train[42/50][167/240] Loss: 0.10406289994716644, BER : 0.0392109375\n",
            "Train[42/50][168/240] Loss: 0.1033136248588562, BER : 0.038703125\n",
            "Train[42/50][169/240] Loss: 0.10285443067550659, BER : 0.03795703125\n",
            "Train[42/50][170/240] Loss: 0.10480545461177826, BER : 0.0394296875\n",
            "Train[42/50][171/240] Loss: 0.10294542461633682, BER : 0.038203125\n",
            "Train[42/50][172/240] Loss: 0.10287761688232422, BER : 0.0384453125\n",
            "Train[42/50][173/240] Loss: 0.10134346783161163, BER : 0.0380234375\n",
            "Train[42/50][174/240] Loss: 0.10256904363632202, BER : 0.03898046875\n",
            "Train[42/50][175/240] Loss: 0.10249689221382141, BER : 0.03868359375\n",
            "Train[42/50][176/240] Loss: 0.10455282032489777, BER : 0.0389453125\n",
            "Train[42/50][177/240] Loss: 0.10267424583435059, BER : 0.0381640625\n",
            "Train[42/50][178/240] Loss: 0.10605211555957794, BER : 0.040125\n",
            "Train[42/50][179/240] Loss: 0.10304362326860428, BER : 0.0384765625\n",
            "Train[42/50][180/240] Loss: 0.10252360254526138, BER : 0.0387578125\n",
            "Train[42/50][181/240] Loss: 0.10286331921815872, BER : 0.03864453125\n",
            "Train[42/50][182/240] Loss: 0.10435688495635986, BER : 0.03940234375\n",
            "Train[42/50][183/240] Loss: 0.10391927510499954, BER : 0.03959375\n",
            "Train[42/50][184/240] Loss: 0.10281666368246078, BER : 0.0389140625\n",
            "Train[42/50][185/240] Loss: 0.10334303975105286, BER : 0.0389921875\n",
            "Train[42/50][186/240] Loss: 0.10490468144416809, BER : 0.0395390625\n",
            "Train[42/50][187/240] Loss: 0.10466255247592926, BER : 0.039203125\n",
            "Train[42/50][188/240] Loss: 0.1029760092496872, BER : 0.0387421875\n",
            "Train[42/50][189/240] Loss: 0.10110431909561157, BER : 0.037984375\n",
            "Train[42/50][190/240] Loss: 0.10430082678794861, BER : 0.03846875\n",
            "Train[42/50][191/240] Loss: 0.10635034739971161, BER : 0.0406484375\n",
            "Train[42/50][192/240] Loss: 0.10166245698928833, BER : 0.037953125\n",
            "Train[42/50][193/240] Loss: 0.1024860218167305, BER : 0.03777734375\n",
            "Train[42/50][194/240] Loss: 0.1032804623246193, BER : 0.0388515625\n",
            "Train[42/50][195/240] Loss: 0.10344009846448898, BER : 0.03893359375\n",
            "Train[42/50][196/240] Loss: 0.10207991302013397, BER : 0.03846484375\n",
            "Train[42/50][197/240] Loss: 0.1032494381070137, BER : 0.0391328125\n",
            "Train[42/50][198/240] Loss: 0.10503685474395752, BER : 0.04018359375\n",
            "Train[42/50][199/240] Loss: 0.10176323354244232, BER : 0.0379375\n",
            "Train[42/50][200/240] Loss: 0.104248046875, BER : 0.03896484375\n",
            "Train[42/50][201/240] Loss: 0.10204761475324631, BER : 0.037953125\n",
            "Train[42/50][202/240] Loss: 0.10412776470184326, BER : 0.03953125\n",
            "Train[42/50][203/240] Loss: 0.10104302316904068, BER : 0.037234375\n",
            "Train[42/50][204/240] Loss: 0.10461613535881042, BER : 0.039375\n",
            "Train[42/50][205/240] Loss: 0.10636085271835327, BER : 0.040390625\n",
            "Train[42/50][206/240] Loss: 0.10419357568025589, BER : 0.0390859375\n",
            "Train[42/50][207/240] Loss: 0.10685224086046219, BER : 0.0402578125\n",
            "Train[42/50][208/240] Loss: 0.10361911356449127, BER : 0.03913671875\n",
            "Train[42/50][209/240] Loss: 0.10285001993179321, BER : 0.03864453125\n",
            "Train[42/50][210/240] Loss: 0.10196946561336517, BER : 0.0381328125\n",
            "Train[42/50][211/240] Loss: 0.10462543368339539, BER : 0.03976171875\n",
            "Train[42/50][212/240] Loss: 0.10313259065151215, BER : 0.03923828125\n",
            "Train[42/50][213/240] Loss: 0.10275772958993912, BER : 0.03869921875\n",
            "Train[42/50][214/240] Loss: 0.10121269524097443, BER : 0.0379765625\n",
            "Train[42/50][215/240] Loss: 0.10128270089626312, BER : 0.037265625\n",
            "Train[42/50][216/240] Loss: 0.1038329154253006, BER : 0.0391796875\n",
            "Train[42/50][217/240] Loss: 0.1024169772863388, BER : 0.03895703125\n",
            "Train[42/50][218/240] Loss: 0.1027199774980545, BER : 0.038796875\n",
            "Train[42/50][219/240] Loss: 0.10366372019052505, BER : 0.03887890625\n",
            "Train[42/50][220/240] Loss: 0.10571245849132538, BER : 0.03965234375\n",
            "Train[42/50][221/240] Loss: 0.10434887558221817, BER : 0.03944921875\n",
            "Train[42/50][222/240] Loss: 0.10863372683525085, BER : 0.04132421875\n",
            "Train[42/50][223/240] Loss: 0.10467024147510529, BER : 0.03915625\n",
            "Train[42/50][224/240] Loss: 0.10444296896457672, BER : 0.039265625\n",
            "Train[42/50][225/240] Loss: 0.10294431447982788, BER : 0.0381171875\n",
            "Train[42/50][226/240] Loss: 0.10254915803670883, BER : 0.03849609375\n",
            "Train[42/50][227/240] Loss: 0.1026867926120758, BER : 0.03812109375\n",
            "Train[42/50][228/240] Loss: 0.10448837280273438, BER : 0.0399921875\n",
            "Train[42/50][229/240] Loss: 0.10249583423137665, BER : 0.03873046875\n",
            "Train[42/50][230/240] Loss: 0.10327698290348053, BER : 0.03862890625\n",
            "Train[42/50][231/240] Loss: 0.10429325699806213, BER : 0.0397578125\n",
            "Train[42/50][232/240] Loss: 0.10417547821998596, BER : 0.03907421875\n",
            "Train[42/50][233/240] Loss: 0.1001206785440445, BER : 0.0362421875\n",
            "Train[42/50][234/240] Loss: 0.10506904870271683, BER : 0.0390625\n",
            "Train[42/50][235/240] Loss: 0.1014976054430008, BER : 0.03726171875\n",
            "Train[42/50][236/240] Loss: 0.10652130842208862, BER : 0.04118359375\n",
            "Train[42/50][237/240] Loss: 0.10484136641025543, BER : 0.0394296875\n",
            "Train[42/50][238/240] Loss: 0.10217118263244629, BER : 0.03846484375\n",
            "Train[42/50][239/240] Loss: 0.10359429568052292, BER : 0.03876953125\n",
            "Epoch [42/50] Train_Avg_loss(epoch): 0.25993\n",
            "-------------------------------------\n",
            "Test[42/50][0/40]  Loss: 0.0756417065858841, BER (test): 0.04165234375\n",
            "Test[42/50][1/40]  Loss: 0.07617858797311783, BER (test): 0.04237890625\n",
            "Test[42/50][2/40]  Loss: 0.07716257870197296, BER (test): 0.04162890625\n",
            "Test[42/50][3/40]  Loss: 0.07672794163227081, BER (test): 0.04162890625\n",
            "Test[42/50][4/40]  Loss: 0.07659675180912018, BER (test): 0.0413984375\n",
            "Test[42/50][5/40]  Loss: 0.07613853365182877, BER (test): 0.04113671875\n",
            "Test[42/50][6/40]  Loss: 0.0755312442779541, BER (test): 0.04060546875\n",
            "Test[42/50][7/40]  Loss: 0.07715021073818207, BER (test): 0.0416796875\n",
            "Test[42/50][8/40]  Loss: 0.07699192315340042, BER (test): 0.0420625\n",
            "Test[42/50][9/40]  Loss: 0.07606231421232224, BER (test): 0.0416796875\n",
            "Test[42/50][10/40]  Loss: 0.07616522163152695, BER (test): 0.04196875\n",
            "Test[42/50][11/40]  Loss: 0.07718616724014282, BER (test): 0.0419453125\n",
            "Test[42/50][12/40]  Loss: 0.07523862272500992, BER (test): 0.0409609375\n",
            "Test[42/50][13/40]  Loss: 0.07583936303853989, BER (test): 0.04105078125\n",
            "Test[42/50][14/40]  Loss: 0.07649098336696625, BER (test): 0.04238671875\n",
            "Test[42/50][15/40]  Loss: 0.07357726246118546, BER (test): 0.040515625\n",
            "Test[42/50][16/40]  Loss: 0.07732563465833664, BER (test): 0.04218359375\n",
            "Test[42/50][17/40]  Loss: 0.07720571756362915, BER (test): 0.04201171875\n",
            "Test[42/50][18/40]  Loss: 0.07635532319545746, BER (test): 0.0414921875\n",
            "Test[42/50][19/40]  Loss: 0.0765848308801651, BER (test): 0.04187890625\n",
            "Test[42/50][20/40]  Loss: 0.07653630524873734, BER (test): 0.04183984375\n",
            "Test[42/50][21/40]  Loss: 0.07430681586265564, BER (test): 0.0400234375\n",
            "Test[42/50][22/40]  Loss: 0.0789063423871994, BER (test): 0.043890625\n",
            "Test[42/50][23/40]  Loss: 0.07629095762968063, BER (test): 0.04001953125\n",
            "Test[42/50][24/40]  Loss: 0.07728423178195953, BER (test): 0.0425859375\n",
            "Test[42/50][25/40]  Loss: 0.07505449652671814, BER (test): 0.04066015625\n",
            "Test[42/50][26/40]  Loss: 0.07496890425682068, BER (test): 0.040609375\n",
            "Test[42/50][27/40]  Loss: 0.0771147608757019, BER (test): 0.041984375\n",
            "Test[42/50][28/40]  Loss: 0.074205182492733, BER (test): 0.04028515625\n",
            "Test[42/50][29/40]  Loss: 0.07759878039360046, BER (test): 0.04195703125\n",
            "Test[42/50][30/40]  Loss: 0.07685864716768265, BER (test): 0.04226171875\n",
            "Test[42/50][31/40]  Loss: 0.07543152570724487, BER (test): 0.04227734375\n",
            "Test[42/50][32/40]  Loss: 0.07412219792604446, BER (test): 0.04062890625\n",
            "Test[42/50][33/40]  Loss: 0.07710684090852737, BER (test): 0.04245703125\n",
            "Test[42/50][34/40]  Loss: 0.075306735932827, BER (test): 0.04148046875\n",
            "Test[42/50][35/40]  Loss: 0.0771244689822197, BER (test): 0.043046875\n",
            "Test[42/50][36/40]  Loss: 0.07565023750066757, BER (test): 0.04138671875\n",
            "Test[42/50][37/40]  Loss: 0.07549911737442017, BER (test): 0.0406015625\n",
            "Test[42/50][38/40]  Loss: 0.07780567556619644, BER (test): 0.043\n",
            "Test[42/50][39/40]  Loss: 0.0760716050863266, BER (test): 0.0415390625\n",
            "Test_Epoch [42/50] Test_Avg_loss(epoch): 0.07623\n",
            "Train[43/50][0/240] Loss: 0.10089284181594849, BER : 0.0380390625\n",
            "Train[43/50][1/240] Loss: 0.1038070023059845, BER : 0.03882421875\n",
            "Train[43/50][2/240] Loss: 0.10374388843774796, BER : 0.0386875\n",
            "Train[43/50][3/240] Loss: 0.10723761469125748, BER : 0.04026171875\n",
            "Train[43/50][4/240] Loss: 0.10538778454065323, BER : 0.03963671875\n",
            "Train[43/50][5/240] Loss: 0.10431931912899017, BER : 0.03927734375\n",
            "Train[43/50][6/240] Loss: 0.10467550158500671, BER : 0.0390859375\n",
            "Train[43/50][7/240] Loss: 0.10321077704429626, BER : 0.03916015625\n",
            "Train[43/50][8/240] Loss: 0.10562711954116821, BER : 0.04015625\n",
            "Train[43/50][9/240] Loss: 0.10404572635889053, BER : 0.0390625\n",
            "Train[43/50][10/240] Loss: 0.10509587079286575, BER : 0.0393984375\n",
            "Train[43/50][11/240] Loss: 0.10504727065563202, BER : 0.039671875\n",
            "Train[43/50][12/240] Loss: 0.10415489971637726, BER : 0.03958203125\n",
            "Train[43/50][13/240] Loss: 0.10657106339931488, BER : 0.04124609375\n",
            "Train[43/50][14/240] Loss: 0.10655540972948074, BER : 0.0409140625\n",
            "Train[43/50][15/240] Loss: 0.100660040974617, BER : 0.0378125\n",
            "Train[43/50][16/240] Loss: 0.10315246134996414, BER : 0.0390234375\n",
            "Train[43/50][17/240] Loss: 0.10413616895675659, BER : 0.0391796875\n",
            "Train[43/50][18/240] Loss: 0.10310469567775726, BER : 0.0385546875\n",
            "Train[43/50][19/240] Loss: 0.10763785243034363, BER : 0.04072265625\n",
            "Train[43/50][20/240] Loss: 0.10347509384155273, BER : 0.03926953125\n",
            "Train[43/50][21/240] Loss: 0.10262179374694824, BER : 0.03780078125\n",
            "Train[43/50][22/240] Loss: 0.10508657991886139, BER : 0.03936328125\n",
            "Train[43/50][23/240] Loss: 0.10293527692556381, BER : 0.038734375\n",
            "Train[43/50][24/240] Loss: 0.10309227555990219, BER : 0.03860546875\n",
            "Train[43/50][25/240] Loss: 0.10581175982952118, BER : 0.039828125\n",
            "Train[43/50][26/240] Loss: 0.10079442709684372, BER : 0.0373515625\n",
            "Train[43/50][27/240] Loss: 0.10350281000137329, BER : 0.03912890625\n",
            "Train[43/50][28/240] Loss: 0.10328490287065506, BER : 0.0390625\n",
            "Train[43/50][29/240] Loss: 0.10411887615919113, BER : 0.03996875\n",
            "Train[43/50][30/240] Loss: 0.10429167002439499, BER : 0.03890625\n",
            "Train[43/50][31/240] Loss: 0.10414860397577286, BER : 0.0396875\n",
            "Train[43/50][32/240] Loss: 0.10536304861307144, BER : 0.04012109375\n",
            "Train[43/50][33/240] Loss: 0.10062293708324432, BER : 0.03730078125\n",
            "Train[43/50][34/240] Loss: 0.10293659567832947, BER : 0.03845703125\n",
            "Train[43/50][35/240] Loss: 0.1036650538444519, BER : 0.0392734375\n",
            "Train[43/50][36/240] Loss: 0.1024513840675354, BER : 0.03836328125\n",
            "Train[43/50][37/240] Loss: 0.10436436533927917, BER : 0.03953515625\n",
            "Train[43/50][38/240] Loss: 0.10295077413320541, BER : 0.03808984375\n",
            "Train[43/50][39/240] Loss: 0.1039506047964096, BER : 0.04005078125\n",
            "Train[43/50][40/240] Loss: 0.10183914750814438, BER : 0.037546875\n",
            "Train[43/50][41/240] Loss: 0.10552146285772324, BER : 0.0400859375\n",
            "Train[43/50][42/240] Loss: 0.10446402430534363, BER : 0.03928515625\n",
            "Train[43/50][43/240] Loss: 0.10185186564922333, BER : 0.0374765625\n",
            "Train[43/50][44/240] Loss: 0.10088656097650528, BER : 0.03736328125\n",
            "Train[43/50][45/240] Loss: 0.10294748842716217, BER : 0.03855078125\n",
            "Train[43/50][46/240] Loss: 0.10327327251434326, BER : 0.03907421875\n",
            "Train[43/50][47/240] Loss: 0.1041727066040039, BER : 0.03919140625\n",
            "Train[43/50][48/240] Loss: 0.10294200479984283, BER : 0.03840234375\n",
            "Train[43/50][49/240] Loss: 0.10182449221611023, BER : 0.03812109375\n",
            "Train[43/50][50/240] Loss: 0.10106541216373444, BER : 0.037890625\n",
            "Train[43/50][51/240] Loss: 0.10141132771968842, BER : 0.0375234375\n",
            "Train[43/50][52/240] Loss: 0.10437475144863129, BER : 0.03975390625\n",
            "Train[43/50][53/240] Loss: 0.1033000648021698, BER : 0.039765625\n",
            "Train[43/50][54/240] Loss: 0.10386381298303604, BER : 0.0390625\n",
            "Train[43/50][55/240] Loss: 0.10232409834861755, BER : 0.03819140625\n",
            "Train[43/50][56/240] Loss: 0.10429230332374573, BER : 0.03907421875\n",
            "Train[43/50][57/240] Loss: 0.10224549472332001, BER : 0.0378203125\n",
            "Train[43/50][58/240] Loss: 0.10422415286302567, BER : 0.0391484375\n",
            "Train[43/50][59/240] Loss: 0.1013450175523758, BER : 0.0372734375\n",
            "Train[43/50][60/240] Loss: 0.10124193131923676, BER : 0.03765625\n",
            "Train[43/50][61/240] Loss: 0.10382705926895142, BER : 0.0397734375\n",
            "Train[43/50][62/240] Loss: 0.10349378734827042, BER : 0.0388203125\n",
            "Train[43/50][63/240] Loss: 0.10349264740943909, BER : 0.0382265625\n",
            "Train[43/50][64/240] Loss: 0.10168330371379852, BER : 0.0378515625\n",
            "Train[43/50][65/240] Loss: 0.1003042459487915, BER : 0.0374140625\n",
            "Train[43/50][66/240] Loss: 0.10331594198942184, BER : 0.0389375\n",
            "Train[43/50][67/240] Loss: 0.10571610182523727, BER : 0.04057421875\n",
            "Train[43/50][68/240] Loss: 0.1041770651936531, BER : 0.039734375\n",
            "Train[43/50][69/240] Loss: 0.10247978568077087, BER : 0.038390625\n",
            "Train[43/50][70/240] Loss: 0.10548681765794754, BER : 0.039734375\n",
            "Train[43/50][71/240] Loss: 0.10417722910642624, BER : 0.03925390625\n",
            "Train[43/50][72/240] Loss: 0.1042667031288147, BER : 0.0385703125\n",
            "Train[43/50][73/240] Loss: 0.1042342334985733, BER : 0.03940625\n",
            "Train[43/50][74/240] Loss: 0.10247047245502472, BER : 0.03854296875\n",
            "Train[43/50][75/240] Loss: 0.10403711348772049, BER : 0.03955078125\n",
            "Train[43/50][76/240] Loss: 0.10210701078176498, BER : 0.0378125\n",
            "Train[43/50][77/240] Loss: 0.1038941815495491, BER : 0.03891015625\n",
            "Train[43/50][78/240] Loss: 0.10359474271535873, BER : 0.03797265625\n",
            "Train[43/50][79/240] Loss: 0.10318654775619507, BER : 0.038546875\n",
            "Train[43/50][80/240] Loss: 0.10182701796293259, BER : 0.03760546875\n",
            "Train[43/50][81/240] Loss: 0.10215993970632553, BER : 0.0381796875\n",
            "Train[43/50][82/240] Loss: 0.10572842508554459, BER : 0.040171875\n",
            "Train[43/50][83/240] Loss: 0.10597723722457886, BER : 0.0403671875\n",
            "Train[43/50][84/240] Loss: 0.10344523191452026, BER : 0.03930859375\n",
            "Train[43/50][85/240] Loss: 0.10165132582187653, BER : 0.03729296875\n",
            "Train[43/50][86/240] Loss: 0.10298308730125427, BER : 0.0389453125\n",
            "Train[43/50][87/240] Loss: 0.10179980099201202, BER : 0.03807421875\n",
            "Train[43/50][88/240] Loss: 0.10596641153097153, BER : 0.04062890625\n",
            "Train[43/50][89/240] Loss: 0.10336194932460785, BER : 0.03875\n",
            "Train[43/50][90/240] Loss: 0.10324850678443909, BER : 0.03866015625\n",
            "Train[43/50][91/240] Loss: 0.10127235949039459, BER : 0.0379453125\n",
            "Train[43/50][92/240] Loss: 0.10361391305923462, BER : 0.03953515625\n",
            "Train[43/50][93/240] Loss: 0.10015559196472168, BER : 0.0367265625\n",
            "Train[43/50][94/240] Loss: 0.10086055845022202, BER : 0.0375078125\n",
            "Train[43/50][95/240] Loss: 0.10397865623235703, BER : 0.038828125\n",
            "Train[43/50][96/240] Loss: 0.10343767702579498, BER : 0.03908203125\n",
            "Train[43/50][97/240] Loss: 0.10522840917110443, BER : 0.04016015625\n",
            "Train[43/50][98/240] Loss: 0.10354748368263245, BER : 0.038671875\n",
            "Train[43/50][99/240] Loss: 0.10297687351703644, BER : 0.03906640625\n",
            "Train[43/50][100/240] Loss: 0.10202176868915558, BER : 0.03821484375\n",
            "Train[43/50][101/240] Loss: 0.10183216631412506, BER : 0.03791015625\n",
            "Train[43/50][102/240] Loss: 0.10196931660175323, BER : 0.0379375\n",
            "Train[43/50][103/240] Loss: 0.10309810936450958, BER : 0.03929296875\n",
            "Train[43/50][104/240] Loss: 0.10091877728700638, BER : 0.03746875\n",
            "Train[43/50][105/240] Loss: 0.10175562649965286, BER : 0.03812109375\n",
            "Train[43/50][106/240] Loss: 0.10239898413419724, BER : 0.03834765625\n",
            "Train[43/50][107/240] Loss: 0.10419350862503052, BER : 0.03893359375\n",
            "Train[43/50][108/240] Loss: 0.10579999536275864, BER : 0.0401875\n",
            "Train[43/50][109/240] Loss: 0.10176093876361847, BER : 0.03783984375\n",
            "Train[43/50][110/240] Loss: 0.10267229378223419, BER : 0.0387421875\n",
            "Train[43/50][111/240] Loss: 0.10427403450012207, BER : 0.03942578125\n",
            "Train[43/50][112/240] Loss: 0.10380947589874268, BER : 0.03934375\n",
            "Train[43/50][113/240] Loss: 0.10074235498905182, BER : 0.037546875\n",
            "Train[43/50][114/240] Loss: 0.10729333758354187, BER : 0.04152734375\n",
            "Train[43/50][115/240] Loss: 0.10426464676856995, BER : 0.0395390625\n",
            "Train[43/50][116/240] Loss: 0.10155782103538513, BER : 0.0383125\n",
            "Train[43/50][117/240] Loss: 0.10242074728012085, BER : 0.03823828125\n",
            "Train[43/50][118/240] Loss: 0.10300848633050919, BER : 0.03874609375\n",
            "Train[43/50][119/240] Loss: 0.10199759155511856, BER : 0.0377265625\n",
            "Train[43/50][120/240] Loss: 0.10108166188001633, BER : 0.03748046875\n",
            "Train[43/50][121/240] Loss: 0.10415372252464294, BER : 0.0397578125\n",
            "Train[43/50][122/240] Loss: 0.10014919936656952, BER : 0.03783203125\n",
            "Train[43/50][123/240] Loss: 0.10437056422233582, BER : 0.0396015625\n",
            "Train[43/50][124/240] Loss: 0.10275761783123016, BER : 0.0388125\n",
            "Train[43/50][125/240] Loss: 0.10367350280284882, BER : 0.03908203125\n",
            "Train[43/50][126/240] Loss: 0.10545645654201508, BER : 0.03999609375\n",
            "Train[43/50][127/240] Loss: 0.10213778167963028, BER : 0.03837109375\n",
            "Train[43/50][128/240] Loss: 0.10355588793754578, BER : 0.039296875\n",
            "Train[43/50][129/240] Loss: 0.10482379794120789, BER : 0.03990234375\n",
            "Train[43/50][130/240] Loss: 0.10624951124191284, BER : 0.04088671875\n",
            "Train[43/50][131/240] Loss: 0.1043146476149559, BER : 0.03853125\n",
            "Train[43/50][132/240] Loss: 0.10706789791584015, BER : 0.04117578125\n",
            "Train[43/50][133/240] Loss: 0.10217133164405823, BER : 0.03784375\n",
            "Train[43/50][134/240] Loss: 0.10104402154684067, BER : 0.03733984375\n",
            "Train[43/50][135/240] Loss: 0.10498838126659393, BER : 0.03955859375\n",
            "Train[43/50][136/240] Loss: 0.10533854365348816, BER : 0.0401875\n",
            "Train[43/50][137/240] Loss: 0.1022036001086235, BER : 0.0381328125\n",
            "Train[43/50][138/240] Loss: 0.10105562210083008, BER : 0.03810546875\n",
            "Train[43/50][139/240] Loss: 0.10043346136808395, BER : 0.037078125\n",
            "Train[43/50][140/240] Loss: 0.10183022171258926, BER : 0.037453125\n",
            "Train[43/50][141/240] Loss: 0.10641324520111084, BER : 0.04077734375\n",
            "Train[43/50][142/240] Loss: 0.10325121879577637, BER : 0.03887109375\n",
            "Train[43/50][143/240] Loss: 0.10537116229534149, BER : 0.0391875\n",
            "Train[43/50][144/240] Loss: 0.10429463535547256, BER : 0.03951953125\n",
            "Train[43/50][145/240] Loss: 0.10631438344717026, BER : 0.040828125\n",
            "Train[43/50][146/240] Loss: 0.10190244019031525, BER : 0.0377421875\n",
            "Train[43/50][147/240] Loss: 0.1029859408736229, BER : 0.038875\n",
            "Train[43/50][148/240] Loss: 0.10138608515262604, BER : 0.03784765625\n",
            "Train[43/50][149/240] Loss: 0.10328900814056396, BER : 0.0389921875\n",
            "Train[43/50][150/240] Loss: 0.1060575395822525, BER : 0.0401484375\n",
            "Train[43/50][151/240] Loss: 0.10311584174633026, BER : 0.03811328125\n",
            "Train[43/50][152/240] Loss: 0.10339640080928802, BER : 0.03900390625\n",
            "Train[43/50][153/240] Loss: 0.10521651059389114, BER : 0.0394921875\n",
            "Train[43/50][154/240] Loss: 0.10431370139122009, BER : 0.039484375\n",
            "Train[43/50][155/240] Loss: 0.10574198514223099, BER : 0.0401328125\n",
            "Train[43/50][156/240] Loss: 0.10331358015537262, BER : 0.03918359375\n",
            "Train[43/50][157/240] Loss: 0.10309237241744995, BER : 0.03879296875\n",
            "Train[43/50][158/240] Loss: 0.10174394398927689, BER : 0.0374453125\n",
            "Train[43/50][159/240] Loss: 0.10654732584953308, BER : 0.0404609375\n",
            "Train[43/50][160/240] Loss: 0.10109169781208038, BER : 0.03830078125\n",
            "Train[43/50][161/240] Loss: 0.10398656874895096, BER : 0.03933984375\n",
            "Train[43/50][162/240] Loss: 0.10353737324476242, BER : 0.03928515625\n",
            "Train[43/50][163/240] Loss: 0.10548318922519684, BER : 0.0407109375\n",
            "Train[43/50][164/240] Loss: 0.10211456567049026, BER : 0.03752734375\n",
            "Train[43/50][165/240] Loss: 0.10388851910829544, BER : 0.0393046875\n",
            "Train[43/50][166/240] Loss: 0.10163760185241699, BER : 0.03758203125\n",
            "Train[43/50][167/240] Loss: 0.10740143060684204, BER : 0.040921875\n",
            "Train[43/50][168/240] Loss: 0.09959457814693451, BER : 0.03665625\n",
            "Train[43/50][169/240] Loss: 0.10216785967350006, BER : 0.03838671875\n",
            "Train[43/50][170/240] Loss: 0.10161998867988586, BER : 0.03826953125\n",
            "Train[43/50][171/240] Loss: 0.10184182226657867, BER : 0.03751953125\n",
            "Train[43/50][172/240] Loss: 0.10051149874925613, BER : 0.03721875\n",
            "Train[43/50][173/240] Loss: 0.10449552536010742, BER : 0.039578125\n",
            "Train[43/50][174/240] Loss: 0.1042749434709549, BER : 0.038671875\n",
            "Train[43/50][175/240] Loss: 0.10188084095716476, BER : 0.03812109375\n",
            "Train[43/50][176/240] Loss: 0.10133028775453568, BER : 0.0378046875\n",
            "Train[43/50][177/240] Loss: 0.10207801312208176, BER : 0.0386875\n",
            "Train[43/50][178/240] Loss: 0.10285946726799011, BER : 0.03905859375\n",
            "Train[43/50][179/240] Loss: 0.10325507074594498, BER : 0.03854296875\n",
            "Train[43/50][180/240] Loss: 0.1044197827577591, BER : 0.03948046875\n",
            "Train[43/50][181/240] Loss: 0.1043829396367073, BER : 0.03976953125\n",
            "Train[43/50][182/240] Loss: 0.10236356407403946, BER : 0.03896484375\n",
            "Train[43/50][183/240] Loss: 0.10328018665313721, BER : 0.03840625\n",
            "Train[43/50][184/240] Loss: 0.1020498275756836, BER : 0.03826171875\n",
            "Train[43/50][185/240] Loss: 0.10173571109771729, BER : 0.03778515625\n",
            "Train[43/50][186/240] Loss: 0.10307593643665314, BER : 0.03903515625\n",
            "Train[43/50][187/240] Loss: 0.10181117057800293, BER : 0.03815625\n",
            "Train[43/50][188/240] Loss: 0.10225920379161835, BER : 0.0385078125\n",
            "Train[43/50][189/240] Loss: 0.10453180223703384, BER : 0.0394609375\n",
            "Train[43/50][190/240] Loss: 0.10263874381780624, BER : 0.03863671875\n",
            "Train[43/50][191/240] Loss: 0.10372878611087799, BER : 0.03878515625\n",
            "Train[43/50][192/240] Loss: 0.1040579080581665, BER : 0.0396171875\n",
            "Train[43/50][193/240] Loss: 0.10166540741920471, BER : 0.03826171875\n",
            "Train[43/50][194/240] Loss: 0.10559207201004028, BER : 0.0403046875\n",
            "Train[43/50][195/240] Loss: 0.1025359183549881, BER : 0.03847265625\n",
            "Train[43/50][196/240] Loss: 0.10461568832397461, BER : 0.0400546875\n",
            "Train[43/50][197/240] Loss: 0.10352513194084167, BER : 0.039390625\n",
            "Train[43/50][198/240] Loss: 0.10288448631763458, BER : 0.0390078125\n",
            "Train[43/50][199/240] Loss: 0.10341010987758636, BER : 0.03928125\n",
            "Train[43/50][200/240] Loss: 0.10233938694000244, BER : 0.03794140625\n",
            "Train[43/50][201/240] Loss: 0.10407601296901703, BER : 0.039078125\n",
            "Train[43/50][202/240] Loss: 0.10321159660816193, BER : 0.03816796875\n",
            "Train[43/50][203/240] Loss: 0.09943463653326035, BER : 0.03654296875\n",
            "Train[43/50][204/240] Loss: 0.10281187295913696, BER : 0.0392109375\n",
            "Train[43/50][205/240] Loss: 0.10313502699136734, BER : 0.03880859375\n",
            "Train[43/50][206/240] Loss: 0.09859029948711395, BER : 0.03655078125\n",
            "Train[43/50][207/240] Loss: 0.10304193943738937, BER : 0.03891015625\n",
            "Train[43/50][208/240] Loss: 0.10144975036382675, BER : 0.0377421875\n",
            "Train[43/50][209/240] Loss: 0.10298161208629608, BER : 0.03901953125\n",
            "Train[43/50][210/240] Loss: 0.1053011417388916, BER : 0.03983984375\n",
            "Train[43/50][211/240] Loss: 0.10232686251401901, BER : 0.03837890625\n",
            "Train[43/50][212/240] Loss: 0.10256274789571762, BER : 0.038515625\n",
            "Train[43/50][213/240] Loss: 0.10457940399646759, BER : 0.03991796875\n",
            "Train[43/50][214/240] Loss: 0.10367998480796814, BER : 0.038984375\n",
            "Train[43/50][215/240] Loss: 0.1015811339020729, BER : 0.03809765625\n",
            "Train[43/50][216/240] Loss: 0.10307331383228302, BER : 0.03891015625\n",
            "Train[43/50][217/240] Loss: 0.10435585677623749, BER : 0.03958984375\n",
            "Train[43/50][218/240] Loss: 0.10252915322780609, BER : 0.03888671875\n",
            "Train[43/50][219/240] Loss: 0.10297753661870956, BER : 0.03876953125\n",
            "Train[43/50][220/240] Loss: 0.10395646095275879, BER : 0.0391875\n",
            "Train[43/50][221/240] Loss: 0.10275308042764664, BER : 0.0386640625\n",
            "Train[43/50][222/240] Loss: 0.10540195554494858, BER : 0.039921875\n",
            "Train[43/50][223/240] Loss: 0.1030491292476654, BER : 0.03934765625\n",
            "Train[43/50][224/240] Loss: 0.10085465013980865, BER : 0.03812890625\n",
            "Train[43/50][225/240] Loss: 0.10202642530202866, BER : 0.0380625\n",
            "Train[43/50][226/240] Loss: 0.10169009864330292, BER : 0.03757421875\n",
            "Train[43/50][227/240] Loss: 0.10233400762081146, BER : 0.0384140625\n",
            "Train[43/50][228/240] Loss: 0.1034998968243599, BER : 0.038828125\n",
            "Train[43/50][229/240] Loss: 0.10125696659088135, BER : 0.03798828125\n",
            "Train[43/50][230/240] Loss: 0.10482169687747955, BER : 0.03920703125\n",
            "Train[43/50][231/240] Loss: 0.10393164306879044, BER : 0.03910546875\n",
            "Train[43/50][232/240] Loss: 0.10432691872119904, BER : 0.03903515625\n",
            "Train[43/50][233/240] Loss: 0.1061307042837143, BER : 0.040078125\n",
            "Train[43/50][234/240] Loss: 0.10150355100631714, BER : 0.0380546875\n",
            "Train[43/50][235/240] Loss: 0.10401302576065063, BER : 0.03918359375\n",
            "Train[43/50][236/240] Loss: 0.10027259588241577, BER : 0.03690625\n",
            "Train[43/50][237/240] Loss: 0.10140368342399597, BER : 0.0380390625\n",
            "Train[43/50][238/240] Loss: 0.10140654444694519, BER : 0.03787890625\n",
            "Train[43/50][239/240] Loss: 0.10369475930929184, BER : 0.0389375\n",
            "Epoch [43/50] Train_Avg_loss(epoch): 0.25637\n",
            "-------------------------------------\n",
            "Test[43/50][0/40]  Loss: 0.07442104816436768, BER (test): 0.04112109375\n",
            "Test[43/50][1/40]  Loss: 0.07118992507457733, BER (test): 0.03912890625\n",
            "Test[43/50][2/40]  Loss: 0.07432368397712708, BER (test): 0.0406875\n",
            "Test[43/50][3/40]  Loss: 0.07178953289985657, BER (test): 0.039578125\n",
            "Test[43/50][4/40]  Loss: 0.0686899870634079, BER (test): 0.037765625\n",
            "Test[43/50][5/40]  Loss: 0.07393400371074677, BER (test): 0.04134765625\n",
            "Test[43/50][6/40]  Loss: 0.07475811243057251, BER (test): 0.04090234375\n",
            "Test[43/50][7/40]  Loss: 0.07628821581602097, BER (test): 0.042625\n",
            "Test[43/50][8/40]  Loss: 0.07273544371128082, BER (test): 0.03998828125\n",
            "Test[43/50][9/40]  Loss: 0.07262879610061646, BER (test): 0.04065234375\n",
            "Test[43/50][10/40]  Loss: 0.0733892172574997, BER (test): 0.0408515625\n",
            "Test[43/50][11/40]  Loss: 0.07467515021562576, BER (test): 0.04156640625\n",
            "Test[43/50][12/40]  Loss: 0.07502290606498718, BER (test): 0.042265625\n",
            "Test[43/50][13/40]  Loss: 0.07394029200077057, BER (test): 0.04112890625\n",
            "Test[43/50][14/40]  Loss: 0.07443688064813614, BER (test): 0.04158203125\n",
            "Test[43/50][15/40]  Loss: 0.07127181440591812, BER (test): 0.0396875\n",
            "Test[43/50][16/40]  Loss: 0.07356627285480499, BER (test): 0.040671875\n",
            "Test[43/50][17/40]  Loss: 0.073590949177742, BER (test): 0.04067578125\n",
            "Test[43/50][18/40]  Loss: 0.07497918605804443, BER (test): 0.0416328125\n",
            "Test[43/50][19/40]  Loss: 0.07645976543426514, BER (test): 0.04225390625\n",
            "Test[43/50][20/40]  Loss: 0.07397180050611496, BER (test): 0.0410546875\n",
            "Test[43/50][21/40]  Loss: 0.07326985150575638, BER (test): 0.04065234375\n",
            "Test[43/50][22/40]  Loss: 0.07383308559656143, BER (test): 0.0408046875\n",
            "Test[43/50][23/40]  Loss: 0.07321316003799438, BER (test): 0.04055078125\n",
            "Test[43/50][24/40]  Loss: 0.07326247543096542, BER (test): 0.04116015625\n",
            "Test[43/50][25/40]  Loss: 0.07599631696939468, BER (test): 0.04254296875\n",
            "Test[43/50][26/40]  Loss: 0.07279162108898163, BER (test): 0.0400859375\n",
            "Test[43/50][27/40]  Loss: 0.0722484439611435, BER (test): 0.0403125\n",
            "Test[43/50][28/40]  Loss: 0.07558763027191162, BER (test): 0.041328125\n",
            "Test[43/50][29/40]  Loss: 0.07323391735553741, BER (test): 0.04011328125\n",
            "Test[43/50][30/40]  Loss: 0.07454624772071838, BER (test): 0.04108203125\n",
            "Test[43/50][31/40]  Loss: 0.0750175416469574, BER (test): 0.04193359375\n",
            "Test[43/50][32/40]  Loss: 0.07427603751420975, BER (test): 0.04083203125\n",
            "Test[43/50][33/40]  Loss: 0.07443685829639435, BER (test): 0.04131640625\n",
            "Test[43/50][34/40]  Loss: 0.07358580827713013, BER (test): 0.04115234375\n",
            "Test[43/50][35/40]  Loss: 0.07310162484645844, BER (test): 0.03958203125\n",
            "Test[43/50][36/40]  Loss: 0.0725785344839096, BER (test): 0.040875\n",
            "Test[43/50][37/40]  Loss: 0.07510850578546524, BER (test): 0.0417734375\n",
            "Test[43/50][38/40]  Loss: 0.07131040841341019, BER (test): 0.03952734375\n",
            "Test[43/50][39/40]  Loss: 0.07414346933364868, BER (test): 0.0410859375\n",
            "Test_Epoch [43/50] Test_Avg_loss(epoch): 0.07369\n",
            "Train[44/50][0/240] Loss: 0.10416257381439209, BER : 0.0397890625\n",
            "Train[44/50][1/240] Loss: 0.10419243574142456, BER : 0.0396640625\n",
            "Train[44/50][2/240] Loss: 0.10011284798383713, BER : 0.036953125\n",
            "Train[44/50][3/240] Loss: 0.10152456164360046, BER : 0.03767578125\n",
            "Train[44/50][4/240] Loss: 0.10304883867502213, BER : 0.03901171875\n",
            "Train[44/50][5/240] Loss: 0.10217245668172836, BER : 0.03778515625\n",
            "Train[44/50][6/240] Loss: 0.10304055362939835, BER : 0.03892578125\n",
            "Train[44/50][7/240] Loss: 0.10103557258844376, BER : 0.037515625\n",
            "Train[44/50][8/240] Loss: 0.10078743100166321, BER : 0.037421875\n",
            "Train[44/50][9/240] Loss: 0.10121293365955353, BER : 0.03741015625\n",
            "Train[44/50][10/240] Loss: 0.10517489165067673, BER : 0.039734375\n",
            "Train[44/50][11/240] Loss: 0.1034640520811081, BER : 0.03965625\n",
            "Train[44/50][12/240] Loss: 0.10255683213472366, BER : 0.0388828125\n",
            "Train[44/50][13/240] Loss: 0.10312698781490326, BER : 0.03856640625\n",
            "Train[44/50][14/240] Loss: 0.10015308856964111, BER : 0.03739453125\n",
            "Train[44/50][15/240] Loss: 0.10220450162887573, BER : 0.03790625\n",
            "Train[44/50][16/240] Loss: 0.10349281877279282, BER : 0.0397421875\n",
            "Train[44/50][17/240] Loss: 0.10209330171346664, BER : 0.03889453125\n",
            "Train[44/50][18/240] Loss: 0.1022166833281517, BER : 0.03888671875\n",
            "Train[44/50][19/240] Loss: 0.10176973044872284, BER : 0.03772265625\n",
            "Train[44/50][20/240] Loss: 0.10261078178882599, BER : 0.039125\n",
            "Train[44/50][21/240] Loss: 0.10201825946569443, BER : 0.03827734375\n",
            "Train[44/50][22/240] Loss: 0.10364468395709991, BER : 0.03869140625\n",
            "Train[44/50][23/240] Loss: 0.10164535790681839, BER : 0.037515625\n",
            "Train[44/50][24/240] Loss: 0.10135285556316376, BER : 0.0378515625\n",
            "Train[44/50][25/240] Loss: 0.10204192996025085, BER : 0.03819921875\n",
            "Train[44/50][26/240] Loss: 0.10271429270505905, BER : 0.0383046875\n",
            "Train[44/50][27/240] Loss: 0.10375572741031647, BER : 0.0395625\n",
            "Train[44/50][28/240] Loss: 0.10036608576774597, BER : 0.037078125\n",
            "Train[44/50][29/240] Loss: 0.10129465907812119, BER : 0.037796875\n",
            "Train[44/50][30/240] Loss: 0.10247348248958588, BER : 0.0384609375\n",
            "Train[44/50][31/240] Loss: 0.1035633310675621, BER : 0.03919921875\n",
            "Train[44/50][32/240] Loss: 0.10132317245006561, BER : 0.03840234375\n",
            "Train[44/50][33/240] Loss: 0.10465401411056519, BER : 0.03944140625\n",
            "Train[44/50][34/240] Loss: 0.09972620010375977, BER : 0.0371953125\n",
            "Train[44/50][35/240] Loss: 0.10369333624839783, BER : 0.04021875\n",
            "Train[44/50][36/240] Loss: 0.10599912703037262, BER : 0.04118359375\n",
            "Train[44/50][37/240] Loss: 0.10147392749786377, BER : 0.03770703125\n",
            "Train[44/50][38/240] Loss: 0.10311741381883621, BER : 0.0386484375\n",
            "Train[44/50][39/240] Loss: 0.10624727606773376, BER : 0.0405390625\n",
            "Train[44/50][40/240] Loss: 0.10525833070278168, BER : 0.03990625\n",
            "Train[44/50][41/240] Loss: 0.10594473034143448, BER : 0.0407265625\n",
            "Train[44/50][42/240] Loss: 0.10258957743644714, BER : 0.03883203125\n",
            "Train[44/50][43/240] Loss: 0.10375247895717621, BER : 0.03859765625\n",
            "Train[44/50][44/240] Loss: 0.10404489189386368, BER : 0.03940234375\n",
            "Train[44/50][45/240] Loss: 0.10312312841415405, BER : 0.038859375\n",
            "Train[44/50][46/240] Loss: 0.10222435742616653, BER : 0.0385859375\n",
            "Train[44/50][47/240] Loss: 0.1015201285481453, BER : 0.0377421875\n",
            "Train[44/50][48/240] Loss: 0.10230068862438202, BER : 0.03809765625\n",
            "Train[44/50][49/240] Loss: 0.10186005383729935, BER : 0.03745703125\n",
            "Train[44/50][50/240] Loss: 0.10330849140882492, BER : 0.0382421875\n",
            "Train[44/50][51/240] Loss: 0.10613666474819183, BER : 0.0411328125\n",
            "Train[44/50][52/240] Loss: 0.10182841122150421, BER : 0.03847265625\n",
            "Train[44/50][53/240] Loss: 0.10451511293649673, BER : 0.03990234375\n",
            "Train[44/50][54/240] Loss: 0.1037515327334404, BER : 0.039078125\n",
            "Train[44/50][55/240] Loss: 0.1041557639837265, BER : 0.03948046875\n",
            "Train[44/50][56/240] Loss: 0.10093747079372406, BER : 0.0376328125\n",
            "Train[44/50][57/240] Loss: 0.10232259333133698, BER : 0.0384453125\n",
            "Train[44/50][58/240] Loss: 0.1014336496591568, BER : 0.0385\n",
            "Train[44/50][59/240] Loss: 0.101754330098629, BER : 0.03837109375\n",
            "Train[44/50][60/240] Loss: 0.10173019766807556, BER : 0.03784375\n",
            "Train[44/50][61/240] Loss: 0.10223311185836792, BER : 0.0385859375\n",
            "Train[44/50][62/240] Loss: 0.10338829457759857, BER : 0.039109375\n",
            "Train[44/50][63/240] Loss: 0.10166424512863159, BER : 0.03756640625\n",
            "Train[44/50][64/240] Loss: 0.10516233742237091, BER : 0.0398984375\n",
            "Train[44/50][65/240] Loss: 0.10481930524110794, BER : 0.039203125\n",
            "Train[44/50][66/240] Loss: 0.10662560164928436, BER : 0.04044921875\n",
            "Train[44/50][67/240] Loss: 0.10303225368261337, BER : 0.0384375\n",
            "Train[44/50][68/240] Loss: 0.10608816146850586, BER : 0.0405546875\n",
            "Train[44/50][69/240] Loss: 0.10247177630662918, BER : 0.03786328125\n",
            "Train[44/50][70/240] Loss: 0.10173475742340088, BER : 0.0380234375\n",
            "Train[44/50][71/240] Loss: 0.10363239794969559, BER : 0.03876171875\n",
            "Train[44/50][72/240] Loss: 0.10585062205791473, BER : 0.04006640625\n",
            "Train[44/50][73/240] Loss: 0.10220370441675186, BER : 0.03859375\n",
            "Train[44/50][74/240] Loss: 0.10228031128644943, BER : 0.03821875\n",
            "Train[44/50][75/240] Loss: 0.10697896778583527, BER : 0.04116015625\n",
            "Train[44/50][76/240] Loss: 0.10265697538852692, BER : 0.03836328125\n",
            "Train[44/50][77/240] Loss: 0.10243602097034454, BER : 0.03806640625\n",
            "Train[44/50][78/240] Loss: 0.10345880687236786, BER : 0.0388125\n",
            "Train[44/50][79/240] Loss: 0.10060553252696991, BER : 0.03759765625\n",
            "Train[44/50][80/240] Loss: 0.1033259779214859, BER : 0.0393125\n",
            "Train[44/50][81/240] Loss: 0.10246243327856064, BER : 0.0381796875\n",
            "Train[44/50][82/240] Loss: 0.10300732403993607, BER : 0.038703125\n",
            "Train[44/50][83/240] Loss: 0.10444071888923645, BER : 0.03966015625\n",
            "Train[44/50][84/240] Loss: 0.10223974287509918, BER : 0.037890625\n",
            "Train[44/50][85/240] Loss: 0.10252487659454346, BER : 0.03836328125\n",
            "Train[44/50][86/240] Loss: 0.10431906580924988, BER : 0.0390390625\n",
            "Train[44/50][87/240] Loss: 0.10563471168279648, BER : 0.039859375\n",
            "Train[44/50][88/240] Loss: 0.10346853733062744, BER : 0.039046875\n",
            "Train[44/50][89/240] Loss: 0.09930528700351715, BER : 0.03664453125\n",
            "Train[44/50][90/240] Loss: 0.1032382994890213, BER : 0.0386953125\n",
            "Train[44/50][91/240] Loss: 0.1051228791475296, BER : 0.03916796875\n",
            "Train[44/50][92/240] Loss: 0.10555778443813324, BER : 0.04063671875\n",
            "Train[44/50][93/240] Loss: 0.1020386815071106, BER : 0.03880859375\n",
            "Train[44/50][94/240] Loss: 0.10450408607721329, BER : 0.0395625\n",
            "Train[44/50][95/240] Loss: 0.10216912627220154, BER : 0.03816015625\n",
            "Train[44/50][96/240] Loss: 0.10244664549827576, BER : 0.03905078125\n",
            "Train[44/50][97/240] Loss: 0.10197340697050095, BER : 0.0379375\n",
            "Train[44/50][98/240] Loss: 0.1020372062921524, BER : 0.03794140625\n",
            "Train[44/50][99/240] Loss: 0.10093647241592407, BER : 0.03808984375\n",
            "Train[44/50][100/240] Loss: 0.1004418283700943, BER : 0.03716015625\n",
            "Train[44/50][101/240] Loss: 0.10466810315847397, BER : 0.0393671875\n",
            "Train[44/50][102/240] Loss: 0.10414683073759079, BER : 0.039359375\n",
            "Train[44/50][103/240] Loss: 0.1041533499956131, BER : 0.03928125\n",
            "Train[44/50][104/240] Loss: 0.10529230535030365, BER : 0.03984765625\n",
            "Train[44/50][105/240] Loss: 0.10134454816579819, BER : 0.03782421875\n",
            "Train[44/50][106/240] Loss: 0.10140720009803772, BER : 0.03812109375\n",
            "Train[44/50][107/240] Loss: 0.10193631052970886, BER : 0.03809765625\n",
            "Train[44/50][108/240] Loss: 0.1021965891122818, BER : 0.03811328125\n",
            "Train[44/50][109/240] Loss: 0.10209573805332184, BER : 0.03826953125\n",
            "Train[44/50][110/240] Loss: 0.10277475416660309, BER : 0.03830078125\n",
            "Train[44/50][111/240] Loss: 0.10056553781032562, BER : 0.037171875\n",
            "Train[44/50][112/240] Loss: 0.10177553445100784, BER : 0.0384765625\n",
            "Train[44/50][113/240] Loss: 0.10192926228046417, BER : 0.037796875\n",
            "Train[44/50][114/240] Loss: 0.10336670279502869, BER : 0.03900390625\n",
            "Train[44/50][115/240] Loss: 0.10214896500110626, BER : 0.03826171875\n",
            "Train[44/50][116/240] Loss: 0.10350010544061661, BER : 0.0393671875\n",
            "Train[44/50][117/240] Loss: 0.09937784075737, BER : 0.03680078125\n",
            "Train[44/50][118/240] Loss: 0.10289192199707031, BER : 0.038515625\n",
            "Train[44/50][119/240] Loss: 0.10337559878826141, BER : 0.038828125\n",
            "Train[44/50][120/240] Loss: 0.10504428297281265, BER : 0.03933984375\n",
            "Train[44/50][121/240] Loss: 0.10355003923177719, BER : 0.039140625\n",
            "Train[44/50][122/240] Loss: 0.10171152651309967, BER : 0.03788671875\n",
            "Train[44/50][123/240] Loss: 0.10183684527873993, BER : 0.0386875\n",
            "Train[44/50][124/240] Loss: 0.10308390855789185, BER : 0.0387109375\n",
            "Train[44/50][125/240] Loss: 0.10394365340471268, BER : 0.0389921875\n",
            "Train[44/50][126/240] Loss: 0.10206218808889389, BER : 0.03798828125\n",
            "Train[44/50][127/240] Loss: 0.10041265189647675, BER : 0.03729296875\n",
            "Train[44/50][128/240] Loss: 0.1020960807800293, BER : 0.0383984375\n",
            "Train[44/50][129/240] Loss: 0.10339602828025818, BER : 0.03887890625\n",
            "Train[44/50][130/240] Loss: 0.10409781336784363, BER : 0.04028125\n",
            "Train[44/50][131/240] Loss: 0.10161090642213821, BER : 0.03767578125\n",
            "Train[44/50][132/240] Loss: 0.0996377244591713, BER : 0.0369453125\n",
            "Train[44/50][133/240] Loss: 0.10300341248512268, BER : 0.03841015625\n",
            "Train[44/50][134/240] Loss: 0.10177217423915863, BER : 0.03827734375\n",
            "Train[44/50][135/240] Loss: 0.10303038358688354, BER : 0.0392578125\n",
            "Train[44/50][136/240] Loss: 0.1013374924659729, BER : 0.03803125\n",
            "Train[44/50][137/240] Loss: 0.1025310754776001, BER : 0.03805859375\n",
            "Train[44/50][138/240] Loss: 0.10443001240491867, BER : 0.0394609375\n",
            "Train[44/50][139/240] Loss: 0.10484923422336578, BER : 0.0401328125\n",
            "Train[44/50][140/240] Loss: 0.10335639119148254, BER : 0.03839453125\n",
            "Train[44/50][141/240] Loss: 0.1007399633526802, BER : 0.0374765625\n",
            "Train[44/50][142/240] Loss: 0.10539025813341141, BER : 0.04041015625\n",
            "Train[44/50][143/240] Loss: 0.10093975812196732, BER : 0.03815234375\n",
            "Train[44/50][144/240] Loss: 0.1007310301065445, BER : 0.03692578125\n",
            "Train[44/50][145/240] Loss: 0.10123325884342194, BER : 0.0367890625\n",
            "Train[44/50][146/240] Loss: 0.10218963772058487, BER : 0.03870703125\n",
            "Train[44/50][147/240] Loss: 0.10403803735971451, BER : 0.03938671875\n",
            "Train[44/50][148/240] Loss: 0.10734684020280838, BER : 0.04164453125\n",
            "Train[44/50][149/240] Loss: 0.10107693076133728, BER : 0.037578125\n",
            "Train[44/50][150/240] Loss: 0.10343815386295319, BER : 0.03980078125\n",
            "Train[44/50][151/240] Loss: 0.10414748638868332, BER : 0.03938671875\n",
            "Train[44/50][152/240] Loss: 0.10298889875411987, BER : 0.03898828125\n",
            "Train[44/50][153/240] Loss: 0.10402795672416687, BER : 0.0393359375\n",
            "Train[44/50][154/240] Loss: 0.10455318540334702, BER : 0.04000390625\n",
            "Train[44/50][155/240] Loss: 0.10498887300491333, BER : 0.039921875\n",
            "Train[44/50][156/240] Loss: 0.10116039961576462, BER : 0.037359375\n",
            "Train[44/50][157/240] Loss: 0.10165617614984512, BER : 0.038609375\n",
            "Train[44/50][158/240] Loss: 0.10052213072776794, BER : 0.0371796875\n",
            "Train[44/50][159/240] Loss: 0.10334260761737823, BER : 0.03883984375\n",
            "Train[44/50][160/240] Loss: 0.10595528781414032, BER : 0.0397109375\n",
            "Train[44/50][161/240] Loss: 0.10252124071121216, BER : 0.03844140625\n",
            "Train[44/50][162/240] Loss: 0.10313290357589722, BER : 0.03948828125\n",
            "Train[44/50][163/240] Loss: 0.10355737805366516, BER : 0.03968359375\n",
            "Train[44/50][164/240] Loss: 0.09711265563964844, BER : 0.03582421875\n",
            "Train[44/50][165/240] Loss: 0.1045769602060318, BER : 0.03944140625\n",
            "Train[44/50][166/240] Loss: 0.1029658168554306, BER : 0.0387890625\n",
            "Train[44/50][167/240] Loss: 0.10220205783843994, BER : 0.03826953125\n",
            "Train[44/50][168/240] Loss: 0.10106870532035828, BER : 0.037640625\n",
            "Train[44/50][169/240] Loss: 0.10401512682437897, BER : 0.039234375\n",
            "Train[44/50][170/240] Loss: 0.09973793476819992, BER : 0.0363984375\n",
            "Train[44/50][171/240] Loss: 0.10474511981010437, BER : 0.04004296875\n",
            "Train[44/50][172/240] Loss: 0.10406668484210968, BER : 0.03953125\n",
            "Train[44/50][173/240] Loss: 0.10073843598365784, BER : 0.03741796875\n",
            "Train[44/50][174/240] Loss: 0.10376936942338943, BER : 0.03958203125\n",
            "Train[44/50][175/240] Loss: 0.10141739249229431, BER : 0.038328125\n",
            "Train[44/50][176/240] Loss: 0.10138828307390213, BER : 0.037453125\n",
            "Train[44/50][177/240] Loss: 0.10262732207775116, BER : 0.038765625\n",
            "Train[44/50][178/240] Loss: 0.10154630988836288, BER : 0.0375234375\n",
            "Train[44/50][179/240] Loss: 0.10328546166419983, BER : 0.039078125\n",
            "Train[44/50][180/240] Loss: 0.10326936841011047, BER : 0.0384765625\n",
            "Train[44/50][181/240] Loss: 0.10225585848093033, BER : 0.03776953125\n",
            "Train[44/50][182/240] Loss: 0.10310062021017075, BER : 0.0392734375\n",
            "Train[44/50][183/240] Loss: 0.1049557775259018, BER : 0.040859375\n",
            "Train[44/50][184/240] Loss: 0.10250431299209595, BER : 0.0389453125\n",
            "Train[44/50][185/240] Loss: 0.10074463486671448, BER : 0.0375078125\n",
            "Train[44/50][186/240] Loss: 0.10173018276691437, BER : 0.03876171875\n",
            "Train[44/50][187/240] Loss: 0.10390732437372208, BER : 0.03928125\n",
            "Train[44/50][188/240] Loss: 0.10366004705429077, BER : 0.039953125\n",
            "Train[44/50][189/240] Loss: 0.10345735400915146, BER : 0.039328125\n",
            "Train[44/50][190/240] Loss: 0.10338440537452698, BER : 0.039984375\n",
            "Train[44/50][191/240] Loss: 0.10360217094421387, BER : 0.03944921875\n",
            "Train[44/50][192/240] Loss: 0.10203077644109726, BER : 0.038703125\n",
            "Train[44/50][193/240] Loss: 0.10228265076875687, BER : 0.038140625\n",
            "Train[44/50][194/240] Loss: 0.10274505615234375, BER : 0.03876953125\n",
            "Train[44/50][195/240] Loss: 0.10254979878664017, BER : 0.03914453125\n",
            "Train[44/50][196/240] Loss: 0.10457131266593933, BER : 0.040171875\n",
            "Train[44/50][197/240] Loss: 0.10304996371269226, BER : 0.0395625\n",
            "Train[44/50][198/240] Loss: 0.10273083299398422, BER : 0.03887890625\n",
            "Train[44/50][199/240] Loss: 0.1034250482916832, BER : 0.03994140625\n",
            "Train[44/50][200/240] Loss: 0.10134772956371307, BER : 0.03773046875\n",
            "Train[44/50][201/240] Loss: 0.10110057145357132, BER : 0.03782421875\n",
            "Train[44/50][202/240] Loss: 0.10582251846790314, BER : 0.04051953125\n",
            "Train[44/50][203/240] Loss: 0.1085139811038971, BER : 0.0423203125\n",
            "Train[44/50][204/240] Loss: 0.10117590427398682, BER : 0.037484375\n",
            "Train[44/50][205/240] Loss: 0.10494156926870346, BER : 0.03973828125\n",
            "Train[44/50][206/240] Loss: 0.10493895411491394, BER : 0.040078125\n",
            "Train[44/50][207/240] Loss: 0.10567381232976913, BER : 0.04026953125\n",
            "Train[44/50][208/240] Loss: 0.10356950759887695, BER : 0.0394609375\n",
            "Train[44/50][209/240] Loss: 0.10336580872535706, BER : 0.03940234375\n",
            "Train[44/50][210/240] Loss: 0.10430746525526047, BER : 0.03962890625\n",
            "Train[44/50][211/240] Loss: 0.10276569426059723, BER : 0.038140625\n",
            "Train[44/50][212/240] Loss: 0.1016046553850174, BER : 0.0378046875\n",
            "Train[44/50][213/240] Loss: 0.10251471400260925, BER : 0.03817578125\n",
            "Train[44/50][214/240] Loss: 0.10214035212993622, BER : 0.03823046875\n",
            "Train[44/50][215/240] Loss: 0.1062183752655983, BER : 0.0407734375\n",
            "Train[44/50][216/240] Loss: 0.10172592103481293, BER : 0.03883984375\n",
            "Train[44/50][217/240] Loss: 0.10270524024963379, BER : 0.03813671875\n",
            "Train[44/50][218/240] Loss: 0.10062450170516968, BER : 0.03828125\n",
            "Train[44/50][219/240] Loss: 0.10477051883935928, BER : 0.03987890625\n",
            "Train[44/50][220/240] Loss: 0.10644838958978653, BER : 0.040109375\n",
            "Train[44/50][221/240] Loss: 0.10114096105098724, BER : 0.03774609375\n",
            "Train[44/50][222/240] Loss: 0.10221077501773834, BER : 0.0385078125\n",
            "Train[44/50][223/240] Loss: 0.1022544875741005, BER : 0.037890625\n",
            "Train[44/50][224/240] Loss: 0.10355188697576523, BER : 0.0392109375\n",
            "Train[44/50][225/240] Loss: 0.10299889743328094, BER : 0.03915234375\n",
            "Train[44/50][226/240] Loss: 0.1039469838142395, BER : 0.03893359375\n",
            "Train[44/50][227/240] Loss: 0.1020040214061737, BER : 0.03881640625\n",
            "Train[44/50][228/240] Loss: 0.10023512691259384, BER : 0.03746875\n",
            "Train[44/50][229/240] Loss: 0.10120189934968948, BER : 0.038078125\n",
            "Train[44/50][230/240] Loss: 0.10253265500068665, BER : 0.03811328125\n",
            "Train[44/50][231/240] Loss: 0.10240049660205841, BER : 0.03839453125\n",
            "Train[44/50][232/240] Loss: 0.10221295803785324, BER : 0.03833984375\n",
            "Train[44/50][233/240] Loss: 0.10403381288051605, BER : 0.0389453125\n",
            "Train[44/50][234/240] Loss: 0.10370197892189026, BER : 0.0396328125\n",
            "Train[44/50][235/240] Loss: 0.1014004498720169, BER : 0.03774609375\n",
            "Train[44/50][236/240] Loss: 0.10257423669099808, BER : 0.03775\n",
            "Train[44/50][237/240] Loss: 0.09930622577667236, BER : 0.03655078125\n",
            "Train[44/50][238/240] Loss: 0.102617546916008, BER : 0.03869140625\n",
            "Train[44/50][239/240] Loss: 0.1022908091545105, BER : 0.0384765625\n",
            "Epoch [44/50] Train_Avg_loss(epoch): 0.25296\n",
            "-------------------------------------\n",
            "Test[44/50][0/40]  Loss: 0.07116204500198364, BER (test): 0.04046484375\n",
            "Test[44/50][1/40]  Loss: 0.07066913694143295, BER (test): 0.04052734375\n",
            "Test[44/50][2/40]  Loss: 0.07038672268390656, BER (test): 0.0397890625\n",
            "Test[44/50][3/40]  Loss: 0.06899620592594147, BER (test): 0.0393359375\n",
            "Test[44/50][4/40]  Loss: 0.07042941451072693, BER (test): 0.03966796875\n",
            "Test[44/50][5/40]  Loss: 0.07493146508932114, BER (test): 0.0429375\n",
            "Test[44/50][6/40]  Loss: 0.07082866132259369, BER (test): 0.040578125\n",
            "Test[44/50][7/40]  Loss: 0.07095643877983093, BER (test): 0.0403203125\n",
            "Test[44/50][8/40]  Loss: 0.06991609930992126, BER (test): 0.03891015625\n",
            "Test[44/50][9/40]  Loss: 0.06874848902225494, BER (test): 0.0391796875\n",
            "Test[44/50][10/40]  Loss: 0.07185286283493042, BER (test): 0.04122265625\n",
            "Test[44/50][11/40]  Loss: 0.06921149045228958, BER (test): 0.03940625\n",
            "Test[44/50][12/40]  Loss: 0.07063544541597366, BER (test): 0.040265625\n",
            "Test[44/50][13/40]  Loss: 0.07259386777877808, BER (test): 0.04123046875\n",
            "Test[44/50][14/40]  Loss: 0.07141728699207306, BER (test): 0.0405\n",
            "Test[44/50][15/40]  Loss: 0.07161904871463776, BER (test): 0.04037890625\n",
            "Test[44/50][16/40]  Loss: 0.07136963307857513, BER (test): 0.04070703125\n",
            "Test[44/50][17/40]  Loss: 0.07033167034387589, BER (test): 0.03940234375\n",
            "Test[44/50][18/40]  Loss: 0.07097573578357697, BER (test): 0.04048828125\n",
            "Test[44/50][19/40]  Loss: 0.07092367112636566, BER (test): 0.04060546875\n",
            "Test[44/50][20/40]  Loss: 0.07202136516571045, BER (test): 0.04073046875\n",
            "Test[44/50][21/40]  Loss: 0.07032252848148346, BER (test): 0.0401171875\n",
            "Test[44/50][22/40]  Loss: 0.0696551576256752, BER (test): 0.03944140625\n",
            "Test[44/50][23/40]  Loss: 0.07005134969949722, BER (test): 0.03972265625\n",
            "Test[44/50][24/40]  Loss: 0.07128938287496567, BER (test): 0.04076953125\n",
            "Test[44/50][25/40]  Loss: 0.07234995067119598, BER (test): 0.0411484375\n",
            "Test[44/50][26/40]  Loss: 0.07285229116678238, BER (test): 0.0412890625\n",
            "Test[44/50][27/40]  Loss: 0.07022355496883392, BER (test): 0.04008203125\n",
            "Test[44/50][28/40]  Loss: 0.0701981782913208, BER (test): 0.03987109375\n",
            "Test[44/50][29/40]  Loss: 0.06982498615980148, BER (test): 0.04002734375\n",
            "Test[44/50][30/40]  Loss: 0.07425789535045624, BER (test): 0.04258984375\n",
            "Test[44/50][31/40]  Loss: 0.07118520140647888, BER (test): 0.03987890625\n",
            "Test[44/50][32/40]  Loss: 0.07105275243520737, BER (test): 0.04040625\n",
            "Test[44/50][33/40]  Loss: 0.07240182906389236, BER (test): 0.04144921875\n",
            "Test[44/50][34/40]  Loss: 0.07237794995307922, BER (test): 0.04198046875\n",
            "Test[44/50][35/40]  Loss: 0.07147640734910965, BER (test): 0.04019921875\n",
            "Test[44/50][36/40]  Loss: 0.07431665807962418, BER (test): 0.04233203125\n",
            "Test[44/50][37/40]  Loss: 0.07298315316438675, BER (test): 0.0414140625\n",
            "Test[44/50][38/40]  Loss: 0.07005241513252258, BER (test): 0.03987890625\n",
            "Test[44/50][39/40]  Loss: 0.07071778923273087, BER (test): 0.0403515625\n",
            "Test_Epoch [44/50] Test_Avg_loss(epoch): 0.07119\n",
            "Train[45/50][0/240] Loss: 0.1008472591638565, BER : 0.038078125\n",
            "Train[45/50][1/240] Loss: 0.10320104658603668, BER : 0.03884765625\n",
            "Train[45/50][2/240] Loss: 0.10053917020559311, BER : 0.0369609375\n",
            "Train[45/50][3/240] Loss: 0.10367338359355927, BER : 0.038859375\n",
            "Train[45/50][4/240] Loss: 0.1030505895614624, BER : 0.0379375\n",
            "Train[45/50][5/240] Loss: 0.10120704770088196, BER : 0.038125\n",
            "Train[45/50][6/240] Loss: 0.10132058709859848, BER : 0.037734375\n",
            "Train[45/50][7/240] Loss: 0.10172276943922043, BER : 0.03814453125\n",
            "Train[45/50][8/240] Loss: 0.10175775736570358, BER : 0.038859375\n",
            "Train[45/50][9/240] Loss: 0.10234235972166061, BER : 0.03807421875\n",
            "Train[45/50][10/240] Loss: 0.1027400940656662, BER : 0.039484375\n",
            "Train[45/50][11/240] Loss: 0.10038446635007858, BER : 0.03719140625\n",
            "Train[45/50][12/240] Loss: 0.1024102047085762, BER : 0.0387578125\n",
            "Train[45/50][13/240] Loss: 0.10120099782943726, BER : 0.03793359375\n",
            "Train[45/50][14/240] Loss: 0.10380041599273682, BER : 0.039765625\n",
            "Train[45/50][15/240] Loss: 0.10060763359069824, BER : 0.0373515625\n",
            "Train[45/50][16/240] Loss: 0.10544050484895706, BER : 0.04039453125\n",
            "Train[45/50][17/240] Loss: 0.10039810836315155, BER : 0.03717578125\n",
            "Train[45/50][18/240] Loss: 0.10227874666452408, BER : 0.0375390625\n",
            "Train[45/50][19/240] Loss: 0.10303505510091782, BER : 0.03880078125\n",
            "Train[45/50][20/240] Loss: 0.1031503975391388, BER : 0.038859375\n",
            "Train[45/50][21/240] Loss: 0.10356253385543823, BER : 0.03858203125\n",
            "Train[45/50][22/240] Loss: 0.10526129603385925, BER : 0.03976171875\n",
            "Train[45/50][23/240] Loss: 0.10260607302188873, BER : 0.0389140625\n",
            "Train[45/50][24/240] Loss: 0.10294035077095032, BER : 0.03848046875\n",
            "Train[45/50][25/240] Loss: 0.10494108498096466, BER : 0.03998828125\n",
            "Train[45/50][26/240] Loss: 0.10224241763353348, BER : 0.03825390625\n",
            "Train[45/50][27/240] Loss: 0.10377667099237442, BER : 0.03943359375\n",
            "Train[45/50][28/240] Loss: 0.10253477841615677, BER : 0.038453125\n",
            "Train[45/50][29/240] Loss: 0.10251650214195251, BER : 0.03825\n",
            "Train[45/50][30/240] Loss: 0.10270409286022186, BER : 0.0384609375\n",
            "Train[45/50][31/240] Loss: 0.10284670442342758, BER : 0.03837890625\n",
            "Train[45/50][32/240] Loss: 0.10354109108448029, BER : 0.03966015625\n",
            "Train[45/50][33/240] Loss: 0.10320822894573212, BER : 0.03910546875\n",
            "Train[45/50][34/240] Loss: 0.10117638111114502, BER : 0.03775390625\n",
            "Train[45/50][35/240] Loss: 0.10362789779901505, BER : 0.04008984375\n",
            "Train[45/50][36/240] Loss: 0.10328204184770584, BER : 0.0387109375\n",
            "Train[45/50][37/240] Loss: 0.10127518326044083, BER : 0.0380234375\n",
            "Train[45/50][38/240] Loss: 0.10315075516700745, BER : 0.0393125\n",
            "Train[45/50][39/240] Loss: 0.10148382186889648, BER : 0.03830078125\n",
            "Train[45/50][40/240] Loss: 0.10346204042434692, BER : 0.03883984375\n",
            "Train[45/50][41/240] Loss: 0.10115870833396912, BER : 0.037484375\n",
            "Train[45/50][42/240] Loss: 0.10405086725950241, BER : 0.0390078125\n",
            "Train[45/50][43/240] Loss: 0.10266788303852081, BER : 0.03840625\n",
            "Train[45/50][44/240] Loss: 0.1025688573718071, BER : 0.0383125\n",
            "Train[45/50][45/240] Loss: 0.10119110345840454, BER : 0.0375390625\n",
            "Train[45/50][46/240] Loss: 0.10546901822090149, BER : 0.0403828125\n",
            "Train[45/50][47/240] Loss: 0.10417792946100235, BER : 0.03944140625\n",
            "Train[45/50][48/240] Loss: 0.10113154351711273, BER : 0.037921875\n",
            "Train[45/50][49/240] Loss: 0.10244327783584595, BER : 0.03805859375\n",
            "Train[45/50][50/240] Loss: 0.10476324707269669, BER : 0.04002734375\n",
            "Train[45/50][51/240] Loss: 0.10293544828891754, BER : 0.039546875\n",
            "Train[45/50][52/240] Loss: 0.10317955911159515, BER : 0.03859375\n",
            "Train[45/50][53/240] Loss: 0.10199644416570663, BER : 0.03803515625\n",
            "Train[45/50][54/240] Loss: 0.10334709286689758, BER : 0.03865625\n",
            "Train[45/50][55/240] Loss: 0.10211051255464554, BER : 0.03848046875\n",
            "Train[45/50][56/240] Loss: 0.1043558195233345, BER : 0.03943359375\n",
            "Train[45/50][57/240] Loss: 0.09995472431182861, BER : 0.03690625\n",
            "Train[45/50][58/240] Loss: 0.09995926171541214, BER : 0.03719140625\n",
            "Train[45/50][59/240] Loss: 0.10127013921737671, BER : 0.0387109375\n",
            "Train[45/50][60/240] Loss: 0.10566916316747665, BER : 0.04017578125\n",
            "Train[45/50][61/240] Loss: 0.10517147183418274, BER : 0.0395546875\n",
            "Train[45/50][62/240] Loss: 0.10259391367435455, BER : 0.03851171875\n",
            "Train[45/50][63/240] Loss: 0.10135461390018463, BER : 0.03816796875\n",
            "Train[45/50][64/240] Loss: 0.10207958519458771, BER : 0.0382109375\n",
            "Train[45/50][65/240] Loss: 0.10229907929897308, BER : 0.03823828125\n",
            "Train[45/50][66/240] Loss: 0.10072004795074463, BER : 0.037046875\n",
            "Train[45/50][67/240] Loss: 0.10406722128391266, BER : 0.03898046875\n",
            "Train[45/50][68/240] Loss: 0.10350170731544495, BER : 0.0392734375\n",
            "Train[45/50][69/240] Loss: 0.10215875506401062, BER : 0.038265625\n",
            "Train[45/50][70/240] Loss: 0.09777433425188065, BER : 0.035640625\n",
            "Train[45/50][71/240] Loss: 0.09976378083229065, BER : 0.03688671875\n",
            "Train[45/50][72/240] Loss: 0.10229435563087463, BER : 0.03861328125\n",
            "Train[45/50][73/240] Loss: 0.10150738060474396, BER : 0.03818359375\n",
            "Train[45/50][74/240] Loss: 0.10227812826633453, BER : 0.03865234375\n",
            "Train[45/50][75/240] Loss: 0.10160641372203827, BER : 0.0384375\n",
            "Train[45/50][76/240] Loss: 0.10332056879997253, BER : 0.0392578125\n",
            "Train[45/50][77/240] Loss: 0.10488411784172058, BER : 0.04019140625\n",
            "Train[45/50][78/240] Loss: 0.10629862546920776, BER : 0.040875\n",
            "Train[45/50][79/240] Loss: 0.10269413888454437, BER : 0.03826953125\n",
            "Train[45/50][80/240] Loss: 0.10338690876960754, BER : 0.039\n",
            "Train[45/50][81/240] Loss: 0.10446524620056152, BER : 0.03938671875\n",
            "Train[45/50][82/240] Loss: 0.10090598464012146, BER : 0.03813671875\n",
            "Train[45/50][83/240] Loss: 0.1008627712726593, BER : 0.037390625\n",
            "Train[45/50][84/240] Loss: 0.09977059066295624, BER : 0.03741796875\n",
            "Train[45/50][85/240] Loss: 0.10212857276201248, BER : 0.0388046875\n",
            "Train[45/50][86/240] Loss: 0.1015854999423027, BER : 0.03828125\n",
            "Train[45/50][87/240] Loss: 0.10318926721811295, BER : 0.038953125\n",
            "Train[45/50][88/240] Loss: 0.10312561690807343, BER : 0.03860546875\n",
            "Train[45/50][89/240] Loss: 0.10074437409639359, BER : 0.0379296875\n",
            "Train[45/50][90/240] Loss: 0.10041533410549164, BER : 0.03726171875\n",
            "Train[45/50][91/240] Loss: 0.10451187193393707, BER : 0.03930078125\n",
            "Train[45/50][92/240] Loss: 0.10463492572307587, BER : 0.03987109375\n",
            "Train[45/50][93/240] Loss: 0.10092045366764069, BER : 0.0375078125\n",
            "Train[45/50][94/240] Loss: 0.10471924394369125, BER : 0.03945703125\n",
            "Train[45/50][95/240] Loss: 0.10160566866397858, BER : 0.03815234375\n",
            "Train[45/50][96/240] Loss: 0.10290653258562088, BER : 0.03925\n",
            "Train[45/50][97/240] Loss: 0.09967213869094849, BER : 0.036546875\n",
            "Train[45/50][98/240] Loss: 0.10445494204759598, BER : 0.03941015625\n",
            "Train[45/50][99/240] Loss: 0.10161450505256653, BER : 0.03828515625\n",
            "Train[45/50][100/240] Loss: 0.10747350752353668, BER : 0.0414296875\n",
            "Train[45/50][101/240] Loss: 0.10552126169204712, BER : 0.03991015625\n",
            "Train[45/50][102/240] Loss: 0.1014183908700943, BER : 0.03799609375\n",
            "Train[45/50][103/240] Loss: 0.10345624387264252, BER : 0.0396796875\n",
            "Train[45/50][104/240] Loss: 0.10195581614971161, BER : 0.0384765625\n",
            "Train[45/50][105/240] Loss: 0.102826789021492, BER : 0.0387890625\n",
            "Train[45/50][106/240] Loss: 0.10530596971511841, BER : 0.0397734375\n",
            "Train[45/50][107/240] Loss: 0.10264303535223007, BER : 0.0389140625\n",
            "Train[45/50][108/240] Loss: 0.10424849390983582, BER : 0.03966015625\n",
            "Train[45/50][109/240] Loss: 0.0998254045844078, BER : 0.0367421875\n",
            "Train[45/50][110/240] Loss: 0.09941057115793228, BER : 0.03614453125\n",
            "Train[45/50][111/240] Loss: 0.10359219461679459, BER : 0.03956640625\n",
            "Train[45/50][112/240] Loss: 0.10289880633354187, BER : 0.03891796875\n",
            "Train[45/50][113/240] Loss: 0.10080237686634064, BER : 0.0376171875\n",
            "Train[45/50][114/240] Loss: 0.10245788097381592, BER : 0.03830078125\n",
            "Train[45/50][115/240] Loss: 0.10407619923353195, BER : 0.03976171875\n",
            "Train[45/50][116/240] Loss: 0.10533734411001205, BER : 0.0392578125\n",
            "Train[45/50][117/240] Loss: 0.10544970631599426, BER : 0.03962890625\n",
            "Train[45/50][118/240] Loss: 0.10403595119714737, BER : 0.03881640625\n",
            "Train[45/50][119/240] Loss: 0.10057975351810455, BER : 0.03783984375\n",
            "Train[45/50][120/240] Loss: 0.10184647142887115, BER : 0.0380859375\n",
            "Train[45/50][121/240] Loss: 0.10182449221611023, BER : 0.0378671875\n",
            "Train[45/50][122/240] Loss: 0.10144747048616409, BER : 0.03848046875\n",
            "Train[45/50][123/240] Loss: 0.10118250548839569, BER : 0.03866796875\n",
            "Train[45/50][124/240] Loss: 0.1007956936955452, BER : 0.03680859375\n",
            "Train[45/50][125/240] Loss: 0.10027237236499786, BER : 0.0377265625\n",
            "Train[45/50][126/240] Loss: 0.10269007086753845, BER : 0.03881640625\n",
            "Train[45/50][127/240] Loss: 0.10337912291288376, BER : 0.03938671875\n",
            "Train[45/50][128/240] Loss: 0.10069946944713593, BER : 0.03776953125\n",
            "Train[45/50][129/240] Loss: 0.10293364524841309, BER : 0.03815625\n",
            "Train[45/50][130/240] Loss: 0.10491922497749329, BER : 0.0396640625\n",
            "Train[45/50][131/240] Loss: 0.10310062766075134, BER : 0.03871484375\n",
            "Train[45/50][132/240] Loss: 0.10026553273200989, BER : 0.03740234375\n",
            "Train[45/50][133/240] Loss: 0.10216020792722702, BER : 0.0385390625\n",
            "Train[45/50][134/240] Loss: 0.10082565993070602, BER : 0.037765625\n",
            "Train[45/50][135/240] Loss: 0.10133202373981476, BER : 0.0380546875\n",
            "Train[45/50][136/240] Loss: 0.1021413654088974, BER : 0.03864453125\n",
            "Train[45/50][137/240] Loss: 0.10032280534505844, BER : 0.0376484375\n",
            "Train[45/50][138/240] Loss: 0.10494966059923172, BER : 0.0404765625\n",
            "Train[45/50][139/240] Loss: 0.10543844848871231, BER : 0.03990625\n",
            "Train[45/50][140/240] Loss: 0.1014024093747139, BER : 0.03810546875\n",
            "Train[45/50][141/240] Loss: 0.10268649458885193, BER : 0.03924609375\n",
            "Train[45/50][142/240] Loss: 0.1036287248134613, BER : 0.03937109375\n",
            "Train[45/50][143/240] Loss: 0.09937625378370285, BER : 0.03753515625\n",
            "Train[45/50][144/240] Loss: 0.10481558740139008, BER : 0.04049609375\n",
            "Train[45/50][145/240] Loss: 0.10012444108724594, BER : 0.036921875\n",
            "Train[45/50][146/240] Loss: 0.10187927633523941, BER : 0.037015625\n",
            "Train[45/50][147/240] Loss: 0.10316000133752823, BER : 0.039046875\n",
            "Train[45/50][148/240] Loss: 0.10204159468412399, BER : 0.038453125\n",
            "Train[45/50][149/240] Loss: 0.1011909693479538, BER : 0.03844921875\n",
            "Train[45/50][150/240] Loss: 0.1008833646774292, BER : 0.0380859375\n",
            "Train[45/50][151/240] Loss: 0.10083139687776566, BER : 0.03728125\n",
            "Train[45/50][152/240] Loss: 0.10299591720104218, BER : 0.03919921875\n",
            "Train[45/50][153/240] Loss: 0.10388735681772232, BER : 0.038859375\n",
            "Train[45/50][154/240] Loss: 0.1022275909781456, BER : 0.03878125\n",
            "Train[45/50][155/240] Loss: 0.10067947953939438, BER : 0.0380703125\n",
            "Train[45/50][156/240] Loss: 0.10187479853630066, BER : 0.0378203125\n",
            "Train[45/50][157/240] Loss: 0.10230207443237305, BER : 0.03855078125\n",
            "Train[45/50][158/240] Loss: 0.10340061038732529, BER : 0.039265625\n",
            "Train[45/50][159/240] Loss: 0.10244931280612946, BER : 0.03826171875\n",
            "Train[45/50][160/240] Loss: 0.09906993806362152, BER : 0.03742578125\n",
            "Train[45/50][161/240] Loss: 0.10070382058620453, BER : 0.0373828125\n",
            "Train[45/50][162/240] Loss: 0.10293705761432648, BER : 0.03859375\n",
            "Train[45/50][163/240] Loss: 0.10207760334014893, BER : 0.03802734375\n",
            "Train[45/50][164/240] Loss: 0.10368824750185013, BER : 0.03881640625\n",
            "Train[45/50][165/240] Loss: 0.10078100860118866, BER : 0.0373359375\n",
            "Train[45/50][166/240] Loss: 0.10150004178285599, BER : 0.03803125\n",
            "Train[45/50][167/240] Loss: 0.09936569631099701, BER : 0.03759375\n",
            "Train[45/50][168/240] Loss: 0.10349038243293762, BER : 0.03952734375\n",
            "Train[45/50][169/240] Loss: 0.10133975744247437, BER : 0.037734375\n",
            "Train[45/50][170/240] Loss: 0.09967610239982605, BER : 0.03715625\n",
            "Train[45/50][171/240] Loss: 0.10322224348783493, BER : 0.03862109375\n",
            "Train[45/50][172/240] Loss: 0.09987026453018188, BER : 0.0368203125\n",
            "Train[45/50][173/240] Loss: 0.10312148928642273, BER : 0.03901953125\n",
            "Train[45/50][174/240] Loss: 0.10502541065216064, BER : 0.04029296875\n",
            "Train[45/50][175/240] Loss: 0.1038275957107544, BER : 0.03963671875\n",
            "Train[45/50][176/240] Loss: 0.10110799968242645, BER : 0.03816015625\n",
            "Train[45/50][177/240] Loss: 0.10057388246059418, BER : 0.03744140625\n",
            "Train[45/50][178/240] Loss: 0.10337615013122559, BER : 0.0386640625\n",
            "Train[45/50][179/240] Loss: 0.10510103404521942, BER : 0.03999609375\n",
            "Train[45/50][180/240] Loss: 0.10170117020606995, BER : 0.037859375\n",
            "Train[45/50][181/240] Loss: 0.10272637009620667, BER : 0.0388828125\n",
            "Train[45/50][182/240] Loss: 0.1030036062002182, BER : 0.038359375\n",
            "Train[45/50][183/240] Loss: 0.10415738821029663, BER : 0.0389296875\n",
            "Train[45/50][184/240] Loss: 0.10257372260093689, BER : 0.038734375\n",
            "Train[45/50][185/240] Loss: 0.10011152923107147, BER : 0.037515625\n",
            "Train[45/50][186/240] Loss: 0.10336127132177353, BER : 0.03893359375\n",
            "Train[45/50][187/240] Loss: 0.10071929544210434, BER : 0.0375390625\n",
            "Train[45/50][188/240] Loss: 0.09980372339487076, BER : 0.03734765625\n",
            "Train[45/50][189/240] Loss: 0.10080035030841827, BER : 0.03721484375\n",
            "Train[45/50][190/240] Loss: 0.10279528796672821, BER : 0.03875\n",
            "Train[45/50][191/240] Loss: 0.10259927809238434, BER : 0.03915625\n",
            "Train[45/50][192/240] Loss: 0.10323488712310791, BER : 0.03848828125\n",
            "Train[45/50][193/240] Loss: 0.10492998361587524, BER : 0.039375\n",
            "Train[45/50][194/240] Loss: 0.1016465574502945, BER : 0.03776171875\n",
            "Train[45/50][195/240] Loss: 0.10027928650379181, BER : 0.03693359375\n",
            "Train[45/50][196/240] Loss: 0.10243459045886993, BER : 0.0386328125\n",
            "Train[45/50][197/240] Loss: 0.10384199023246765, BER : 0.03799609375\n",
            "Train[45/50][198/240] Loss: 0.10248027741909027, BER : 0.0388046875\n",
            "Train[45/50][199/240] Loss: 0.09968556463718414, BER : 0.036875\n",
            "Train[45/50][200/240] Loss: 0.10236351937055588, BER : 0.03874609375\n",
            "Train[45/50][201/240] Loss: 0.1025664433836937, BER : 0.03885546875\n",
            "Train[45/50][202/240] Loss: 0.10299012064933777, BER : 0.03901171875\n",
            "Train[45/50][203/240] Loss: 0.10448362678289413, BER : 0.03929296875\n",
            "Train[45/50][204/240] Loss: 0.10306902229785919, BER : 0.03933203125\n",
            "Train[45/50][205/240] Loss: 0.1014973521232605, BER : 0.03719921875\n",
            "Train[45/50][206/240] Loss: 0.10066788643598557, BER : 0.03790234375\n",
            "Train[45/50][207/240] Loss: 0.1042613834142685, BER : 0.03971484375\n",
            "Train[45/50][208/240] Loss: 0.10580326616764069, BER : 0.03982421875\n",
            "Train[45/50][209/240] Loss: 0.1017593964934349, BER : 0.038234375\n",
            "Train[45/50][210/240] Loss: 0.1022464781999588, BER : 0.03823046875\n",
            "Train[45/50][211/240] Loss: 0.10125596076250076, BER : 0.038453125\n",
            "Train[45/50][212/240] Loss: 0.09975096583366394, BER : 0.03719921875\n",
            "Train[45/50][213/240] Loss: 0.10172228515148163, BER : 0.03844921875\n",
            "Train[45/50][214/240] Loss: 0.099800705909729, BER : 0.0372890625\n",
            "Train[45/50][215/240] Loss: 0.10133934020996094, BER : 0.03844140625\n",
            "Train[45/50][216/240] Loss: 0.1015489250421524, BER : 0.03795703125\n",
            "Train[45/50][217/240] Loss: 0.10234146565198898, BER : 0.03779296875\n",
            "Train[45/50][218/240] Loss: 0.10184291005134583, BER : 0.03875\n",
            "Train[45/50][219/240] Loss: 0.10270066559314728, BER : 0.037875\n",
            "Train[45/50][220/240] Loss: 0.10246767848730087, BER : 0.03914453125\n",
            "Train[45/50][221/240] Loss: 0.10047230124473572, BER : 0.0381484375\n",
            "Train[45/50][222/240] Loss: 0.09959186613559723, BER : 0.0370546875\n",
            "Train[45/50][223/240] Loss: 0.10258041322231293, BER : 0.03861328125\n",
            "Train[45/50][224/240] Loss: 0.10230866074562073, BER : 0.03918359375\n",
            "Train[45/50][225/240] Loss: 0.10322943329811096, BER : 0.039234375\n",
            "Train[45/50][226/240] Loss: 0.10049090534448624, BER : 0.03725390625\n",
            "Train[45/50][227/240] Loss: 0.1039506196975708, BER : 0.03983203125\n",
            "Train[45/50][228/240] Loss: 0.10568203032016754, BER : 0.0399609375\n",
            "Train[45/50][229/240] Loss: 0.10495994985103607, BER : 0.04025\n",
            "Train[45/50][230/240] Loss: 0.09992428123950958, BER : 0.03629296875\n",
            "Train[45/50][231/240] Loss: 0.10322980582714081, BER : 0.03874609375\n",
            "Train[45/50][232/240] Loss: 0.10311122983694077, BER : 0.0392890625\n",
            "Train[45/50][233/240] Loss: 0.10524694621562958, BER : 0.0405390625\n",
            "Train[45/50][234/240] Loss: 0.10479744523763657, BER : 0.03981640625\n",
            "Train[45/50][235/240] Loss: 0.10271110385656357, BER : 0.0378671875\n",
            "Train[45/50][236/240] Loss: 0.10475994646549225, BER : 0.04075\n",
            "Train[45/50][237/240] Loss: 0.10410874336957932, BER : 0.03965234375\n",
            "Train[45/50][238/240] Loss: 0.10007534921169281, BER : 0.03651171875\n",
            "Train[45/50][239/240] Loss: 0.10256358981132507, BER : 0.03862109375\n",
            "Epoch [45/50] Train_Avg_loss(epoch): 0.24969\n",
            "-------------------------------------\n",
            "Test[45/50][0/40]  Loss: 0.07082013040781021, BER (test): 0.0404375\n",
            "Test[45/50][1/40]  Loss: 0.07021807134151459, BER (test): 0.04053125\n",
            "Test[45/50][2/40]  Loss: 0.06950537115335464, BER (test): 0.04001953125\n",
            "Test[45/50][3/40]  Loss: 0.06896655261516571, BER (test): 0.040671875\n",
            "Test[45/50][4/40]  Loss: 0.07111784815788269, BER (test): 0.04148046875\n",
            "Test[45/50][5/40]  Loss: 0.0718960389494896, BER (test): 0.041671875\n",
            "Test[45/50][6/40]  Loss: 0.07195033878087997, BER (test): 0.0413984375\n",
            "Test[45/50][7/40]  Loss: 0.06756500154733658, BER (test): 0.0389296875\n",
            "Test[45/50][8/40]  Loss: 0.06974215805530548, BER (test): 0.03992578125\n",
            "Test[45/50][9/40]  Loss: 0.06739766150712967, BER (test): 0.0392890625\n",
            "Test[45/50][10/40]  Loss: 0.0705457255244255, BER (test): 0.040640625\n",
            "Test[45/50][11/40]  Loss: 0.06944168359041214, BER (test): 0.04098828125\n",
            "Test[45/50][12/40]  Loss: 0.06888027489185333, BER (test): 0.0398515625\n",
            "Test[45/50][13/40]  Loss: 0.06876203417778015, BER (test): 0.03985546875\n",
            "Test[45/50][14/40]  Loss: 0.0696764811873436, BER (test): 0.03993359375\n",
            "Test[45/50][15/40]  Loss: 0.07178737968206406, BER (test): 0.04141796875\n",
            "Test[45/50][16/40]  Loss: 0.0672873929142952, BER (test): 0.03844921875\n",
            "Test[45/50][17/40]  Loss: 0.06673119217157364, BER (test): 0.038875\n",
            "Test[45/50][18/40]  Loss: 0.07090114057064056, BER (test): 0.04116796875\n",
            "Test[45/50][19/40]  Loss: 0.06712523847818375, BER (test): 0.03879296875\n",
            "Test[45/50][20/40]  Loss: 0.06886470317840576, BER (test): 0.03966015625\n",
            "Test[45/50][21/40]  Loss: 0.06888729333877563, BER (test): 0.03956640625\n",
            "Test[45/50][22/40]  Loss: 0.06952941417694092, BER (test): 0.0403828125\n",
            "Test[45/50][23/40]  Loss: 0.0687170997262001, BER (test): 0.03963671875\n",
            "Test[45/50][24/40]  Loss: 0.07008184492588043, BER (test): 0.040828125\n",
            "Test[45/50][25/40]  Loss: 0.06903815269470215, BER (test): 0.03950390625\n",
            "Test[45/50][26/40]  Loss: 0.06713831424713135, BER (test): 0.03900390625\n",
            "Test[45/50][27/40]  Loss: 0.07198236882686615, BER (test): 0.0421640625\n",
            "Test[45/50][28/40]  Loss: 0.0687786117196083, BER (test): 0.0400625\n",
            "Test[45/50][29/40]  Loss: 0.06725334376096725, BER (test): 0.038390625\n",
            "Test[45/50][30/40]  Loss: 0.07154399901628494, BER (test): 0.04153515625\n",
            "Test[45/50][31/40]  Loss: 0.07085555046796799, BER (test): 0.0408828125\n",
            "Test[45/50][32/40]  Loss: 0.06935726851224899, BER (test): 0.0397578125\n",
            "Test[45/50][33/40]  Loss: 0.06922329217195511, BER (test): 0.03996875\n",
            "Test[45/50][34/40]  Loss: 0.06736396998167038, BER (test): 0.03855859375\n",
            "Test[45/50][35/40]  Loss: 0.07009121775627136, BER (test): 0.03994921875\n",
            "Test[45/50][36/40]  Loss: 0.0700395479798317, BER (test): 0.04105859375\n",
            "Test[45/50][37/40]  Loss: 0.06940102577209473, BER (test): 0.04008984375\n",
            "Test[45/50][38/40]  Loss: 0.07043203711509705, BER (test): 0.04158203125\n",
            "Test[45/50][39/40]  Loss: 0.07125657796859741, BER (test): 0.04138671875\n",
            "Test_Epoch [45/50] Test_Avg_loss(epoch): 0.06950\n",
            "Train[46/50][0/240] Loss: 0.10357865691184998, BER : 0.03812109375\n",
            "Train[46/50][1/240] Loss: 0.10232523828744888, BER : 0.03804296875\n",
            "Train[46/50][2/240] Loss: 0.1042829155921936, BER : 0.03980078125\n",
            "Train[46/50][3/240] Loss: 0.10128431767225266, BER : 0.038359375\n",
            "Train[46/50][4/240] Loss: 0.10077597200870514, BER : 0.03784375\n",
            "Train[46/50][5/240] Loss: 0.10407264530658722, BER : 0.039765625\n",
            "Train[46/50][6/240] Loss: 0.1003652811050415, BER : 0.03790234375\n",
            "Train[46/50][7/240] Loss: 0.101526640355587, BER : 0.03812109375\n",
            "Train[46/50][8/240] Loss: 0.10358822345733643, BER : 0.0385625\n",
            "Train[46/50][9/240] Loss: 0.10191944241523743, BER : 0.03752734375\n",
            "Train[46/50][10/240] Loss: 0.10242927074432373, BER : 0.03839453125\n",
            "Train[46/50][11/240] Loss: 0.10226889699697495, BER : 0.03871484375\n",
            "Train[46/50][12/240] Loss: 0.10049079358577728, BER : 0.0376171875\n",
            "Train[46/50][13/240] Loss: 0.10271505266427994, BER : 0.0387265625\n",
            "Train[46/50][14/240] Loss: 0.10237807035446167, BER : 0.03868359375\n",
            "Train[46/50][15/240] Loss: 0.10497710853815079, BER : 0.03966015625\n",
            "Train[46/50][16/240] Loss: 0.10355596244335175, BER : 0.03947265625\n",
            "Train[46/50][17/240] Loss: 0.10211388766765594, BER : 0.03790234375\n",
            "Train[46/50][18/240] Loss: 0.10330434888601303, BER : 0.039515625\n",
            "Train[46/50][19/240] Loss: 0.10275811702013016, BER : 0.0380390625\n",
            "Train[46/50][20/240] Loss: 0.10042469948530197, BER : 0.03677734375\n",
            "Train[46/50][21/240] Loss: 0.10031783580780029, BER : 0.03806640625\n",
            "Train[46/50][22/240] Loss: 0.09884945303201675, BER : 0.03659765625\n",
            "Train[46/50][23/240] Loss: 0.10080015659332275, BER : 0.03745703125\n",
            "Train[46/50][24/240] Loss: 0.10286971926689148, BER : 0.038546875\n",
            "Train[46/50][25/240] Loss: 0.1025795042514801, BER : 0.03821875\n",
            "Train[46/50][26/240] Loss: 0.10521266609430313, BER : 0.04015625\n",
            "Train[46/50][27/240] Loss: 0.10007409751415253, BER : 0.03711328125\n",
            "Train[46/50][28/240] Loss: 0.10249409079551697, BER : 0.03894921875\n",
            "Train[46/50][29/240] Loss: 0.10061691701412201, BER : 0.03737109375\n",
            "Train[46/50][30/240] Loss: 0.1031033843755722, BER : 0.03857421875\n",
            "Train[46/50][31/240] Loss: 0.10267752408981323, BER : 0.03884765625\n",
            "Train[46/50][32/240] Loss: 0.10264239460229874, BER : 0.03862890625\n",
            "Train[46/50][33/240] Loss: 0.10211504250764847, BER : 0.0383359375\n",
            "Train[46/50][34/240] Loss: 0.10171399265527725, BER : 0.0381171875\n",
            "Train[46/50][35/240] Loss: 0.09982126951217651, BER : 0.03628125\n",
            "Train[46/50][36/240] Loss: 0.10234268009662628, BER : 0.0384140625\n",
            "Train[46/50][37/240] Loss: 0.09931232035160065, BER : 0.03712109375\n",
            "Train[46/50][38/240] Loss: 0.10625355690717697, BER : 0.04052734375\n",
            "Train[46/50][39/240] Loss: 0.10480517148971558, BER : 0.03925390625\n",
            "Train[46/50][40/240] Loss: 0.10152344405651093, BER : 0.03810546875\n",
            "Train[46/50][41/240] Loss: 0.10144327580928802, BER : 0.03889453125\n",
            "Train[46/50][42/240] Loss: 0.10126860439777374, BER : 0.03791015625\n",
            "Train[46/50][43/240] Loss: 0.10349017381668091, BER : 0.0395078125\n",
            "Train[46/50][44/240] Loss: 0.10320873558521271, BER : 0.038515625\n",
            "Train[46/50][45/240] Loss: 0.10267890989780426, BER : 0.03929296875\n",
            "Train[46/50][46/240] Loss: 0.10491842031478882, BER : 0.039953125\n",
            "Train[46/50][47/240] Loss: 0.09941831976175308, BER : 0.03681640625\n",
            "Train[46/50][48/240] Loss: 0.10065524280071259, BER : 0.03719921875\n",
            "Train[46/50][49/240] Loss: 0.10393538326025009, BER : 0.0393984375\n",
            "Train[46/50][50/240] Loss: 0.1028275340795517, BER : 0.0391953125\n",
            "Train[46/50][51/240] Loss: 0.10389741510152817, BER : 0.04037109375\n",
            "Train[46/50][52/240] Loss: 0.10505072772502899, BER : 0.0399140625\n",
            "Train[46/50][53/240] Loss: 0.10208165645599365, BER : 0.03851953125\n",
            "Train[46/50][54/240] Loss: 0.10349109768867493, BER : 0.0392734375\n",
            "Train[46/50][55/240] Loss: 0.10381785035133362, BER : 0.039859375\n",
            "Train[46/50][56/240] Loss: 0.10255023092031479, BER : 0.03793359375\n",
            "Train[46/50][57/240] Loss: 0.10296345502138138, BER : 0.03895703125\n",
            "Train[46/50][58/240] Loss: 0.10236774384975433, BER : 0.03892578125\n",
            "Train[46/50][59/240] Loss: 0.10296038538217545, BER : 0.0390234375\n",
            "Train[46/50][60/240] Loss: 0.10310030728578568, BER : 0.0384140625\n",
            "Train[46/50][61/240] Loss: 0.1024685800075531, BER : 0.038671875\n",
            "Train[46/50][62/240] Loss: 0.10086923837661743, BER : 0.03807421875\n",
            "Train[46/50][63/240] Loss: 0.1009482815861702, BER : 0.03828125\n",
            "Train[46/50][64/240] Loss: 0.1027139201760292, BER : 0.03852734375\n",
            "Train[46/50][65/240] Loss: 0.10468896478414536, BER : 0.0395703125\n",
            "Train[46/50][66/240] Loss: 0.10271447896957397, BER : 0.0382578125\n",
            "Train[46/50][67/240] Loss: 0.0969955176115036, BER : 0.035578125\n",
            "Train[46/50][68/240] Loss: 0.10183006525039673, BER : 0.03833984375\n",
            "Train[46/50][69/240] Loss: 0.10245980322360992, BER : 0.03865234375\n",
            "Train[46/50][70/240] Loss: 0.10358846932649612, BER : 0.039625\n",
            "Train[46/50][71/240] Loss: 0.10197899490594864, BER : 0.037765625\n",
            "Train[46/50][72/240] Loss: 0.1007559522986412, BER : 0.0370859375\n",
            "Train[46/50][73/240] Loss: 0.09960692375898361, BER : 0.0363359375\n",
            "Train[46/50][74/240] Loss: 0.09933855384588242, BER : 0.03711328125\n",
            "Train[46/50][75/240] Loss: 0.10112413018941879, BER : 0.03796875\n",
            "Train[46/50][76/240] Loss: 0.10292614996433258, BER : 0.0380078125\n",
            "Train[46/50][77/240] Loss: 0.10202179849147797, BER : 0.0388984375\n",
            "Train[46/50][78/240] Loss: 0.1012459546327591, BER : 0.03740625\n",
            "Train[46/50][79/240] Loss: 0.10250239074230194, BER : 0.03860546875\n",
            "Train[46/50][80/240] Loss: 0.102320097386837, BER : 0.0384609375\n",
            "Train[46/50][81/240] Loss: 0.1009170413017273, BER : 0.03807421875\n",
            "Train[46/50][82/240] Loss: 0.10067835450172424, BER : 0.03782421875\n",
            "Train[46/50][83/240] Loss: 0.09978035092353821, BER : 0.03711328125\n",
            "Train[46/50][84/240] Loss: 0.10052461922168732, BER : 0.03708984375\n",
            "Train[46/50][85/240] Loss: 0.10123986750841141, BER : 0.0378125\n",
            "Train[46/50][86/240] Loss: 0.10186322033405304, BER : 0.03803125\n",
            "Train[46/50][87/240] Loss: 0.10411179065704346, BER : 0.0393125\n",
            "Train[46/50][88/240] Loss: 0.10313671827316284, BER : 0.039078125\n",
            "Train[46/50][89/240] Loss: 0.10210718959569931, BER : 0.038390625\n",
            "Train[46/50][90/240] Loss: 0.10087059438228607, BER : 0.03717578125\n",
            "Train[46/50][91/240] Loss: 0.10032207518815994, BER : 0.03733984375\n",
            "Train[46/50][92/240] Loss: 0.10654919594526291, BER : 0.03991015625\n",
            "Train[46/50][93/240] Loss: 0.10113655775785446, BER : 0.037671875\n",
            "Train[46/50][94/240] Loss: 0.10177704691886902, BER : 0.03793359375\n",
            "Train[46/50][95/240] Loss: 0.10299541056156158, BER : 0.03825\n",
            "Train[46/50][96/240] Loss: 0.10383854806423187, BER : 0.03765234375\n",
            "Train[46/50][97/240] Loss: 0.10330940783023834, BER : 0.0393046875\n",
            "Train[46/50][98/240] Loss: 0.10160162299871445, BER : 0.03719140625\n",
            "Train[46/50][99/240] Loss: 0.10157161951065063, BER : 0.0381796875\n",
            "Train[46/50][100/240] Loss: 0.10197513550519943, BER : 0.03834375\n",
            "Train[46/50][101/240] Loss: 0.10050028562545776, BER : 0.03815234375\n",
            "Train[46/50][102/240] Loss: 0.10306878387928009, BER : 0.03907421875\n",
            "Train[46/50][103/240] Loss: 0.10309384018182755, BER : 0.0386328125\n",
            "Train[46/50][104/240] Loss: 0.10363604128360748, BER : 0.0395546875\n",
            "Train[46/50][105/240] Loss: 0.10372404754161835, BER : 0.03896875\n",
            "Train[46/50][106/240] Loss: 0.10207701474428177, BER : 0.0388984375\n",
            "Train[46/50][107/240] Loss: 0.10442237555980682, BER : 0.03912109375\n",
            "Train[46/50][108/240] Loss: 0.1012645736336708, BER : 0.03780078125\n",
            "Train[46/50][109/240] Loss: 0.1037493348121643, BER : 0.03899609375\n",
            "Train[46/50][110/240] Loss: 0.10161048173904419, BER : 0.0379765625\n",
            "Train[46/50][111/240] Loss: 0.10294783115386963, BER : 0.0386484375\n",
            "Train[46/50][112/240] Loss: 0.09953092783689499, BER : 0.03649609375\n",
            "Train[46/50][113/240] Loss: 0.10139461606740952, BER : 0.0381015625\n",
            "Train[46/50][114/240] Loss: 0.10353489965200424, BER : 0.03861328125\n",
            "Train[46/50][115/240] Loss: 0.10120100528001785, BER : 0.0373125\n",
            "Train[46/50][116/240] Loss: 0.10675302892923355, BER : 0.041046875\n",
            "Train[46/50][117/240] Loss: 0.1004270389676094, BER : 0.036921875\n",
            "Train[46/50][118/240] Loss: 0.10221496224403381, BER : 0.0381640625\n",
            "Train[46/50][119/240] Loss: 0.09954993426799774, BER : 0.03730859375\n",
            "Train[46/50][120/240] Loss: 0.09817246347665787, BER : 0.0360625\n",
            "Train[46/50][121/240] Loss: 0.09980518370866776, BER : 0.03698046875\n",
            "Train[46/50][122/240] Loss: 0.10391983389854431, BER : 0.0394765625\n",
            "Train[46/50][123/240] Loss: 0.10402053594589233, BER : 0.039453125\n",
            "Train[46/50][124/240] Loss: 0.10269738733768463, BER : 0.03889453125\n",
            "Train[46/50][125/240] Loss: 0.10155893862247467, BER : 0.037890625\n",
            "Train[46/50][126/240] Loss: 0.1020238846540451, BER : 0.0380546875\n",
            "Train[46/50][127/240] Loss: 0.10051533579826355, BER : 0.0370859375\n",
            "Train[46/50][128/240] Loss: 0.10373659431934357, BER : 0.03898046875\n",
            "Train[46/50][129/240] Loss: 0.10061155259609222, BER : 0.03734765625\n",
            "Train[46/50][130/240] Loss: 0.10327764600515366, BER : 0.03858984375\n",
            "Train[46/50][131/240] Loss: 0.1015372946858406, BER : 0.038140625\n",
            "Train[46/50][132/240] Loss: 0.10245799273252487, BER : 0.038328125\n",
            "Train[46/50][133/240] Loss: 0.10197527706623077, BER : 0.03837890625\n",
            "Train[46/50][134/240] Loss: 0.10174006223678589, BER : 0.038359375\n",
            "Train[46/50][135/240] Loss: 0.1014302596449852, BER : 0.037875\n",
            "Train[46/50][136/240] Loss: 0.10454434901475906, BER : 0.0402734375\n",
            "Train[46/50][137/240] Loss: 0.10373089462518692, BER : 0.0395078125\n",
            "Train[46/50][138/240] Loss: 0.10009235888719559, BER : 0.03720703125\n",
            "Train[46/50][139/240] Loss: 0.1005239337682724, BER : 0.0382265625\n",
            "Train[46/50][140/240] Loss: 0.1048283651471138, BER : 0.0393125\n",
            "Train[46/50][141/240] Loss: 0.10063809156417847, BER : 0.03736328125\n",
            "Train[46/50][142/240] Loss: 0.10229464620351791, BER : 0.038\n",
            "Train[46/50][143/240] Loss: 0.10222059488296509, BER : 0.0387890625\n",
            "Train[46/50][144/240] Loss: 0.10255350172519684, BER : 0.03864453125\n",
            "Train[46/50][145/240] Loss: 0.10082560777664185, BER : 0.0374453125\n",
            "Train[46/50][146/240] Loss: 0.10344211757183075, BER : 0.039109375\n",
            "Train[46/50][147/240] Loss: 0.10088663548231125, BER : 0.03791796875\n",
            "Train[46/50][148/240] Loss: 0.1013341173529625, BER : 0.0378515625\n",
            "Train[46/50][149/240] Loss: 0.10280247032642365, BER : 0.03888671875\n",
            "Train[46/50][150/240] Loss: 0.1033603847026825, BER : 0.03970703125\n",
            "Train[46/50][151/240] Loss: 0.10081744939088821, BER : 0.03761328125\n",
            "Train[46/50][152/240] Loss: 0.09926377236843109, BER : 0.03682421875\n",
            "Train[46/50][153/240] Loss: 0.10076925158500671, BER : 0.0378828125\n",
            "Train[46/50][154/240] Loss: 0.10587766766548157, BER : 0.04033984375\n",
            "Train[46/50][155/240] Loss: 0.09999930113554001, BER : 0.03819140625\n",
            "Train[46/50][156/240] Loss: 0.10186640918254852, BER : 0.0382109375\n",
            "Train[46/50][157/240] Loss: 0.10215618461370468, BER : 0.03896875\n",
            "Train[46/50][158/240] Loss: 0.10177671164274216, BER : 0.03894921875\n",
            "Train[46/50][159/240] Loss: 0.10223767161369324, BER : 0.03735546875\n",
            "Train[46/50][160/240] Loss: 0.09947673976421356, BER : 0.03709765625\n",
            "Train[46/50][161/240] Loss: 0.10187597572803497, BER : 0.0383671875\n",
            "Train[46/50][162/240] Loss: 0.10332660377025604, BER : 0.03949609375\n",
            "Train[46/50][163/240] Loss: 0.10365129262208939, BER : 0.039515625\n",
            "Train[46/50][164/240] Loss: 0.1012362390756607, BER : 0.0376875\n",
            "Train[46/50][165/240] Loss: 0.10133548080921173, BER : 0.0376171875\n",
            "Train[46/50][166/240] Loss: 0.10231469571590424, BER : 0.0387265625\n",
            "Train[46/50][167/240] Loss: 0.10409758239984512, BER : 0.03973046875\n",
            "Train[46/50][168/240] Loss: 0.10072775185108185, BER : 0.0379453125\n",
            "Train[46/50][169/240] Loss: 0.10242543369531631, BER : 0.0386953125\n",
            "Train[46/50][170/240] Loss: 0.10186396539211273, BER : 0.03831640625\n",
            "Train[46/50][171/240] Loss: 0.10228675603866577, BER : 0.03803125\n",
            "Train[46/50][172/240] Loss: 0.10387875139713287, BER : 0.03984765625\n",
            "Train[46/50][173/240] Loss: 0.10276868939399719, BER : 0.038296875\n",
            "Train[46/50][174/240] Loss: 0.10233820229768753, BER : 0.03860546875\n",
            "Train[46/50][175/240] Loss: 0.10366249829530716, BER : 0.03896875\n",
            "Train[46/50][176/240] Loss: 0.10023218393325806, BER : 0.03766015625\n",
            "Train[46/50][177/240] Loss: 0.0995408296585083, BER : 0.0363671875\n",
            "Train[46/50][178/240] Loss: 0.10137180984020233, BER : 0.038171875\n",
            "Train[46/50][179/240] Loss: 0.10308734327554703, BER : 0.0387421875\n",
            "Train[46/50][180/240] Loss: 0.09919309616088867, BER : 0.03663671875\n",
            "Train[46/50][181/240] Loss: 0.1007194072008133, BER : 0.03759375\n",
            "Train[46/50][182/240] Loss: 0.1038605347275734, BER : 0.03972265625\n",
            "Train[46/50][183/240] Loss: 0.1017017588019371, BER : 0.03829296875\n",
            "Train[46/50][184/240] Loss: 0.1000671461224556, BER : 0.0365703125\n",
            "Train[46/50][185/240] Loss: 0.10252058506011963, BER : 0.03806640625\n",
            "Train[46/50][186/240] Loss: 0.09933889657258987, BER : 0.0371796875\n",
            "Train[46/50][187/240] Loss: 0.09960643947124481, BER : 0.037140625\n",
            "Train[46/50][188/240] Loss: 0.10174141079187393, BER : 0.03775390625\n",
            "Train[46/50][189/240] Loss: 0.1053115576505661, BER : 0.040359375\n",
            "Train[46/50][190/240] Loss: 0.10430912673473358, BER : 0.03961328125\n",
            "Train[46/50][191/240] Loss: 0.10547512024641037, BER : 0.04064453125\n",
            "Train[46/50][192/240] Loss: 0.1006387323141098, BER : 0.037515625\n",
            "Train[46/50][193/240] Loss: 0.10289053618907928, BER : 0.03966796875\n",
            "Train[46/50][194/240] Loss: 0.10143951326608658, BER : 0.0382734375\n",
            "Train[46/50][195/240] Loss: 0.10261159390211105, BER : 0.03923046875\n",
            "Train[46/50][196/240] Loss: 0.10254518687725067, BER : 0.0388203125\n",
            "Train[46/50][197/240] Loss: 0.1043374240398407, BER : 0.03978125\n",
            "Train[46/50][198/240] Loss: 0.10265341401100159, BER : 0.03851171875\n",
            "Train[46/50][199/240] Loss: 0.0997626855969429, BER : 0.0368359375\n",
            "Train[46/50][200/240] Loss: 0.1042223647236824, BER : 0.0391953125\n",
            "Train[46/50][201/240] Loss: 0.10059010982513428, BER : 0.0376171875\n",
            "Train[46/50][202/240] Loss: 0.1001201868057251, BER : 0.03753515625\n",
            "Train[46/50][203/240] Loss: 0.10110437124967575, BER : 0.03712890625\n",
            "Train[46/50][204/240] Loss: 0.10579073429107666, BER : 0.04040234375\n",
            "Train[46/50][205/240] Loss: 0.10292819142341614, BER : 0.0396796875\n",
            "Train[46/50][206/240] Loss: 0.1044883206486702, BER : 0.039515625\n",
            "Train[46/50][207/240] Loss: 0.10086967796087265, BER : 0.03818359375\n",
            "Train[46/50][208/240] Loss: 0.1013384461402893, BER : 0.037640625\n",
            "Train[46/50][209/240] Loss: 0.10301554203033447, BER : 0.03896484375\n",
            "Train[46/50][210/240] Loss: 0.10359831154346466, BER : 0.03953125\n",
            "Train[46/50][211/240] Loss: 0.10288576781749725, BER : 0.03886328125\n",
            "Train[46/50][212/240] Loss: 0.10108998417854309, BER : 0.03823046875\n",
            "Train[46/50][213/240] Loss: 0.0991889163851738, BER : 0.03704296875\n",
            "Train[46/50][214/240] Loss: 0.10270531475543976, BER : 0.0388359375\n",
            "Train[46/50][215/240] Loss: 0.10146227478981018, BER : 0.0383828125\n",
            "Train[46/50][216/240] Loss: 0.10225831717252731, BER : 0.03817578125\n",
            "Train[46/50][217/240] Loss: 0.10464512556791306, BER : 0.0393203125\n",
            "Train[46/50][218/240] Loss: 0.1069960817694664, BER : 0.0405546875\n",
            "Train[46/50][219/240] Loss: 0.10272017121315002, BER : 0.03887890625\n",
            "Train[46/50][220/240] Loss: 0.10249164700508118, BER : 0.03866796875\n",
            "Train[46/50][221/240] Loss: 0.10105283558368683, BER : 0.03698046875\n",
            "Train[46/50][222/240] Loss: 0.10170367360115051, BER : 0.0381171875\n",
            "Train[46/50][223/240] Loss: 0.09963306784629822, BER : 0.03696875\n",
            "Train[46/50][224/240] Loss: 0.10304905474185944, BER : 0.03844140625\n",
            "Train[46/50][225/240] Loss: 0.1029760092496872, BER : 0.03823046875\n",
            "Train[46/50][226/240] Loss: 0.10400184988975525, BER : 0.0389609375\n",
            "Train[46/50][227/240] Loss: 0.10396866500377655, BER : 0.039734375\n",
            "Train[46/50][228/240] Loss: 0.10239562392234802, BER : 0.03858984375\n",
            "Train[46/50][229/240] Loss: 0.10453595221042633, BER : 0.039515625\n",
            "Train[46/50][230/240] Loss: 0.10250867903232574, BER : 0.03902734375\n",
            "Train[46/50][231/240] Loss: 0.10343115776777267, BER : 0.03942578125\n",
            "Train[46/50][232/240] Loss: 0.09964198619127274, BER : 0.03755859375\n",
            "Train[46/50][233/240] Loss: 0.09869649261236191, BER : 0.0371640625\n",
            "Train[46/50][234/240] Loss: 0.10177494585514069, BER : 0.03790234375\n",
            "Train[46/50][235/240] Loss: 0.10060817748308182, BER : 0.03735546875\n",
            "Train[46/50][236/240] Loss: 0.10165049880743027, BER : 0.03793359375\n",
            "Train[46/50][237/240] Loss: 0.09993916749954224, BER : 0.03671484375\n",
            "Train[46/50][238/240] Loss: 0.10295341908931732, BER : 0.038703125\n",
            "Train[46/50][239/240] Loss: 0.10283084213733673, BER : 0.03889453125\n",
            "Epoch [46/50] Train_Avg_loss(epoch): 0.24655\n",
            "-------------------------------------\n",
            "Test[46/50][0/40]  Loss: 0.06881598383188248, BER (test): 0.0395625\n",
            "Test[46/50][1/40]  Loss: 0.06802543252706528, BER (test): 0.03893359375\n",
            "Test[46/50][2/40]  Loss: 0.06669306755065918, BER (test): 0.0378671875\n",
            "Test[46/50][3/40]  Loss: 0.06873103976249695, BER (test): 0.03923828125\n",
            "Test[46/50][4/40]  Loss: 0.07060334831476212, BER (test): 0.04035546875\n",
            "Test[46/50][5/40]  Loss: 0.06790538132190704, BER (test): 0.0386796875\n",
            "Test[46/50][6/40]  Loss: 0.06848239153623581, BER (test): 0.0387265625\n",
            "Test[46/50][7/40]  Loss: 0.06869763880968094, BER (test): 0.0396171875\n",
            "Test[46/50][8/40]  Loss: 0.06825768202543259, BER (test): 0.03962109375\n",
            "Test[46/50][9/40]  Loss: 0.06959647685289383, BER (test): 0.040078125\n",
            "Test[46/50][10/40]  Loss: 0.06964363902807236, BER (test): 0.0399921875\n",
            "Test[46/50][11/40]  Loss: 0.06813077628612518, BER (test): 0.0388984375\n",
            "Test[46/50][12/40]  Loss: 0.06645558774471283, BER (test): 0.0381875\n",
            "Test[46/50][13/40]  Loss: 0.06958264112472534, BER (test): 0.03921875\n",
            "Test[46/50][14/40]  Loss: 0.0697450339794159, BER (test): 0.0398671875\n",
            "Test[46/50][15/40]  Loss: 0.06890958547592163, BER (test): 0.03908203125\n",
            "Test[46/50][16/40]  Loss: 0.06859972327947617, BER (test): 0.03896484375\n",
            "Test[46/50][17/40]  Loss: 0.06919228285551071, BER (test): 0.0394921875\n",
            "Test[46/50][18/40]  Loss: 0.06687113642692566, BER (test): 0.03808203125\n",
            "Test[46/50][19/40]  Loss: 0.06991797685623169, BER (test): 0.03919140625\n",
            "Test[46/50][20/40]  Loss: 0.07154355943202972, BER (test): 0.04101171875\n",
            "Test[46/50][21/40]  Loss: 0.06918201595544815, BER (test): 0.0396875\n",
            "Test[46/50][22/40]  Loss: 0.06604046374559402, BER (test): 0.0374453125\n",
            "Test[46/50][23/40]  Loss: 0.06870248168706894, BER (test): 0.0392265625\n",
            "Test[46/50][24/40]  Loss: 0.06601443886756897, BER (test): 0.0379296875\n",
            "Test[46/50][25/40]  Loss: 0.07234974950551987, BER (test): 0.041671875\n",
            "Test[46/50][26/40]  Loss: 0.07322172820568085, BER (test): 0.04216796875\n",
            "Test[46/50][27/40]  Loss: 0.06950496882200241, BER (test): 0.039765625\n",
            "Test[46/50][28/40]  Loss: 0.0682433620095253, BER (test): 0.0392421875\n",
            "Test[46/50][29/40]  Loss: 0.0673367977142334, BER (test): 0.0381875\n",
            "Test[46/50][30/40]  Loss: 0.07204511016607285, BER (test): 0.04121484375\n",
            "Test[46/50][31/40]  Loss: 0.06992525607347488, BER (test): 0.04003515625\n",
            "Test[46/50][32/40]  Loss: 0.06793628633022308, BER (test): 0.0389765625\n",
            "Test[46/50][33/40]  Loss: 0.06851018965244293, BER (test): 0.038484375\n",
            "Test[46/50][34/40]  Loss: 0.06805076450109482, BER (test): 0.03919921875\n",
            "Test[46/50][35/40]  Loss: 0.0672849714756012, BER (test): 0.0387265625\n",
            "Test[46/50][36/40]  Loss: 0.07041507959365845, BER (test): 0.0405625\n",
            "Test[46/50][37/40]  Loss: 0.06680071353912354, BER (test): 0.0377421875\n",
            "Test[46/50][38/40]  Loss: 0.06993841379880905, BER (test): 0.0396796875\n",
            "Test[46/50][39/40]  Loss: 0.0714048445224762, BER (test): 0.04105859375\n",
            "Test_Epoch [46/50] Test_Avg_loss(epoch): 0.06893\n",
            "Train[47/50][0/240] Loss: 0.10360706597566605, BER : 0.03924609375\n",
            "Train[47/50][1/240] Loss: 0.10170279443264008, BER : 0.0378359375\n",
            "Train[47/50][2/240] Loss: 0.10283029079437256, BER : 0.03863671875\n",
            "Train[47/50][3/240] Loss: 0.1017349511384964, BER : 0.038203125\n",
            "Train[47/50][4/240] Loss: 0.10300192981958389, BER : 0.03851953125\n",
            "Train[47/50][5/240] Loss: 0.10351493954658508, BER : 0.03926171875\n",
            "Train[47/50][6/240] Loss: 0.1014566570520401, BER : 0.03779296875\n",
            "Train[47/50][7/240] Loss: 0.09953734278678894, BER : 0.0373125\n",
            "Train[47/50][8/240] Loss: 0.10163683444261551, BER : 0.03801953125\n",
            "Train[47/50][9/240] Loss: 0.10314420610666275, BER : 0.03866796875\n",
            "Train[47/50][10/240] Loss: 0.10419584065675735, BER : 0.0398671875\n",
            "Train[47/50][11/240] Loss: 0.10342008620500565, BER : 0.03925\n",
            "Train[47/50][12/240] Loss: 0.1033121719956398, BER : 0.03878515625\n",
            "Train[47/50][13/240] Loss: 0.09937051683664322, BER : 0.03686328125\n",
            "Train[47/50][14/240] Loss: 0.10070392489433289, BER : 0.037421875\n",
            "Train[47/50][15/240] Loss: 0.10016142576932907, BER : 0.03712890625\n",
            "Train[47/50][16/240] Loss: 0.10604804754257202, BER : 0.04062890625\n",
            "Train[47/50][17/240] Loss: 0.10373897850513458, BER : 0.039734375\n",
            "Train[47/50][18/240] Loss: 0.10287725180387497, BER : 0.03860546875\n",
            "Train[47/50][19/240] Loss: 0.10372478514909744, BER : 0.0389609375\n",
            "Train[47/50][20/240] Loss: 0.10276547074317932, BER : 0.038625\n",
            "Train[47/50][21/240] Loss: 0.10285511612892151, BER : 0.03910546875\n",
            "Train[47/50][22/240] Loss: 0.10146874934434891, BER : 0.037734375\n",
            "Train[47/50][23/240] Loss: 0.10003384202718735, BER : 0.0371796875\n",
            "Train[47/50][24/240] Loss: 0.10274069011211395, BER : 0.0383046875\n",
            "Train[47/50][25/240] Loss: 0.10070416331291199, BER : 0.037453125\n",
            "Train[47/50][26/240] Loss: 0.10158062726259232, BER : 0.03773046875\n",
            "Train[47/50][27/240] Loss: 0.10184646397829056, BER : 0.0379921875\n",
            "Train[47/50][28/240] Loss: 0.09960579872131348, BER : 0.03699609375\n",
            "Train[47/50][29/240] Loss: 0.10030466318130493, BER : 0.03652734375\n",
            "Train[47/50][30/240] Loss: 0.09762956202030182, BER : 0.03585546875\n",
            "Train[47/50][31/240] Loss: 0.10100584477186203, BER : 0.03775\n",
            "Train[47/50][32/240] Loss: 0.10086843371391296, BER : 0.03759765625\n",
            "Train[47/50][33/240] Loss: 0.1038266271352768, BER : 0.03876953125\n",
            "Train[47/50][34/240] Loss: 0.09902259707450867, BER : 0.035953125\n",
            "Train[47/50][35/240] Loss: 0.1000785231590271, BER : 0.03681640625\n",
            "Train[47/50][36/240] Loss: 0.10099585354328156, BER : 0.03748046875\n",
            "Train[47/50][37/240] Loss: 0.0992744117975235, BER : 0.0365390625\n",
            "Train[47/50][38/240] Loss: 0.10283175110816956, BER : 0.039234375\n",
            "Train[47/50][39/240] Loss: 0.1041526049375534, BER : 0.03928515625\n",
            "Train[47/50][40/240] Loss: 0.10318709909915924, BER : 0.03947265625\n",
            "Train[47/50][41/240] Loss: 0.10079836845397949, BER : 0.03730859375\n",
            "Train[47/50][42/240] Loss: 0.10551682114601135, BER : 0.040984375\n",
            "Train[47/50][43/240] Loss: 0.10272584855556488, BER : 0.038734375\n",
            "Train[47/50][44/240] Loss: 0.1012459397315979, BER : 0.037828125\n",
            "Train[47/50][45/240] Loss: 0.10244432836771011, BER : 0.03927734375\n",
            "Train[47/50][46/240] Loss: 0.10512979328632355, BER : 0.0409765625\n",
            "Train[47/50][47/240] Loss: 0.10380349308252335, BER : 0.0387265625\n",
            "Train[47/50][48/240] Loss: 0.10212785750627518, BER : 0.03885546875\n",
            "Train[47/50][49/240] Loss: 0.10119921714067459, BER : 0.03821875\n",
            "Train[47/50][50/240] Loss: 0.10033343732357025, BER : 0.03759765625\n",
            "Train[47/50][51/240] Loss: 0.10230530053377151, BER : 0.0387109375\n",
            "Train[47/50][52/240] Loss: 0.09962252527475357, BER : 0.0369140625\n",
            "Train[47/50][53/240] Loss: 0.09804607927799225, BER : 0.03648828125\n",
            "Train[47/50][54/240] Loss: 0.10241713374853134, BER : 0.03875\n",
            "Train[47/50][55/240] Loss: 0.10429572314023972, BER : 0.0394296875\n",
            "Train[47/50][56/240] Loss: 0.09984412044286728, BER : 0.03675390625\n",
            "Train[47/50][57/240] Loss: 0.10325628519058228, BER : 0.03932421875\n",
            "Train[47/50][58/240] Loss: 0.10172323137521744, BER : 0.03859375\n",
            "Train[47/50][59/240] Loss: 0.10282252728939056, BER : 0.03963671875\n",
            "Train[47/50][60/240] Loss: 0.10168236494064331, BER : 0.03885546875\n",
            "Train[47/50][61/240] Loss: 0.10112214088439941, BER : 0.0382265625\n",
            "Train[47/50][62/240] Loss: 0.10081029683351517, BER : 0.03727734375\n",
            "Train[47/50][63/240] Loss: 0.10114343464374542, BER : 0.03759375\n",
            "Train[47/50][64/240] Loss: 0.10280320048332214, BER : 0.0390234375\n",
            "Train[47/50][65/240] Loss: 0.10174265503883362, BER : 0.0391875\n",
            "Train[47/50][66/240] Loss: 0.10214678198099136, BER : 0.03848828125\n",
            "Train[47/50][67/240] Loss: 0.10140762478113174, BER : 0.0379140625\n",
            "Train[47/50][68/240] Loss: 0.10003342479467392, BER : 0.03753515625\n",
            "Train[47/50][69/240] Loss: 0.10192330926656723, BER : 0.038125\n",
            "Train[47/50][70/240] Loss: 0.1013750433921814, BER : 0.03790625\n",
            "Train[47/50][71/240] Loss: 0.10261022299528122, BER : 0.0390625\n",
            "Train[47/50][72/240] Loss: 0.10104236006736755, BER : 0.0380625\n",
            "Train[47/50][73/240] Loss: 0.10394953936338425, BER : 0.03924609375\n",
            "Train[47/50][74/240] Loss: 0.10311833024024963, BER : 0.03867578125\n",
            "Train[47/50][75/240] Loss: 0.1012873500585556, BER : 0.0377265625\n",
            "Train[47/50][76/240] Loss: 0.10018683224916458, BER : 0.03737890625\n",
            "Train[47/50][77/240] Loss: 0.10121315717697144, BER : 0.03821875\n",
            "Train[47/50][78/240] Loss: 0.10331720113754272, BER : 0.03925\n",
            "Train[47/50][79/240] Loss: 0.10104568302631378, BER : 0.03784375\n",
            "Train[47/50][80/240] Loss: 0.10241018235683441, BER : 0.0391953125\n",
            "Train[47/50][81/240] Loss: 0.10002024471759796, BER : 0.037640625\n",
            "Train[47/50][82/240] Loss: 0.10051559656858444, BER : 0.03734375\n",
            "Train[47/50][83/240] Loss: 0.10203282535076141, BER : 0.03825\n",
            "Train[47/50][84/240] Loss: 0.10372339934110641, BER : 0.03926953125\n",
            "Train[47/50][85/240] Loss: 0.099293053150177, BER : 0.03718359375\n",
            "Train[47/50][86/240] Loss: 0.10004916042089462, BER : 0.037171875\n",
            "Train[47/50][87/240] Loss: 0.10357853025197983, BER : 0.0391484375\n",
            "Train[47/50][88/240] Loss: 0.10292352735996246, BER : 0.03954296875\n",
            "Train[47/50][89/240] Loss: 0.10177094489336014, BER : 0.03771484375\n",
            "Train[47/50][90/240] Loss: 0.10322876274585724, BER : 0.03938671875\n",
            "Train[47/50][91/240] Loss: 0.10173429548740387, BER : 0.03792578125\n",
            "Train[47/50][92/240] Loss: 0.10315876454114914, BER : 0.03946875\n",
            "Train[47/50][93/240] Loss: 0.10360337793827057, BER : 0.038875\n",
            "Train[47/50][94/240] Loss: 0.10218576341867447, BER : 0.0379765625\n",
            "Train[47/50][95/240] Loss: 0.10111092776060104, BER : 0.03787890625\n",
            "Train[47/50][96/240] Loss: 0.09914868324995041, BER : 0.03687890625\n",
            "Train[47/50][97/240] Loss: 0.0972314327955246, BER : 0.03578125\n",
            "Train[47/50][98/240] Loss: 0.10026044398546219, BER : 0.037328125\n",
            "Train[47/50][99/240] Loss: 0.10158640146255493, BER : 0.03882421875\n",
            "Train[47/50][100/240] Loss: 0.10332386940717697, BER : 0.0390234375\n",
            "Train[47/50][101/240] Loss: 0.1007649153470993, BER : 0.0376875\n",
            "Train[47/50][102/240] Loss: 0.1015533059835434, BER : 0.03840234375\n",
            "Train[47/50][103/240] Loss: 0.10411092638969421, BER : 0.03984375\n",
            "Train[47/50][104/240] Loss: 0.10394318401813507, BER : 0.03912890625\n",
            "Train[47/50][105/240] Loss: 0.10428658872842789, BER : 0.038984375\n",
            "Train[47/50][106/240] Loss: 0.1041557565331459, BER : 0.0398828125\n",
            "Train[47/50][107/240] Loss: 0.1007770299911499, BER : 0.03795703125\n",
            "Train[47/50][108/240] Loss: 0.10306306183338165, BER : 0.0381875\n",
            "Train[47/50][109/240] Loss: 0.10191620886325836, BER : 0.0381953125\n",
            "Train[47/50][110/240] Loss: 0.09920753538608551, BER : 0.03683203125\n",
            "Train[47/50][111/240] Loss: 0.10306084156036377, BER : 0.0392578125\n",
            "Train[47/50][112/240] Loss: 0.1008988544344902, BER : 0.037703125\n",
            "Train[47/50][113/240] Loss: 0.10480859875679016, BER : 0.0403125\n",
            "Train[47/50][114/240] Loss: 0.10006599873304367, BER : 0.0371796875\n",
            "Train[47/50][115/240] Loss: 0.10113821923732758, BER : 0.03815234375\n",
            "Train[47/50][116/240] Loss: 0.10182558745145798, BER : 0.038\n",
            "Train[47/50][117/240] Loss: 0.10108494013547897, BER : 0.03790625\n",
            "Train[47/50][118/240] Loss: 0.10173891484737396, BER : 0.03868359375\n",
            "Train[47/50][119/240] Loss: 0.10109156370162964, BER : 0.038484375\n",
            "Train[47/50][120/240] Loss: 0.10156833380460739, BER : 0.037578125\n",
            "Train[47/50][121/240] Loss: 0.10191422700881958, BER : 0.03809375\n",
            "Train[47/50][122/240] Loss: 0.1010490134358406, BER : 0.0381875\n",
            "Train[47/50][123/240] Loss: 0.10335781425237656, BER : 0.03891015625\n",
            "Train[47/50][124/240] Loss: 0.10171179473400116, BER : 0.03791015625\n",
            "Train[47/50][125/240] Loss: 0.10142950713634491, BER : 0.038140625\n",
            "Train[47/50][126/240] Loss: 0.1012275218963623, BER : 0.03820703125\n",
            "Train[47/50][127/240] Loss: 0.10432890057563782, BER : 0.039625\n",
            "Train[47/50][128/240] Loss: 0.10068468749523163, BER : 0.0373984375\n",
            "Train[47/50][129/240] Loss: 0.10084833949804306, BER : 0.037609375\n",
            "Train[47/50][130/240] Loss: 0.10112011432647705, BER : 0.037703125\n",
            "Train[47/50][131/240] Loss: 0.10183832794427872, BER : 0.0381640625\n",
            "Train[47/50][132/240] Loss: 0.10079355537891388, BER : 0.03818359375\n",
            "Train[47/50][133/240] Loss: 0.10177294909954071, BER : 0.03837109375\n",
            "Train[47/50][134/240] Loss: 0.10226727277040482, BER : 0.0389609375\n",
            "Train[47/50][135/240] Loss: 0.1026589572429657, BER : 0.03877734375\n",
            "Train[47/50][136/240] Loss: 0.10250077396631241, BER : 0.03867578125\n",
            "Train[47/50][137/240] Loss: 0.10178694128990173, BER : 0.0388984375\n",
            "Train[47/50][138/240] Loss: 0.10369250178337097, BER : 0.03957421875\n",
            "Train[47/50][139/240] Loss: 0.10269228368997574, BER : 0.0383515625\n",
            "Train[47/50][140/240] Loss: 0.10021132230758667, BER : 0.0375234375\n",
            "Train[47/50][141/240] Loss: 0.10041876882314682, BER : 0.037578125\n",
            "Train[47/50][142/240] Loss: 0.10014495998620987, BER : 0.0378125\n",
            "Train[47/50][143/240] Loss: 0.10174831748008728, BER : 0.03848828125\n",
            "Train[47/50][144/240] Loss: 0.10280080884695053, BER : 0.0384609375\n",
            "Train[47/50][145/240] Loss: 0.10032973438501358, BER : 0.03721484375\n",
            "Train[47/50][146/240] Loss: 0.1014045923948288, BER : 0.037328125\n",
            "Train[47/50][147/240] Loss: 0.10334853082895279, BER : 0.0395234375\n",
            "Train[47/50][148/240] Loss: 0.10066492855548859, BER : 0.0376875\n",
            "Train[47/50][149/240] Loss: 0.1009485125541687, BER : 0.03787890625\n",
            "Train[47/50][150/240] Loss: 0.10013885051012039, BER : 0.038109375\n",
            "Train[47/50][151/240] Loss: 0.10123734176158905, BER : 0.03823828125\n",
            "Train[47/50][152/240] Loss: 0.099363312125206, BER : 0.03680859375\n",
            "Train[47/50][153/240] Loss: 0.10047878324985504, BER : 0.03775\n",
            "Train[47/50][154/240] Loss: 0.10444949567317963, BER : 0.0403359375\n",
            "Train[47/50][155/240] Loss: 0.10358258336782455, BER : 0.03935546875\n",
            "Train[47/50][156/240] Loss: 0.10275126248598099, BER : 0.0389375\n",
            "Train[47/50][157/240] Loss: 0.10295900702476501, BER : 0.0393359375\n",
            "Train[47/50][158/240] Loss: 0.1007138341665268, BER : 0.0374453125\n",
            "Train[47/50][159/240] Loss: 0.10290181636810303, BER : 0.03950390625\n",
            "Train[47/50][160/240] Loss: 0.09826512634754181, BER : 0.03625390625\n",
            "Train[47/50][161/240] Loss: 0.10040603578090668, BER : 0.03811328125\n",
            "Train[47/50][162/240] Loss: 0.10480829328298569, BER : 0.04012890625\n",
            "Train[47/50][163/240] Loss: 0.09977397322654724, BER : 0.03703125\n",
            "Train[47/50][164/240] Loss: 0.10211370885372162, BER : 0.03883203125\n",
            "Train[47/50][165/240] Loss: 0.1041308343410492, BER : 0.03918359375\n",
            "Train[47/50][166/240] Loss: 0.10472829639911652, BER : 0.0396640625\n",
            "Train[47/50][167/240] Loss: 0.10530835390090942, BER : 0.04058203125\n",
            "Train[47/50][168/240] Loss: 0.10230030119419098, BER : 0.03879296875\n",
            "Train[47/50][169/240] Loss: 0.10003423690795898, BER : 0.0380703125\n",
            "Train[47/50][170/240] Loss: 0.09915591031312943, BER : 0.03646484375\n",
            "Train[47/50][171/240] Loss: 0.1025252640247345, BER : 0.03882421875\n",
            "Train[47/50][172/240] Loss: 0.10123474895954132, BER : 0.03722265625\n",
            "Train[47/50][173/240] Loss: 0.09991641342639923, BER : 0.03666015625\n",
            "Train[47/50][174/240] Loss: 0.0998695120215416, BER : 0.03695703125\n",
            "Train[47/50][175/240] Loss: 0.10207124799489975, BER : 0.03823828125\n",
            "Train[47/50][176/240] Loss: 0.10163488984107971, BER : 0.0373515625\n",
            "Train[47/50][177/240] Loss: 0.10210061073303223, BER : 0.03908203125\n",
            "Train[47/50][178/240] Loss: 0.10392248630523682, BER : 0.0395\n",
            "Train[47/50][179/240] Loss: 0.10175325721502304, BER : 0.0394453125\n",
            "Train[47/50][180/240] Loss: 0.10133133083581924, BER : 0.03836328125\n",
            "Train[47/50][181/240] Loss: 0.1022365614771843, BER : 0.03832421875\n",
            "Train[47/50][182/240] Loss: 0.09950341284275055, BER : 0.03670703125\n",
            "Train[47/50][183/240] Loss: 0.10103440284729004, BER : 0.03746484375\n",
            "Train[47/50][184/240] Loss: 0.1002085953950882, BER : 0.037765625\n",
            "Train[47/50][185/240] Loss: 0.10166018456220627, BER : 0.0388515625\n",
            "Train[47/50][186/240] Loss: 0.10364624857902527, BER : 0.03904296875\n",
            "Train[47/50][187/240] Loss: 0.10478679835796356, BER : 0.04076171875\n",
            "Train[47/50][188/240] Loss: 0.09934671223163605, BER : 0.03683203125\n",
            "Train[47/50][189/240] Loss: 0.1023639366030693, BER : 0.03824609375\n",
            "Train[47/50][190/240] Loss: 0.10179570317268372, BER : 0.0377109375\n",
            "Train[47/50][191/240] Loss: 0.10074613988399506, BER : 0.03807421875\n",
            "Train[47/50][192/240] Loss: 0.10212913900613785, BER : 0.03854296875\n",
            "Train[47/50][193/240] Loss: 0.10157069563865662, BER : 0.038109375\n",
            "Train[47/50][194/240] Loss: 0.09974256902933121, BER : 0.03716015625\n",
            "Train[47/50][195/240] Loss: 0.10305015742778778, BER : 0.0386796875\n",
            "Train[47/50][196/240] Loss: 0.10210411250591278, BER : 0.03876953125\n",
            "Train[47/50][197/240] Loss: 0.10388404875993729, BER : 0.0393828125\n",
            "Train[47/50][198/240] Loss: 0.09906524419784546, BER : 0.03680078125\n",
            "Train[47/50][199/240] Loss: 0.10487869381904602, BER : 0.04015234375\n",
            "Train[47/50][200/240] Loss: 0.10303962975740433, BER : 0.03887109375\n",
            "Train[47/50][201/240] Loss: 0.10391832143068314, BER : 0.038609375\n",
            "Train[47/50][202/240] Loss: 0.10185004770755768, BER : 0.03765625\n",
            "Train[47/50][203/240] Loss: 0.10299324989318848, BER : 0.038203125\n",
            "Train[47/50][204/240] Loss: 0.10347028076648712, BER : 0.0393125\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"Saved_Trained_Checkpoints/\"\n",
        "Output_Spikes = \"Output_Spikes/\"\n",
        "Enc_syn_Spikes = \"Enc_syn_Spikes/\"\n",
        "Intermediate_Lyrs = \"Intermediate_Lyrs/\"\n",
        "epoch_activations_list = [] # Create a list to store activations for each epoch\n",
        "epoch_activations = {}\n",
        "\n",
        "\n",
        "\n",
        "# # Run training and testing\n",
        "# for e in range(epochs):\n",
        "#     train_loss = train(net, train_loader, optimizer, e)\n",
        "#     train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "#     test_loss = test(net, test_loader, optimizer, e)\n",
        "#     test_avg_loss_rec.append(sum(test_loss_rec) / (len(test_loader)))\n",
        "\n",
        "\n",
        "#     #----------Save the model every 10 epochs\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#         model_path = checkpoint_path + f\"model_epoch_{e + 1}.pt\"\n",
        "#         torch.save(net.state_dict(), model_path)\n",
        "\n",
        "#     # # ---------------------------------------------- Add hooks for specific layers\n",
        "\n",
        "#     hook_layers = [net.encoder[2], net.encoder[5], net.encoder[8], net.encoder[11], net.decoder[1], net.decoder[4], net.decoder[7], net.decoder[9]]  # You can choose the layers you want to capture activations from\n",
        "#     hook_names = [\"Enc_Lk1\",\"Enc_syn1\", \"Enc_syn2\", \"Enc_Lk2\", \"Dec_Lk1\", \"Dec_syn1\", \"Dec_syn2\", \"Dec_Lk2\"]  # Names for the captured activations\n",
        "#     hooks = []\n",
        "#     for i, layer in enumerate(hook_layers):\n",
        "#       hook_fn = get_activation(hook_names[i])\n",
        "#       hooks.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "#     epoch_activations[e] = {}\n",
        "#     for i, name in enumerate(hook_names):\n",
        "#       epoch_activations[e][name] = activation[name]\n",
        "\n",
        "#     # Check if the current epoch is a multiple of 10\n",
        "#     if (e + 1) % 10 == 0:\n",
        "#       # Save the epoch_activations dictionary to a file\n",
        "#       activations_path = Intermediate_Lyrs + f\"Intermediate_Lyrs_epoch_{e + 1}.pkl\"\n",
        "#       with open(activations_path, 'wb') as file:\n",
        "#         pickle.dump(epoch_activations, file)\n",
        "\n",
        "\n",
        "\n",
        "# ///////////////////////////////////\n",
        "\n",
        "# Define hook_layers and hook_names\n",
        "hook_layers = [net.encoder[2], net.encoder[5], net.encoder[8], net.encoder[11], net.decoder[1], net.decoder[4], net.decoder[7], net.decoder[9]]\n",
        "hook_names = [\"Enc_Lk1\", \"Enc_syn1\", \"Enc_syn2\", \"Enc_Lk2\", \"Dec_Lk1\", \"Dec_syn1\", \"Dec_syn2\", \"Dec_Lk2\"]\n",
        "\n",
        "# Create an empty dictionary to store activations\n",
        "epoch_activations = {}\n",
        "\n",
        "# Register hooks for capturing activations\n",
        "hooks = []\n",
        "for i, layer in enumerate(hook_layers):\n",
        "    hook_fn = get_activation(hook_names[i])\n",
        "    hooks.append(layer.register_forward_hook(hook_fn))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "import os\n",
        "\n",
        "# Run training and testing\n",
        "for e in range(epochs):\n",
        "    train_loss = train(net, train_loader, optimizer, e)\n",
        "    train_avg_loss_rec.append(sum(train_loss_rec) / len(train_loader))\n",
        "\n",
        "    test_loss = test(net, test_loader, optimizer, e)\n",
        "    test_avg_loss_rec.append(sum(test_loss_rec) / (len(test_loader)))\n",
        "\n",
        "     #----------Save the model every 10 epochs\n",
        "    if (e + 1) % 10 == 0:\n",
        "       model_path = checkpoint_path + f\"StdDev0_1_model_epoch_{e + 1}.pt\"\n",
        "       torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    # #---------------------------------------------------------- Access the out_en tensor\n",
        "    out_en = net.out_en\n",
        "    out_en_numpy = out_en.cpu().detach().numpy()\n",
        "    # Save with a different name for each epoch\n",
        "    out_en_filename = Output_Spikes + f\"StdDev0_1_out_en_epoch_{e + 1}.npy\"\n",
        "    np.save(out_en_filename, out_en_numpy)\n",
        "\n",
        "    # #-----------------------------------------------------------Access the out tensor\n",
        "    out = net.out\n",
        "    out_numpy = out.cpu().detach().numpy()\n",
        "    # Save with a different name for each epoch\n",
        "    out_filename = Output_Spikes + f\"StdDev0_1_out_epoch_{e + 1}.npy\"\n",
        "    np.save(out_filename, out_numpy)\n",
        "    # -------------------------------------------------------------------- Intermediate Layers\n",
        "# Check if the current epoch is a multiple of 10\n",
        "    if (e + 1) % 10 == 0:\n",
        "        # Save the epoch_activations dictionary to a file\n",
        "        activations_path = Intermediate_Lyrs +  f\"StdDev0_1_activations_epoch_{e + 1}.pkl\"\n",
        "        # activations_path = Intermediate_Lyrs +  f\"StdDev0_1_OverFrng_activations_epoch_{e + 1}.pkl\"\n",
        "        with open(activations_path, 'wb') as file:\n",
        "            pickle.dump(epoch_activations, file)\n",
        "    # Capture activations for the current epoch\n",
        "    epoch_activations[e] = {}\n",
        "    for i, name in enumerate(hook_names):\n",
        "        epoch_activations[e][name] = activation.get(name, None)  # Use get to avoid KeyError\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# beta_syn=0.9\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ////////////////////////////////////// just for printing elapsed time\n",
        "def format_time(seconds):\n",
        "    if seconds < 300:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 3600:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)\n",
        "\n",
        "dt = datetime.datetime.now() - start_time\n",
        "t = format_time(dt.total_seconds())\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "dt = datetime.datetime.now() - start_time\n",
        "seconds = dt.total_seconds()\n",
        "t = format_time(seconds)\n",
        "print(\"time: %s \" % (t))\n",
        "# /////////////////////////////////////////////////"
      ],
      "metadata": {
        "id": "db91bAgTvFEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sfe1K9O5v1FS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}